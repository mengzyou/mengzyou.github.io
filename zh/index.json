[{"categories":["OpenTool"],"contents":"我们通常可以将一台或多台服务器作为Docker主机，使用容器跑一些开源的工具服务。而往往我们不知道该什么时候这个这些应用有了更新的版本，最近发现了一个开源的工具，可以检查主机上运行的容器的镜像是否有更新，并可以通过集成多种渠道发送更新通知，这款工具就是 DIUN(Docker Image Update Notifier) 。\nDUIN介绍 DUIN是一款使用GO语言编写的命令行工具，可以本地运行，也可以通过容器运行（开发者提供了构建好的镜像 )，当监控的容器镜像在相应的注册表（Registry）中更新时，可以接收到相应的通知。\nDUIN支持多种监控配置（Providers）：\nDocker - 分析Docker主机上运行容器的镜像，并检查其更新 Podman - 类似Docker，需要Podman以服务方式启动 Kubernetes - 分析Kubernetes集群中的Pods，检查pod使用的镜像 Swarm - 分析Swarm集群中服务使用的镜像 Nomad - 类似Docker，分析Nomad引擎运行的镜像 Dockerfile - 分析Dockerfile中引用的镜像 File - yaml格式的配置文件，直接配置需要检查的镜像信息 DUIN支持集成多种通知渠道，例如 Discord， Slack，Matrix，Telegram 以及 Webhook 等。\nDUIN使用示例 这里将演示在Docker主机上使用Docker Compose来运行duin服务，并集成Slack，将通知发送到相应的频道。\ndocker-compose.yml :\nservices: diun: image: crazymax/diun:latest container_name: diun hostname: home200-diun command: serve volumes: - diundata:/data - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; environment: - \u0026#34;TZ=Asia/Shanghai\u0026#34; - \u0026#34;LOG_LEVEL=info\u0026#34; - \u0026#34;LOG_JSON=false\u0026#34; - \u0026#34;DIUN_WATCH_WORKERS=20\u0026#34; - \u0026#34;DIUN_WATCH_SCHEDULE=0 */6 * * *\u0026#34; - \u0026#34;DIUN_WATCH_JITTER=30s\u0026#34; - \u0026#34;DIUN_PROVIDERS_DOCKER=true\u0026#34; - \u0026#34;DIUN_PROVIDERS_DOCKER_WATCHBYDEFAULT=true\u0026#34; - \u0026#34;DIUN_NOTIF_SLACK_WEBHOOKURL=https://hooks.slack.com/services/xxxxxxxxxxxxx\u0026#34; restart: on-failure volumes: diundata: 上面的环境变量中\nDIUN_WATCH_SCHEDULE=0 */6 * * * 指定每6小时做一次检查 DIUN_PROVIDERS_DOCKER=true 指定使用Docker Provider，因此需要绑定 /var/run/docker.sock:/var/run/docker.sock DIUN_PROVIDERS_DOCKER_WATCHBYDEFAULT=true 指定默认检查当前Docker环境中运行的所有容器的镜像，如果该值设置为 false，则在运行需要检查镜像的容器时，需要添加标签 diun.enable=true DIUN_NOTIF_SLACK_WEBHOOKURL= 指定了发现更新时，将通知发送到Slack的频道，配置的值只需要在Slack的某个频道中添加一个Incoming Webhook应用即可 启动更多的配置，可参考文档。\n启动容器，可进入容器进行通知测试\n➜ docker compose exec diun sh / # diun notif test Notification sent for slack notifier(s) 在Slack中，将收到如下所示的通知\n之后，当DIUN发现有新的镜像发布到镜像仓库后，就会收到相应的通知，我们就可以选择是否进行应用升级。\n当我们为应用使用固定标签的镜像时，我们可以指定相应的标签来进行检查，如\nlabels: - \u0026#39;diun.enable=true\u0026#39; - \u0026#39;diun.watch_repo=true\u0026#39; - \u0026#39;diun.include_tags=^\\d+\\.\\d+\\.\\d+$\u0026#39; 上面的正则指定了需要检查的标签。\n总结 这里推荐了一个开源的容器镜像更新通知工具，同时演示了基于Docker+Slack的集成，更多的使用方式请参考其文档 。\n","date":"March 18, 2023","hero":"/zh/posts/opentool/diun-intro/diun-banner.jpg","permalink":"/zh/posts/opentool/diun-intro/","summary":"我们通常可以将一台或多台服务器作为Docker主机，使用容器跑一些开源的工具服务。而往往我们不知道该什么时候这个这些应用有了更新的版本，最近发现了一个开源的工具，可以检查主机上运行的容器的镜像是否有更新，并可以通过集成多种渠道发送更新通知，这款工具就是 DIUN(Docker Image Update Notifier) 。\nDUIN介绍 DUIN是一款使用GO语言编写的命令行工具，可以本地运行，也可以通过容器运行（开发者提供了构建好的镜像 )，当监控的容器镜像在相应的注册表（Registry）中更新时，可以接收到相应的通知。\nDUIN支持多种监控配置（Providers）：\nDocker - 分析Docker主机上运行容器的镜像，并检查其更新 Podman - 类似Docker，需要Podman以服务方式启动 Kubernetes - 分析Kubernetes集群中的Pods，检查pod使用的镜像 Swarm - 分析Swarm集群中服务使用的镜像 Nomad - 类似Docker，分析Nomad引擎运行的镜像 Dockerfile - 分析Dockerfile中引用的镜像 File - yaml格式的配置文件，直接配置需要检查的镜像信息 DUIN支持集成多种通知渠道，例如 Discord， Slack，Matrix，Telegram 以及 Webhook 等。\nDUIN使用示例 这里将演示在Docker主机上使用Docker Compose来运行duin服务，并集成Slack，将通知发送到相应的频道。\ndocker-compose.yml :\nservices: diun: image: crazymax/diun:latest container_name: diun hostname: home200-diun command: serve volumes: - diundata:/data - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; environment: - \u0026#34;TZ=Asia/Shanghai\u0026#34; - \u0026#34;LOG_LEVEL=info\u0026#34; - \u0026#34;LOG_JSON=false\u0026#34; - \u0026#34;DIUN_WATCH_WORKERS=20\u0026#34; - \u0026#34;DIUN_WATCH_SCHEDULE=0 */6 * * *\u0026#34; - \u0026#34;DIUN_WATCH_JITTER=30s\u0026#34; - \u0026#34;DIUN_PROVIDERS_DOCKER=true\u0026#34; - \u0026#34;DIUN_PROVIDERS_DOCKER_WATCHBYDEFAULT=true\u0026#34; - \u0026#34;DIUN_NOTIF_SLACK_WEBHOOKURL=https://hooks.","tags":["cotainer","docker","slack"],"title":"DIUN-容器镜像更新通知"},{"categories":["Container"],"contents":"在现代的B/S架构应用中，我们会做前后端分离，某些前端Web服务会将编译完成的静态文件放到一个web服务器进行部署。例如，我的博客也是基于Hugo编译的静态文件来进行部署的。\n那在容器化部署模式下，我们需要基于一个web服务的基础容器（镜像）将静态文件构建成站点或者Web服务的容器镜像来进行部署。在Docker开发最佳实践中，我们应该尽量保持镜像足够小（Size大小）。因此，我们应该尽量选择满足我们需求的web服务基础镜像足够小。\n大部分情况下，我们会选择Nginx作为我们的web服务器，一开始我也是这么选择的，因为社区在Docker Hub上为我们提供了开箱即用的容器镜像，下面来看看我用来构建静态web服务的过程。\nNginx On Alpine 我们知道在容器构建的实践中，我们可以选择基于AlpineLinux为分发系统的镜像，其比其他（例如 ubuntu, centos等）的镜像会小很多。因此一开始我们也是选择基于Alpine的nginx镜像，例如 nginx:1.22-alpine。\n$ docker image pull nginx:1.22-alpine $ docker image ls | grep nginx nginx 1.22-alpine 23.5MB 可以看到其大小为 23.5MB 。\n基于该惊醒构建我的博客的发布镜像\nFROM mengzyou/hugo:0.106 AS builder COPY --chown=hugo:hugo . /home/hugo/app RUN hugo FROM nginx:1.22-alpine COPY --from=builder /home/hugo/app/public/ /usr/share/nginx/html $ docker build -t myblog:nginx . $ docker image ls --format \u0026#34;{{.Repository}}\\t{{.Tag}}\\t{{.Size}}\u0026#34; | grep myblog myblog nginx 29MB 构建出来而最终交付镜像的大小为 29MB 。\nEasyhttpd On Alpine 后来，我发现了一个用GoLang编写的轻量级web服务器 - easyhttpd，于是我Fork了该项目，编写了一个Dockerfile来构建该web服务器的镜像，具体可查看该文件内容。\n镜像我已发布在mengzyou/easyhttpd。\n$ docker image ls --format \u0026#34;{{.Repository}}\\t{{.Tag}}\\t{{.Size}}\u0026#34; | grep easyhttpd mengzyou/easyhttpd v0.0.1 13.7MB 镜像大小为 13.7MB，比 nginx:alpine 的镜像小了十几MB。使用该镜像构建来构建我的博客站点\n... FROM mengzyou/easyhttpd:v0.0.1 COPY --from=builder --chown=http:www /home/hugo/app/public/ /srv/www $ docker image ls --format \u0026#34;{{.Repository}}\\t{{.Tag}}\\t{{.Size}}\u0026#34; | grep myblog myblog nginx 29MB myblog ehttpd 19.1MB 得到的应用镜像大小为 19.1MB ，进一步减少了应用的镜像大小。\nBusyBox Httpd 最近看到了一个国外的博客文章，可以构建一个只有 ~155KB 大小的web服务器镜像，我非常好奇，向着是否可以进一步减少我的静态站点的镜像大小。\n是使用了BusyBox内置的httpd来静态文件提供web服务。于是我也学习该作者创建了一个基于busybox - httpd的web服务器镜像，将其命名为 bbhttpd，具体的构建内容请参考Github仓库 - docker-bbhttpd。\n构建的镜像我也发布到Docker Hub - mengzyou/bbhttpd\n$ docker image ls --format \u0026#34;{{.Repository}}\\t{{.Tag}}\\t{{.Size}}\u0026#34; | grep bbhttpd mengzyou/bbhttpd 1.35 155kB 镜像大小确实只有 155KB，是不是挺惊人的？使用该镜像来构建我的站点\n... FROM mengzyou/bbhttpd:1.35 COPY --from=builder --chown=www:www /home/hugo/app/public/ /home/www/html docker image ls --format \u0026#34;{{.Repository}}\\t{{.Tag}}\\t{{.Size}}\u0026#34; | grep myblog myblog nginx 29MB myblog ehttpd 19.1MB myblog bbhttpd 5.64MB 最终的交付镜像大小只有 1.64MB，几乎也就是web服务静态文件的大小。\n总结 按照Docker容器镜像构建的最佳实践，我们应该尽量保持最小的经销大小，而减少镜像大小的一个方法就是选择足够小的基础镜像。因此我们在构建静态Web服务的时候，可以通过自己构建基础镜像的方式，大大减少最终的镜像大小。\n基础镜像 nginx:1.22-alpne mengzyou/easyhttpd:v0.0.1 mengzyou/bbhttpd:1.35 Size 23.5MB 13.7MB 155KB ","date":"November 20, 2022","hero":"/zh/posts/container-tech/smallest-web-container/docker-bp.jpg","permalink":"/zh/posts/container-tech/smallest-web-container/","summary":"在现代的B/S架构应用中，我们会做前后端分离，某些前端Web服务会将编译完成的静态文件放到一个web服务器进行部署。例如，我的博客也是基于Hugo编译的静态文件来进行部署的。\n那在容器化部署模式下，我们需要基于一个web服务的基础容器（镜像）将静态文件构建成站点或者Web服务的容器镜像来进行部署。在Docker开发最佳实践中，我们应该尽量保持镜像足够小（Size大小）。因此，我们应该尽量选择满足我们需求的web服务基础镜像足够小。\n大部分情况下，我们会选择Nginx作为我们的web服务器，一开始我也是这么选择的，因为社区在Docker Hub上为我们提供了开箱即用的容器镜像，下面来看看我用来构建静态web服务的过程。\nNginx On Alpine 我们知道在容器构建的实践中，我们可以选择基于AlpineLinux为分发系统的镜像，其比其他（例如 ubuntu, centos等）的镜像会小很多。因此一开始我们也是选择基于Alpine的nginx镜像，例如 nginx:1.22-alpine。\n$ docker image pull nginx:1.22-alpine $ docker image ls | grep nginx nginx 1.22-alpine 23.5MB 可以看到其大小为 23.5MB 。\n基于该惊醒构建我的博客的发布镜像\nFROM mengzyou/hugo:0.106 AS builder COPY --chown=hugo:hugo . /home/hugo/app RUN hugo FROM nginx:1.22-alpine COPY --from=builder /home/hugo/app/public/ /usr/share/nginx/html $ docker build -t myblog:nginx . $ docker image ls --format \u0026#34;{{.Repository}}\\t{{.Tag}}\\t{{.Size}}\u0026#34; | grep myblog myblog nginx 29MB 构建出来而最终交付镜像的大小为 29MB 。\nEasyhttpd On Alpine 后来，我发现了一个用GoLang编写的轻量级web服务器 - easyhttpd，于是我Fork了该项目，编写了一个Dockerfile来构建该web服务器的镜像，具体可查看该文件内容。","tags":["docker","web"],"title":"最小化静态WEB容器实践"},{"categories":["DEVOPS"],"contents":"轻量级Kubernetes集群-K3S文章介绍了一个轻量级的 Kubernetes 发行版本 - k3s 。\n这篇文章，我们将通过使用以下几个 IaC（Infrastructure as Code）工具，在本地环境（例如你的 Linux 工作台）自动化部署一个可用的 K3S 集群\nPacker - HashiCorp 开源的一个系统镜像构建工具。 Terraform - HashiCorp 开源的基础设施及代码自动化管理工具。 Ansible - RedHat赞助的一个开源社区项目，IT自动化配置工具。 环境需求 本演示将的所有操作将在一台支持虚拟化（kvm + qemu + libvirt) Linux 主机上执行。\n在 Ubuntu 上启用虚拟化环境，请参考 KVM hypervisor: a beginner\u0026rsquo;s guide 。\n在 Fedora 上启用虚拟化环境，请参考 Getting startyed with virtualization (libvirt) 。\n在 openSUSE 上启用虚拟化环境，请参考 Virtualization Guide 。\n其他 Linux 发行版，请参考相关文档。\n我是在我的笔记本电脑上执行的操作，系统是 openSUSE Leap 15.4 。\n除了上述的虚拟化需求外，还需要在系统上安装上面提到的几个工具。如果你的环境中有 LinuxBrew，则可通过 Brew 直接安装\n❯ brew install packer terraform ansible 否则，请下载各自官方发布的二进制包，解压后放到 PATH 路径中。\n❯ packer version Packer v1.8.3 ❯ terraform version Terraform v1.3.2 on linux_amd64 ❯ ansible --version ansible [core 2.13.4] 因为本示例中，使用了 Terraform 的 ansbile provisioner，因此还需要安装这个插件\n❯ mkdir -p ~/.terraform.d/plugins ❯ curl -sL \\ https://raw.githubusercontent.com/radekg/terraform-provisioner-ansible/master/bin/deploy-release.sh \\ --output /tmp/deploy-release.sh ❯ chmod +x /tmp/deploy-release.sh ❯ /tmp/deploy-release.sh -v 2.5.0 ❯ rm -rf /tmp/deploy-release.sh 完成以上需求之后，我们将开始执行如下步骤\n使用 Packer 创建一个基于 Debian 11.5.0 的虚拟机系统模板镜像，该镜像中，将会配置好 SSH 免密登陆密钥。 使用 Terraform 在本地虚拟环境中，创建出需要的虚拟机，并生成后续 ansible 配置虚拟机需要的 inventory 文件。 使用 Ansible 配置虚拟机节点安装 k3s 集群，以及演示应用。 以上所有步骤的代码在 （https://github.com/mengzyou/iac-examples）\n将代码克隆到本地\n❯ git clone https://github.com/mengzyou/iac-examples.git ❯ cd iac-example/k3scluster/ 创建虚拟机镜像 首先我们需要通过 Packer 创建一个虚拟机系统镜像，后续需要使用该镜像来创建虚拟机实例，需要的代码在 k3scluster/packer/ 目录下\n❯ cd packer/ ❯ tree . |-- Makefile |-- base.pkr.hcl `-- preseed |-- debian11.txt 这里通过 Makefile 来调用 packer 进行镜像的构建，和上传到虚拟化环境，注意以下变量配置\nLIBVIRT_HYPERVISOR_URI := \u0026#34;qemu:///system\u0026#34; LIBVIRT_TEMPLATES_IMAGES_POOL := \u0026#34;templates\u0026#34; LIBVIRT_TEMPLATES_IMAGES_POOL_DIR := \u0026#34;/var/lib/libvirt/images/templates\u0026#34; LIBVIRT_IMAGE_NAME := \u0026#34;debian11-5.qcow2\u0026#34; ROOT_PASSWORD := \u0026#34;rootPassword\u0026#34; $(eval SSH_IDENTITY=$(shell find ~/.ssh/ -name \u0026#39;id_*\u0026#39; -not -name \u0026#39;*.pub\u0026#39; | head -n 1)) 默认使用 ${HOME}/.ssh/id_rsa 的密钥对作为 SSH 免密访问的密钥，如果没有，请先创建一个。\n执行 make image 进行镜像的构建，以及在本地虚拟化环境创建名为 templates 的存储池，并将镜像上传到该存储池中，命名为 debian11-5.qcow2 的卷，具体的代码，请查看 Makefile 。\n❯ make image ... 完成之后，我们可以通过 virsh 命令查看镜像卷\n❯ sudo virsh vol-list --pool templates 名称 路径 ------------------------------------------------------------------------ debian11-5.qcow2 /var/lib/libvirt/images/templates/debian11-5.qcow2 补充: 在文件 base.pkr.hcl 中，对 iso 文件源的配置\niso_url = \u0026#34;https://mirrors.ustc.edu.cn/debian-cd/current/amd64/iso-cd/debian-11.5.0-amd64-netinst.iso\u0026#34; 配置网络地址时，在 packer 进行构建时，可能会下载 iso 文件超时而导致构建失败。可通过预先下载对应的 iso 文件到本地文件系统，然后将 iso_url 配置为本地路径，例如\niso_url = \u0026#34;/data/debian-11.5.0-amd64-netinst.iso\u0026#34; 这样可以避免由于网络问题导致构建失败。\n创建虚拟机实例 接下来，我们将使用 Terraform 创建并初始化集群所需要的虚拟机实例，进入 k3scluster/terraform/ 目录\n❯ cd ../terraform/ 该目录下包含了创建集群所需虚拟机资源的定义，首先看 provider.tf 文件\nterraform { required_providers { libvirt = { source = \u0026#34;dmacvicar/libvirt\u0026#34; version = \u0026#34;0.7.0\u0026#34; } } required_version = \u0026#34;\u0026gt;= 0.13\u0026#34; } provider \u0026#34;libvirt\u0026#34; { uri = var.libvirt_uri } 因为我们需要通过 libvirt 创建虚拟机，因此这里需要 dmacvicar/libvirt 的 Provider，该 Provider 的 uri 配置为变量 var.libvirt_uri，默认为 qemu:///system，也就是本地虚拟环境。\n其他需要的变量定义都放在 variables.tf 文件中。资源文件 vms.tf 定义了需要创建的资源，其中包括\nresource \u0026#34;libvirt_network\u0026#34; \u0026#34;network\u0026#34; { name = var.net_name mode = \u0026#34;nat\u0026#34; domain = var.net_domain addresses = [var.subnet] dhcp { enabled = true } dns { enabled = true local_only = true } } 创建一个 NAT 模式的虚拟网络，默认的网络地址为 192.168.123.0/24，可通过变量 net_domain 修改。\nresource \u0026#34;libvirt_volume\u0026#34; \u0026#34;disk\u0026#34; { count = length(var.vms) name = \u0026#34;${var.vms[count.index].name}.qcow2\u0026#34; pool = \u0026#34;default\u0026#34; base_volume_name = var.template_img base_volume_pool = var.templates_pool } 根据变量 vms 定义的虚拟机实例，创建虚拟机的系统磁盘，基于变量 templates_pool 和 template_image 指定的模板镜像，默认也就是上面我们通过 Packer 创建的系统镜像。\nresource \u0026#34;libvirt_domain\u0026#34; \u0026#34;vm\u0026#34; { count = length(var.vms) name = var.vms[count.index].name autostart = true qemu_agent = true vcpu = lookup(var.vms[count.index], \u0026#34;cpu\u0026#34;, 1) memory = lookup(var.vms[count.index], \u0026#34;memory\u0026#34;, 512) ... } } libvirt_domain 资源定义了需要创建的虚拟机实例，并通过 ansible provisioner 进行是初始化配置（配置静态IP地址和主机名）。\nresource \u0026#34;local_file\u0026#34; \u0026#34;ansible_hosts\u0026#34; { content = templatefile(\u0026#34;./tpl/ansible_hosts.tpl\u0026#34;, { vms = var.vms subnet = var.subnet gateway = cidrhost(var.subnet, 1) mask = cidrnetmask(var.subnet) nameserver = cidrhost(var.subnet, 1) user = var.user }) filename = \u0026#34;../ansible/k3s_hosts\u0026#34; file_permission = 0644 directory_permission = 0755 } 该资源定义通过模板文件创建虚拟机实例的 Ansible Inventory 文件，便于下一步通过 Ansible 进行 K3S 集群的创建。\n在应用之前，我们需要配置 vms 变量，来指定我们需要的虚拟机实例信息\n❯ cp .k3svms.tfvars k3dcluster.auto.tfvars vms = [ { name = \u0026#34;control\u0026#34; cpu = 1 memory = 1024 ip = 10 groups = [\u0026#34;k3s\u0026#34;] vars = { role = \u0026#34;server\u0026#34; } }, { name = \u0026#34;worker1\u0026#34; cpu = 1 memory = 1024 ip = 21 groups = [\u0026#34;k3s\u0026#34;] vars = { role = \u0026#34;agent\u0026#34; } }, { name = \u0026#34;worker2\u0026#34; cpu = 1 memory = 1024 ip = 22 groups = [\u0026#34;k3s\u0026#34;] vars = { role = \u0026#34;agent\u0026#34; } } ] 上面定义了3台实例，1台作为k3s集群的 server 节点，2台作为k3s集群的 role 节点，默认IP地址将会被配置为\ncontrol : 192.168.123.10 worker1 : 192.168.123.21 worker2 : 192.168.123.22 接下来我们将执行 Terrform 操作\n❯ terraform init ❯ terraform plan ❯ terrafrom apply --auto-approve ... Apply complete! Resources: 8 added, 0 changed, 0 destroyed. Outputs: vms_ip_addresses = { \u0026#34;control\u0026#34; = \u0026#34;192.168.123.10\u0026#34; \u0026#34;worker1\u0026#34; = \u0026#34;192.168.123.21\u0026#34; \u0026#34;worker2\u0026#34; = \u0026#34;192.168.123.22\u0026#34; } 完成之后，3台虚拟机将会创建并运行，同时在 k3scluster/ansible/ 目录中将创建名为 k3s_hosts 的 Inventory 文件。\n部署K3S集群 完成虚拟机的创建之后，我们进入 k3scluster/ansible/ 目录，进行下一步操作\n❯ cd ../ansible/ ❯ ls  apps.yml  init.yml  k3s.yaml  k3s_hosts  main.yml  roles 其中文件 k3s_hosts 是在上一步生成的 Inventory 文件，init.yml 文件是初始化节点的 playbook，在上一步的 Terraform 应用中以及执行了。\nmain.yml 文件是安装配置 K3S 集群的 playbook，roles/ 目录包含了所有的任务。\n在执行具体任务之前，我们可以通过 ansbile 测试下虚拟机节点的可用性\n❯ ansible -i k3s_hosts all -m ping worker1 | SUCCESS =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } worker2 | SUCCESS =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } control | SUCCESS =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } 接下来执行 main.yml playbook\n❯ ansible-playbook -i k3s_hosts main.yml ... PLAY RECAP ******************************************** control : ok=16 changed=7 unreachable=0 failed=0 skipped=4 rescued=0 ignored=0 worker1 : ok=8 changed=4 unreachable=0 failed=0 skipped=6 rescued=0 ignored=0 worker2 : ok=8 changed=4 unreachable=0 failed=0 skipped=6 rescued=0 ignored=0 这将会调用 roles/k3s/ 里定义的任务，安装和配置 K3S 集群，具体的执行任务，请查看 roles 里的代码。\n成功之后，会发现在当前目录生成了一个 k3s.yaml 的文件，这是从 control 节点获取的 kubeconfig 文件，我们需要替换一下 api-server 的 IP\n❯ sed -i \u0026#39;s/127.0.0.1/192.168.123.10/g\u0026#39; k3s.yaml 之后，我们就可以通过该 kubeconfig 文件来访问该集群了，例如\n❯ kubectl --kubeconfig k3s.yaml cluster-info Kubernetes control plane is running at https://192.168.123.10:6443 CoreDNS is running at https://192.168.123.10:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.168.123.10:6443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy ❯ kubectl --kubeconfig k3s.yaml get no NAME STATUS ROLES AGE VERSION worker2.k3s.local Ready \u0026lt;none\u0026gt; 4m19s v1.25.2+k3s1 worker1.k3s.local Ready \u0026lt;none\u0026gt; 4m18s v1.25.2+k3s1 control.k3s.local Ready control-plane,master 4m39s v1.25.2+k3s1 至此，我们就完成了一个 K3S 集群的部署，并可以在部署其他应用。最后，我们也可以继续使用 ansible 来部署演示应用。\n文件 apps.yml 是部署演示应用的 playbook，其通过 roles/k3s-app/ 任务，与 k3s server 节点交互来进行应用部署，其会部署 Traefik Ingress 和一个 whoami web 应用，直接执行\n❯ ansible-playbook -i k3s_hosts apps.yml ... PLAY RECAP ***************************************************** control : ok=4 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 完成之后，通过 kubectl 命令查看部署的 pod\n❯ kubectl --kubeconfig k3s.yaml get po -n whoami NAME READY STATUS RESTARTS AGE whoami-5b844ffb57-mffgf 1/1 Running 0 79s ❯ kubectl --kubeconfig k3s.yaml get ingressroute -n whoami NAME AGE whoami 2m12s 尝试访问 whoami 应用\n❯ http http://192.168.123.10/ HTTP/1.1 200 OK Content-Length: 413 Content-Type: text/plain; charset=utf-8 Date: Thu, 13 Oct 2022 08:58:06 GMT Hostname: whoami-5b844ffb57-mffgf IP: 127.0.0.1 IP: ::1 IP: 10.42.2.3 IP: fe80::b83a:19ff:febe:7a7f RemoteAddr: 10.42.0.5:60580 GET / HTTP/1.1 Host: 192.168.123.10 User-Agent: HTTPie/3.2.1 Accept: */* Accept-Encoding: gzip, deflate X-Forwarded-For: 192.168.123.1 X-Forwarded-Host: 192.168.123.10 X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: traefik-rjbr9 X-Real-Ip: 192.168.123.1 销毁集群 通过以上方式创建的 K3S 集群，我们可以很方便的通过 Terraform 销毁并重新创建。当完成了相关的应用测试之后，我们可以通过以下命令销毁集群\n❯ rm -f k3s.yaml ❯ cd ../terraform/ ❯ terraform destroy --auto-approve ... Destroy complete! Resources: 8 destroyed. 我就将会删除所创建的所有相关资源，恢复干净的本地环境。\n当需要集群的时候，只需要执行上面的步骤就可以创建一个新的 K3S 集群。\n总结 这里演示了一个 IaC 场景，通过代码化基础设施资源，我们可以很容易地通过 Terraform，Ansible 等工具管理并维护相应的基础设施资源。\n这里我们演示在本地虚拟化环境创建虚拟机并部署k3S集群，那通过 Terraform 的其他 Providers (例如 AWS, GCP等共有云)，我们可以代码化管理我们的公有云基础设施环境，并可以将相应的流程加入 CI/CD 中，可快速创建需要的环境做测试。\nIaC 是现代化基础设施运维的方向，结合相关工具，我们可以轻松实现基础设施自动化运维。\n","date":"October 13, 2022","hero":"/zh/posts/devops/k3s-terraform-ansible/k3s-terraform-ansible.jpeg","permalink":"/zh/posts/devops/k3s-terraform-ansible/","summary":"轻量级Kubernetes集群-K3S文章介绍了一个轻量级的 Kubernetes 发行版本 - k3s 。\n这篇文章，我们将通过使用以下几个 IaC（Infrastructure as Code）工具，在本地环境（例如你的 Linux 工作台）自动化部署一个可用的 K3S 集群\nPacker - HashiCorp 开源的一个系统镜像构建工具。 Terraform - HashiCorp 开源的基础设施及代码自动化管理工具。 Ansible - RedHat赞助的一个开源社区项目，IT自动化配置工具。 环境需求 本演示将的所有操作将在一台支持虚拟化（kvm + qemu + libvirt) Linux 主机上执行。\n在 Ubuntu 上启用虚拟化环境，请参考 KVM hypervisor: a beginner\u0026rsquo;s guide 。\n在 Fedora 上启用虚拟化环境，请参考 Getting startyed with virtualization (libvirt) 。\n在 openSUSE 上启用虚拟化环境，请参考 Virtualization Guide 。\n其他 Linux 发行版，请参考相关文档。\n我是在我的笔记本电脑上执行的操作，系统是 openSUSE Leap 15.4 。\n除了上述的虚拟化需求外，还需要在系统上安装上面提到的几个工具。如果你的环境中有 LinuxBrew，则可通过 Brew 直接安装\n❯ brew install packer terraform ansible 否则，请下载各自官方发布的二进制包，解压后放到 PATH 路径中。","tags":["terraform","ansible","k3s"],"title":"IaC示例-TERRAFORM\u0026ANSIBLE创建K3S集群"},{"categories":["Kubernetes"],"contents":"K3S 是 Rancher 为物联网（IoT）和边缘计算环境开发的轻量级 Kubernetes 发行版本。相比原生的 Kubernetes，其移除了很多非必要的组件，例如云控制管理器（CCM）、内置的（In-Tree）的存储插件等，以及为ARM架构的基础设施做了优化。\nK3s 的轻量级同时也体现在其打包成一个二进制可执行文件进行分发，状态存储除了支持 etcd 外，还支持 Sqlite3、MySQl和Postgres。其跟多特性可参考官方文档。\nK3s 支持单节集群部署（可用于开发测试环境），也支持高可用的多节点集群。同时还可以通过 k3d 项目快速在本地开发环境使用Docker容器部署 k3s 集群作为开发环境。\n这里我将演示通过虚拟机部署一个高可用的多节点集群（3个Servers节点 + 3个Agent节点）。\nk3S架构 上图是来自k3s官网的架构图，其架构与Kubernetes的架构是相似的，k3s的server节点也就是控制面节点，agent节点是工作负载节点。k3s默认使用 containerd 作为容器运行时。\n更信息的部署架构可参考官方文档。\n准备虚拟机节点 这里我们将部署 3 + 3 的集群，需要6台虚拟机，基本配置如下\n主机名 IP vCPU 内存 homek3s-server1 192.168.0.150 1 2 GB homek3s-server2 192.168.0.151 1 2 GB homek3s-server3 192.168.0.152 1 2 GB homek3s-agent1 192.168.0.154 2 4 GB homek3s-agent2 192.168.0.155 2 4 GB homek3s-agent3 192.168.0.156 2 4 GB 部署的最小需求，可参考官方文档。\nK3s 支持大部分主流的Linux操作系统，这里我使用的是 openSUSE Leap Micro 15.2，其是一个基于openSUSE，为容器负载而设计的操作系统。\n为了部署简单，这里我们禁用了系统的防火墙，如果开启防火墙，需要为Server节点开放如下端口\n6443/TCP - Kubernetes API 服务 8472/UDP - Flannel VXLAN模式需要 51820/UDP - Flannel Wireguard后端需要 10250/TCP - Kubelet metrics需要 2379-2380/TCP - 基于内嵌etcd高可用部署模式需要 启动Server节点 首先登陆到第一个Server节点 homek3s-server1，然后下载最新版本（v1.24.3+k3s1）的 k3s 二进制文件\n# curl -sfL https://github.com/k3s-io/k3s/releases/download/v1.24.3%2Bk3s1/k3s -o /usr/local/bin/k3s # chmod +x k3s # k3s --version k3s version v1.24.3+k3s1 (990ba0e8) go version go1.18.1 k3s 支持一下几个子命令\nk3s server - 用于运行管理服务节点 k3s agent - 用于运行agent工作节点 k3s kubectl - 运行 kubectl 命令 k3s crictl - 运行 crictl 容器管理命令 其他的命令帮助，请通过 k3s --help 查看。\n这里我们将要创建一个使用内置etcd数据库的高可用集群，执行如下命令\n# k3s server --cluster-init --advertise-address=192.168.0.150 --tls-san=homek3s.mengz.lan --write-kubeconfig-mode=644 参数 \u0026ndash;cluster-init 是使用内置的etcd初始户一个新的集群;\n参数 \u0026ndash;advertise-address 是指定API服务器的监听IP地址，如果不指定，默认为节点的IP地址;\n参数 \u0026ndash;tls-san 是指定额外的域名或者IP地址作为TLS证书的SAN，使得我们从客户端可通过域名访问而API服务器。\n运行成功后，在 homek3s-server1 打开另一个终端执行\n# k3s kubectl get no NAME STATUS ROLES AGE VERSION homek3s-server1 Ready control-plane,etcd,master 27h v1.24.3+k3s1 可以看到，集群中以及运行了一个节点，不过这时k3s服务是启动在前台的，我们需要配置一个 systemd 服务，让其以服务形式运行。\n创建文件 /etc/systemd/system/k3s.service，内如如下\n[Unit] Description=Lightweight Kubernetes Documentation=https://k3s.io After=network-online.target Wants=network-online.target [Service] Type=notify EnvironmentFile=-/etc/default/%N EnvironmentFile=-/etc/sysconfig/%N EnvironmentFile=-/etc/systemd/system/k3s.service.env ExecStartPre=/bin/sh -xc \u0026#39;! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service\u0026#39; ExecStart=/usr/local/bin/k3s server KillMode=process Delegate=yes # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity TimeoutStartSec=0 Restart=always RestartSec=5s [Install] WantedBy=multi-user.target 终止刚才运行的k3s进程，然后执行\n# systemctl enable --now k3s.server # systemctl status k3s ● k3s.service - Lightweight Kubernetes Loaded: loaded (/etc/systemd/system/k3s.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2022-07-22 03:31:21 CST; 18h ago Docs: https://k3s.io Process: 1121 ExecStartPre=/bin/sh -xc ! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service (code=exited, status=0/SUCCESS) Main PID: 1129 (k3s-server) Tasks: 120 CGroup: /system.slice/k3s.service ... 这是后k3s进程将以服务的形式运行在后台了。\n执行完上述步骤后，k3s将会生产集群的管理员 kubeconfig 文件 - /etc/rancher/k3s/k3s.yaml，可将该文件拷贝的本地环境，修改 server 内容\nserver: https://127.0.0.1:6443 server: https://192.168.0.150:6443\n这样就可以在本地主机上使用 kubectl 访问集群了。\n接下来，我们将在其他Server节点上启动k3s，并作为管理节点加入集群，首先在第一个节点 homek3s-server1 上获取到集群的Token\n# cat /var/lib/rancher/k3s/server/node-token K1087c3ff53c94a3c3b20475e84602f1e6d46f1b3903f2979f144800990897b06ac::server:fb8e7aa7ba23d7652942b09cb440ba24 SSH登陆其他Server节点（homek3s-server2 和 homek3s-server3），执行如下步骤\n下载相同版本的 k3s 二进制文件到 /usr/local/bin/k3s\n执行如下命令加入集群\n# k3s server --server=https://192.168.0.150:6443 --token=${NODE_TOKEN} 其中 ${NODE_TOKEN} 是上面从第一个节点获取的 Token 内容。\n如第一个节点一样，配置 etc/systemd/system/k3s.service，将k3s进程启动以服务方式启动。 完成上述步骤后，通过 kubectl 查看节点\n❯ k get no NAME STATUS ROLES AGE VERSION homek3s-server1 Ready control-plane,etcd,master 28h v1.24.3+k3s1 homek3s-server2 Ready control-plane,etcd,master 27h v1.24.3+k3s1 homek3s-server3 Ready control-plane,etcd,master 27h v1.24.3+k3s1 启动Agent节点 在启动完3个管理节点（Server）的集群之后，我接下来继续添加工作节点（Agent）到集群中，登陆到3个Agent节点之下如下步骤（3个节点上的操作相同）\n下载相同版本的 k3s 二进制文件到 /usr/local/bin/k3s （方法与Server节点上一样）\n执行如下命令，以工作节点加入集群\n# k3s agent --server=https://192.168.0.150:6443 --token=${NODE_TOKEN} ${NODE_TOKEN} 是上面获取的节点Token值。\n执行成功之后，也需要将 k3s 的 agent 进程以服务方式启动，这里与Server节点有些不同，创建文件 /etc/systemd/system/k3s-agent.service，内容如下\n[Unit] Description=Lightweight Kubernetes Documentation=https://k3s.io After=network-online.target Wants=network-online.target [Service] Type=notify EnvironmentFile=-/etc/default/%N EnvironmentFile=-/etc/sysconfig/%N EnvironmentFile=-/etc/systemd/system/k3s-agent.service.env ExecStartPre=/bin/sh -xc \u0026#39;! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service\u0026#39; ExecStart=/usr/local/bin/k3s agent --server=${K3S_URL} --token=${K3S_TOKEN} KillMode=process Delegate=yes # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity TimeoutStartSec=0 Restart=always RestartSec=5s [Install] WantedBy=multi-user.target 然后再创建文件 /etc/systemd/system/k3s-agent.service.env，内容如下\nK3S_URL=\u0026#34;https://192.168.0.150:6443\u0026#34; K3S_TOKEN=\u0026#34;K1087c3ff58c94a3c3b20475e84602f8e6d46f1b3903f2979f144800990897b06ac::server:fb8e7aa7ba23d7652942b09cb440ba24\u0026#34; 注意，将以上的 K3S_TOKEN 替换成你的值。\n然后执行\n# sysetmctl enable --now k3s-agent.service 3个Agent节点都启动完成之后，使用 kubectl 查看节点\n❯ k get no NAME STATUS ROLES AGE VERSION homek3s-agent1 Ready \u0026lt;none\u0026gt; 27h v1.24.3+k3s1 homek3s-agent2 Ready \u0026lt;none\u0026gt; 26h v1.24.3+k3s1 homek3s-agent3 Ready \u0026lt;none\u0026gt; 26h v1.24.3+k3s1 homek3s-server1 Ready control-plane,etcd,master 28h v1.24.3+k3s1 homek3s-server2 Ready control-plane,etcd,master 27h v1.24.3+k3s1 homek3s-server3 Ready control-plane,etcd,master 27h v1.24.3+k3s1 可以看到，我们以及通过K3S部署了一个3+3的高可用Kubernetes集群。\n部署应用 成功启动一个K3S集群，除了Kubernetes必要的组件之外，还自动为集群部署以下组件\nFlennel作为CNI插件 rancher-local-path 作为默认的存储类插件 Traefik 作为 Ingress 控制器 kipper Load Balancer 作为服务的负载均均衡控制器 Traefik的Load Balancer类型的服务就直接通过每个节点的 80 和 443 端口暴露了\n❯ k get svc -n kube-system traefik LoadBalancer 10.43.66.163 192.168.0.150,192.168.0.151,192.168.0.152,192.168.0.154,192.168.0.155,192.168.0.156 80:30833/TCP,443:32747/TCP 28h 因此，我们可以直接创建应用，并通过Ingress向外暴露服务，这里我们创建一个简单的Web服务，也就是 Docker的教程，创建如下资源文件 docker-tour.yaml\n--- apiVersion: apps/v1 kind: Deployment metadata: labels: com.docker.project: tutorial name: tutorial spec: replicas: 1 selector: matchLabels: com.docker.project: tutorial strategy: type: Recreate template: metadata: labels: com.docker.project: tutorial spec: containers: - image: docker/getting-started name: tutorial ports: - containerPort: 80 protocol: TCP resources: {} restartPolicy: Always --- apiVersion: v1 kind: Service metadata: name: tutorial spec: ports: - name: 80-tcp port: 80 protocol: TCP targetPort: 80 selector: com.docker.project: tutorial type: ClusterIP --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: tutorial labels: com.docker.project: tutorial spec: # ingressClassName: contour rules: - host: \u0026#34;tour.homek3s.lan\u0026#34; http: paths: - pathType: Prefix path: \u0026#34;/\u0026#34; backend: service: name: tutorial port: number: 80 然后执行\n❯ kubectl apply -f docker-tour.yaml 在你的内部DNS服务器，或者 /etc/hosts 中配置域名解析，将 tour.homek3s.lan 解析到任何一个节点的IP地址，如\n192.168.0.150 tour.homek3s.lan 通过浏览器访问 https://tour.homek3s.lan/ ，将打开部署的应用。\n总结 这里演示了如果通过K3S部署一个高可用的Kubernetes集群，使用k3s内置etcd的方式作为入门，k3s还支持使用外部的数据存储以及其他更多的部署选项，可参考官方文档。K3s作为一个轻量级的kubernetes版本，以了单一的二进制文件 - k3s进行分发，可以快速部署开发测试，以及边缘生产级别的集群。\n后期将会继续探索另一款开源的轻量级Kubeernets版本 - K0S。\n","date":"July 22, 2022","hero":"/zh/posts/k8s/lightweight-k3s/k3s-kubernetes.png","permalink":"/zh/posts/k8s/lightweight-k3s/","summary":"K3S 是 Rancher 为物联网（IoT）和边缘计算环境开发的轻量级 Kubernetes 发行版本。相比原生的 Kubernetes，其移除了很多非必要的组件，例如云控制管理器（CCM）、内置的（In-Tree）的存储插件等，以及为ARM架构的基础设施做了优化。\nK3s 的轻量级同时也体现在其打包成一个二进制可执行文件进行分发，状态存储除了支持 etcd 外，还支持 Sqlite3、MySQl和Postgres。其跟多特性可参考官方文档。\nK3s 支持单节集群部署（可用于开发测试环境），也支持高可用的多节点集群。同时还可以通过 k3d 项目快速在本地开发环境使用Docker容器部署 k3s 集群作为开发环境。\n这里我将演示通过虚拟机部署一个高可用的多节点集群（3个Servers节点 + 3个Agent节点）。\nk3S架构 上图是来自k3s官网的架构图，其架构与Kubernetes的架构是相似的，k3s的server节点也就是控制面节点，agent节点是工作负载节点。k3s默认使用 containerd 作为容器运行时。\n更信息的部署架构可参考官方文档。\n准备虚拟机节点 这里我们将部署 3 + 3 的集群，需要6台虚拟机，基本配置如下\n主机名 IP vCPU 内存 homek3s-server1 192.168.0.150 1 2 GB homek3s-server2 192.168.0.151 1 2 GB homek3s-server3 192.168.0.152 1 2 GB homek3s-agent1 192.168.0.154 2 4 GB homek3s-agent2 192.168.0.155 2 4 GB homek3s-agent3 192.168.0.156 2 4 GB 部署的最小需求，可参考官方文档。\nK3s 支持大部分主流的Linux操作系统，这里我使用的是 openSUSE Leap Micro 15.","tags":["contianer","kubernetes","cluster"],"title":"轻量级Kubernetes集群-K3S"},{"categories":["Kubernetes"],"contents":"随着 Kubernetes 成为主流的应用容器编排平台，其命令行客户端 kubectl 也成为了我们日常部署应用，维护集群最常用的工具。\nkubectl 自身提供了强大的内置自命令来满足我们对集群的操作，例如 get 获取集群内的资源对象，proxy 创建代理之类的，除了内置的这些自命令，kubectl 还提供了可扩展的能力，允许我们安装自己编写或者社区提供的插件来增强我们使用 kubectl 的生产力。\n这里将给大家介绍如何在安装 kubectl 扩展插件，以及几款我在日常工作中常用到的社区提供的插件。\n在安装和使用 kubectl 插件的之前，请确保以及安装和配置好 kubectl 命令行工具和 git 工具。\nkrew 首先介绍的第一款扩展插件就是 krew - k8s特别兴趣小组开发的一款用于安装和管理 kubectl 扩展插件的插件。\n代码： https://github.com/kubernetes-sigs/krew\n安装 krew (在macOS/Linux上):\n在终端执行（Bash或者Zsh）执行 ( set -x; cd \u0026#34;$(mktemp -d)\u0026#34; \u0026amp;\u0026amp; OS=\u0026#34;$(uname | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39;)\u0026#34; \u0026amp;\u0026amp; ARCH=\u0026#34;$(uname -m | sed -e \u0026#39;s/x86_64/amd64/\u0026#39; -e \u0026#39;s/\\(arm\\)\\(64\\)\\?.*/\\1\\2/\u0026#39; -e \u0026#39;s/aarch64$/arm64/\u0026#39;)\u0026#34; \u0026amp;\u0026amp; KREW=\u0026#34;krew-${OS}_${ARCH}\u0026#34; \u0026amp;\u0026amp; curl -fsSLO \u0026#34;https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\u0026#34; \u0026amp;\u0026amp; tar zxvf \u0026#34;${KREW}.tar.gz\u0026#34; \u0026amp;\u0026amp; ./\u0026#34;${KREW}\u0026#34; install krew ) 将 $HOME/.krew/bin 加入到 PATH 环境变量，更新你的 .bashrc 或者 .zshrc 文件，添加下面一行 export PATH=\u0026#34;${KREW_ROOT:-$HOME/.brew}/bin:$PATH\u0026#34; 然后重启你的终端。\n测试 krew 已经安装成功 ❯ k krew version OPTION VALUE GitTag v0.4.3 GitCommit dbfefa5 IndexURI https://github.com/kubernetes-sigs/krew-index.git BasePath /home/mengz/.krew IndexPath /home/mengz/.krew/index/default InstallPath /home/mengz/.krew/store BinPath /home/mengz/.krew/bin DetectedPlatform linux/amd64 （可选）设置 krew 别名 alias krew=\u0026#39;kubectl-krew\u0026#39; echo \u0026#34;alias krew=\u0026#39;kubectl-krew\u0026#39;\u0026#34; \u0026gt;\u0026gt; ~/.alias 这样就安装完成了，krew 是 kubectl 插件管理器，而 krew 自己又是插件，所以之后可以使用 krew 来更新 krew。\n在 Windows 上安装，请参考 https://krew.sigs.k8s.io/docs/user-guide/setup/install/ 。\n使用示例\n列出当前已安装的插件 ❯ krew list PLUGIN VERSION krew v0.4.3 安装插件（下面介绍的其他插件将使用这个方式安装） ❯ krew install [插件名] 更新本地插件索引（查看是否有插件更新） ❯ krew update Updated the local copy of plugin index. New plugins available: * liqo * switch-config Upgrades available for installed plugins: * open-svc v2.5.2 -\u0026gt; v2.5.3 * rbac-tool v1.7.1 -\u0026gt; v1.8.0 * rolesum v1.5.1 -\u0026gt; v1.5.5 升级插件 ❯ krew upgrade open-svc Updated the local copy of plugin index. Upgrading plugin: open-svc 删除插件 ❯ krew uninstall [插件名] krew-index 里维护了上百款可通过 krew 直接安装的插件。接下来将介绍其他几款我日常使用的 kubectl 插件。\nexample 代码： https://github.com/seredot/kubectl-example\n安装 krew install example\nexample 插件可用于快速生成k8s资源对象的yaml文件示例，例如\n❯ k example deploy --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 当我们想临时通过资源声明文件不熟应用的时候，可以使用该插件来生成模板。\nfleet 代码： https://github.com/kubectl-plus/kcf\n安装： krew install fleet\nfleet 插件可快速查看当前配置的集群的概览\n❯ k fleet CLUSTER VERSION NODES NAMESPACES PROVIDER API homek8sc1 v1.23.8 3/3 4 ? https://k8sc1.mengz.lan:6443 api-sandbox-x8i5-p1-openshiftapps-com:6443 v1.23.5+9ce5071 ? ? ? https://api.sandbox.x8i5.p1.openshiftapps.com:6443 minikube v1.22.3 1/1 4 minikube https://192.168.64.3:8443 get-all 代码： https://github.com/corneliusweig/ketall\n安装： krew install get-all\nget-all 插件获取集群（或者某一个名字空间）的所有资源对象\n❯ k get-all -n kube-system NAME NAMESPACE AGE configmap/calico-config kube-system 13d configmap/coredns kube-system 13d configmap/extension-apiserver-authentication kube-system 13d configmap/kube-proxy kube-system 13d configmap/kube-root-ca.crt kube-system 13d configmap/kubeadm-config kube-system 13d configmap/kubelet-config-1.23 kube-system 13d endpoints/kube-dns kube-system 13d ... htpasswd 代码： https://github.com/shibumi/kubectl-htpasswd\n安装： krew install htpasswd\nhtpasswd 是 nginx-ingress 兼容的基础认证密码生成器\n❯ k htpasswd create aaa-basic-auth user1=user1password user2=user2password -o yaml --dry-run apiVersion: v1 data: auth: dXNlcjE6JDJhJDEwJDVNeEJGT3lEUEJYT0xkUldlblNWME91RGtZTzFQOElJNXJuRnh5blpUdC55L2FUUUNDYzJ1CnVzZXIyOiQyYSQxMCRVbFdHOG5NTU4zRGVpOC5GMmVRM3EuYWhxTENYZGtLYUJ1cXZzT3lEOGl0ODJRdU4zV1c1dQ== kind: Secret metadata: creationTimestamp: null name: aaa-basic-auth namespace: default type: Opaque 该插件在为服务创建基础认证的 Ingress 很方便。\nimages 代码： https://github.com/chenjiandongx/kubectl-images\n安装： krew install images\nimages 现实名字空间中使用的容器镜像信息\n❯ k images [Summary]: 1 namespaces, 2 pods, 2 containers and 1 different images +------------------------+---------------+-------------------+ | PodName | ContainerName | ContainerImage | +------------------------+---------------+-------------------+ | webapp-98f7444c5-8772w | nginx | nginx:1.21-alpine | +------------------------+ + + | webapp-98f7444c5-vsxr9 | | | +------------------------+---------------+-------------------+ ktop 代码： https://github.com/vladimirvivien/ktop\n安装： krew install ktop\nktop 插件是以类似 Linux Top 工具的方式来插件k8s集群的负载情况\n❯ k ktop ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ API server: https://k8sc1.mengz.lan:6443 Version: v1.23.8 context: admin@homek8sc1 User: kubernetes-admin namespace: (all) metrics: not connected v0.3.0 └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ╔ 🌡 Cluster Summary ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ║ ptime: 13d Nodes: 3 Namespaces: 4 Pods: 15/15 (20 imgs) Deployments: 5/5 Sets: replicas 5, daemons 6, stateful 0 Jobs: 0 (cron: 0) PVs: 0 (0Gi) PVCs: 0 (0Gi) ║ PU: [||||||||||| ] 1600m/6000m (26.7% requested) Memory: [|| ] 1Gi/11Gi (2.5% requested) ╚ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ═ ┌ 🏭 Nodes (3) ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ NAME STATUS AGE VERSION INT/EXT IPs OS/ARC PODS/IMGs DISK CPU MEM │ homek8sc1-control Ready 13d v1.23.8 192.168.0.140/\u0026lt;none\u0026gt; Ubuntu 20.04.4 LTS/amd64 9/10 16Gi [|||||| ] 1100m/2000m (55%) [|| ] 1Gi/2Gi (13%) │ homek8sc1-worker1 Ready 13d v1.23.8 192.168.0.141/\u0026lt;none\u0026gt; Ubuntu 20.04.4 LTS/amd64 3/5 16Gi [|| ] 250m/2000m (12%) [ ] 0Gi/5Gi (0%) │ homek8sc1-worker2 Ready 13d v1.23.8 192.168.0.142/\u0026lt;none\u0026gt; Ubuntu 20.04.4 LTS/amd64 3/5 16Gi [|| ] 250m/2000m (12%) [ ] 0Gi/5Gi (0%) │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ 📦 Pods (15) ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── │ AMESPACE POD READY STATUS RESTARTS AGE VOLS IP NODE CPU MEMORY │ ube-system kube-controller-manager-homek8sc1-control 1/1 Running 1 13d 8/8 192.168.0.140 homek8sc1-control [| ] 200m 10.0% [ ] 0Gi 0.0% │ ube-system kube-proxy-4c9nq 1/1 Running 2 13d 4/4 192.168.0.141 homek8sc1-worker1 [ ] 0m 0.0% [ ] 0Gi 0.0% │ ube-system kube-proxy-4whcn 1/1 Running 1 13d 4/4 192.168.0.140 homek8sc1-control [ ] 0m 0.0% [ ] 0Gi 0.0% │ ube-system kube-proxy-bz8lt 1/1 Running 3 13d 4/4 192.168.0.142 homek8sc1-worker2 [ ] 0m 0.0% [ ] 0Gi 0.0% │ ube-system kube-scheduler-homek8sc1-control 1/1 Running 1 13d 1/1 192.168.0.140 homek8sc1-control [| ] 100m 5.0% [ ] 0Gi 0.0% └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ 总结 krew-index 里包含了大量功能强大的扩展插件，涵盖了 RBAC 管理，资源管理等功能，这里没法意义列出。不过有了 krew ，我们可以很方便的安装需要的扩展，以在日常工作中提高我们管理k8s集群的的生产力。\n参考\nhttps://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/ https://krew.sigs.k8s.io/docs/user-guide/quickstart/ ","date":"July 4, 2022","hero":"/zh/posts/k8s/kubectl-plugins/krew-banner.png","permalink":"/zh/posts/k8s/kubectl-plugins/","summary":"随着 Kubernetes 成为主流的应用容器编排平台，其命令行客户端 kubectl 也成为了我们日常部署应用，维护集群最常用的工具。\nkubectl 自身提供了强大的内置自命令来满足我们对集群的操作，例如 get 获取集群内的资源对象，proxy 创建代理之类的，除了内置的这些自命令，kubectl 还提供了可扩展的能力，允许我们安装自己编写或者社区提供的插件来增强我们使用 kubectl 的生产力。\n这里将给大家介绍如何在安装 kubectl 扩展插件，以及几款我在日常工作中常用到的社区提供的插件。\n在安装和使用 kubectl 插件的之前，请确保以及安装和配置好 kubectl 命令行工具和 git 工具。\nkrew 首先介绍的第一款扩展插件就是 krew - k8s特别兴趣小组开发的一款用于安装和管理 kubectl 扩展插件的插件。\n代码： https://github.com/kubernetes-sigs/krew\n安装 krew (在macOS/Linux上):\n在终端执行（Bash或者Zsh）执行 ( set -x; cd \u0026#34;$(mktemp -d)\u0026#34; \u0026amp;\u0026amp; OS=\u0026#34;$(uname | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39;)\u0026#34; \u0026amp;\u0026amp; ARCH=\u0026#34;$(uname -m | sed -e \u0026#39;s/x86_64/amd64/\u0026#39; -e \u0026#39;s/\\(arm\\)\\(64\\)\\?.*/\\1\\2/\u0026#39; -e \u0026#39;s/aarch64$/arm64/\u0026#39;)\u0026#34; \u0026amp;\u0026amp; KREW=\u0026#34;krew-${OS}_${ARCH}\u0026#34; \u0026amp;\u0026amp; curl -fsSLO \u0026#34;https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\u0026#34; \u0026amp;\u0026amp; tar zxvf \u0026#34;${KREW}.tar.gz\u0026#34; \u0026amp;\u0026amp; .","tags":["kubernetes","kubectl"],"title":"扩展你的KUBECTL功能"},{"categories":["Container"],"contents":"Compose V2 项目启动于2021年6月，直到2022年4月26号，发布了GA版本。在发布GA版本后，社区也宣布对于Compose V1将不会再进行功能更新，将在6个月后结束生命周期（EOL），期间会进行关键的安全和错误修复。\nV1与V2的兼容对比 确保 V1 和 V2 之间的兼容性对于日常工作流程至关重要，下面是V2中两个关键的更改\n更改 潜在影响 迁移 V2原生支持BuildKit，并且默认开启 开发者在V2中将默认使用BuildKit进行镜像构建 可通过设置环境变量不使用 DOCKER_BUILDKIT=0 容器名字中使用 - 替代了 _ 作为分隔符 如果在脚本中使用了容器名字，这可能会导致错误 可以通过 \u0026ldquo;\u0026ndash;compatibility\u0026rdquo; 标记来关闭此更改 关于更多的兼容性更改，请查看兼容性文档\n如何安装Compose V2 Windows，MacOS和Linux上使用Docker Desktop，就自带了Compose V2，可通过命令 docker compose 执行。也可以通过配置“Use Docker Compose V2“来设置 docker-compose 别名到 docker compose。\n如果没有使用Docker Desktop for Linux，而是直接使用的Docker Engine，则需要额外安装 docker-compose-plugin 或者独立的二进制包。\n例如对于Ubuntu,可以通过Docker官方的APT源直接安装\n❯ sudo apt update ❯ sudo apt install docker-compose-plugin 其他Linux, 例如在我的 openSUSE 上，通过手动从Github下载二进制文件进行安装（注意选择版本和平台架构）\n❯ DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker} ❯ mkdir -p $DOCKER_CONFIG/cli-plugins ❯ wget https://github.com/docker/compose/releases/download/v2.6.0/docker-compose-linux-x86_64 ❯ mv docker-compose-linux-x86_64 $DOCKER_CONFIG/cli-plugins/docker-compose ❯ chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose 执行一下命令测试\n❯ docker compose version Docker Compose version 2.6.0 更多安装方式，请查看官方文档。\nCompose V2的优势 在Docker CLI中快速交付新功能 支持GPU主机 - 如果Docker主机有GPU设备（显卡)并且Docker引擎进行了相关配置, 则Compose服务可以定义GPU设备的预留。 支持配置服务启用（Profiles） - 通过选择性服务器启用来为多种用途和环境启动Compose应用模型，如下Compose文件 version: \u0026#34;3.9\u0026#34; services: frontend: image: frontend profiles: [\u0026#34;frontend\u0026#34;] phpmyadmin: image: phpmyadmin depends_on: - db profiles: - debug backend: image: backend db: image: mysql 默认执行 docker compose up 将只会启动 backend 和 db 服务，要启动相应配置的服务，需要使用 --profile 标记或者设置环境变量 COMPOSE_PROFILES，例如\n❯ docker compose --profile debug --profile frontend up ❯ COMPOSE_PROFILES=frontend,debug docker compose up 新增了 cp 命令 - 在服务容器和本地文件系统直接拷贝文件和目录 新增了 ls 命令 - 列出当前环境中的Compose项目（应用栈） 开发到生产的无缝转换 通过云集成项目，可以容易的使用Compose V2将多容器应用部署到AWS ECS或者Azure ACI环境。\n具体示例可参考 Deploying WordPress to the Cloud。\n在Golang中创建一个同构的Docker生态系统 在Compose V2之前，V1是使用Python语言编写的，不在Docker的语言生态系统里。而V2使用Golang语言编写，可以提供来自Moby、CLI或任何基于Golang的项目代码，减少了很多通过Python重写新功能或缺陷的开发，容易从其他Docker工具（例如BuildKit）增加新功能到Compose中。\n通过Golang，现在可以发布一个静态的二进制执行文件，相比Python，大大简化了更新和依赖管理。\n在没有Compose文件的情况下执行命令 Compose V2可以在以下情况下通过 --project-name|-p 选项来管理运行的Compose项目容器服务\n当前目录不包含项目Compose文件（不在Compose项目文件目录下） 不通过 --file 标记指定Compose文件 不通过 --project-directory 标记指定Compose项目目录 可执行的命令： ps，exec,start，stop，restart，down\n可以先通过 docker compose ls 列出当前环境的Compose项目\n❯ docker compose ls NAME STATUS CONFIG FILES dbweb running(1) docker-compose.yml monitor running(1) /home/mengz/dockerapp/monitor/docker-compose.yml traefik running(1) /home/mengz/dockerapp/traefik/docker-compose.yml truenas running(1) /home/mengz/dockerapp/truenas/docker-compose.yml 然后通过 -p \u0026lt;项目名\u0026gt; 命令 来管理项目服务\n❯ docker compose -p dbweb ps NAME COMMAND SERVICE STATUS PORTS dbweb_pgadmin \u0026#34;/entrypoint.sh\u0026#34; pgadmin running 443/tcp ❯ docker compose -p dbweb exec pgadmin sh /pgadmin4 $ exit 总结 这里简单介绍了Docker Compose V2的一些特性和功能，随着V1的逐渐淘汰，我们要拥抱V2，并且尝试其提供的新功能。\n关于详细的Docker Docker，请参考官方文档。\n参考：\nAnnouncing Compose V2 General Availability Install Docker Compose ","date":"June 2, 2022","hero":"/zh/posts/container-tech/docker-compose-v2/docker-compose-v2.jpg","permalink":"/zh/posts/container-tech/docker-compose-v2/","summary":"Compose V2 项目启动于2021年6月，直到2022年4月26号，发布了GA版本。在发布GA版本后，社区也宣布对于Compose V1将不会再进行功能更新，将在6个月后结束生命周期（EOL），期间会进行关键的安全和错误修复。\nV1与V2的兼容对比 确保 V1 和 V2 之间的兼容性对于日常工作流程至关重要，下面是V2中两个关键的更改\n更改 潜在影响 迁移 V2原生支持BuildKit，并且默认开启 开发者在V2中将默认使用BuildKit进行镜像构建 可通过设置环境变量不使用 DOCKER_BUILDKIT=0 容器名字中使用 - 替代了 _ 作为分隔符 如果在脚本中使用了容器名字，这可能会导致错误 可以通过 \u0026ldquo;\u0026ndash;compatibility\u0026rdquo; 标记来关闭此更改 关于更多的兼容性更改，请查看兼容性文档\n如何安装Compose V2 Windows，MacOS和Linux上使用Docker Desktop，就自带了Compose V2，可通过命令 docker compose 执行。也可以通过配置“Use Docker Compose V2“来设置 docker-compose 别名到 docker compose。\n如果没有使用Docker Desktop for Linux，而是直接使用的Docker Engine，则需要额外安装 docker-compose-plugin 或者独立的二进制包。\n例如对于Ubuntu,可以通过Docker官方的APT源直接安装\n❯ sudo apt update ❯ sudo apt install docker-compose-plugin 其他Linux, 例如在我的 openSUSE 上，通过手动从Github下载二进制文件进行安装（注意选择版本和平台架构）\n❯ DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker} ❯ mkdir -p $DOCKER_CONFIG/cli-plugins ❯ wget https://github.","tags":["docker","compose"],"title":"开始使用DOCKER COMPOSE V2"},{"categories":["Container"],"contents":"在上个月发布的Docker Desktop v4.7.0中，增加了一个新的CLI插件-docker/sbom-cli-plugin，其为Docker CLI增加了一个子命令 - sbom，用于查看Docker容器镜像的软件物料清单（SBOM)。\n什么是SBOM？ 首先介绍下什么是SBOM（Software Bill of Materials），我们称之为软件物料清单，是软件供应链中的术语。软件供应链是用于构建软件应用程序（软件产品）的组件、库和工具的列表，而物料清单则声明这些组件、库的清单，类似于食品的配料清单。软件物料清单可以帮助组织或者个人避免使用有安全漏洞的软件。\nDOCKER SBOM命令 注意: 从Docker Desktop 4.7.0版本开始到现在，docker sbom 命令还是实验性的，该功能也许会在以后版本中删除和更改，当前Linux的Docker CLI还未包含该子命令。\ndocker sbom 命令用于生产一个容器镜像的软件物料清单（SBOM）\nWSL - mengz  docker sbom --help Usage: docker sbom [OPTIONS] COMMAND View the packaged-based Software Bill Of Materials (SBOM) for an image. EXPERIMENTAL: The flags and outputs of this command may change. Leave feedback on https://github.com/docker/sbom-cli-plugin. Examples: docker sbom alpine:latest a summary of discovered packages docker sbom alpine:latest --format syft-json show all possible cataloging details docker sbom alpine:latest --output sbom.txt write report output to a file docker sbom alpine:latest --exclude /lib --exclude \u0026#39;**/*.db\u0026#39; ignore one or more paths/globs in the image Options: -D, --debug show debug logging --exclude stringArray exclude paths from being scanned using a glob expression --format string report output format, options=[syft-json cyclonedx-xml cyclonedx-json github-0-json spdx-tag-value spdx-json table text] (default \u0026#34;table\u0026#34;) --layers string [experimental] selection of layers to catalog, options=[squashed all] (default \u0026#34;squashed\u0026#34;) -o, --output string file to write the default report output to (default is STDOUT) --platform string an optional platform specifier for container image sources (e.g. \u0026#39;linux/arm64\u0026#39;, \u0026#39;linux/arm64/v8\u0026#39;, \u0026#39;arm64\u0026#39;, \u0026#39;linux\u0026#39;) --quiet suppress all non-report output -v, --version version for sbom Commands: version Show Docker sbom version information Run \u0026#39;docker sbom COMMAND --help\u0026#39; for more information on a command. 从命令的帮助信息中可以看到，除了直接生成表格形式的SBOM输出外，还支持使用--format指定多种类型的输出格式。\n我们尝试对镜像 neo4j:4.4.5 生成SBOM:\nWSL - mengz  docker sbom neo4jh:4.4.5 Syft v0.43.0 ✔ Loaded image ✔ Parsed image ✔ Cataloged packages [385 packages] NAME VERSION TYPE CodePointIM 11.0.15 java-archive FastInfoset 1.2.16 java-archive FileChooserDemo 11.0.15 java-archive Font2DTest 11.0.15 java-archive HdrHistogram 2.1.9 java-archive J2Ddemo 11.0.15 java-archive Metalworks 11.0.15 java-archive ... libuuid1 2.36.1-8+deb11u1 deb libxxhash0 0.8.0-2 deb libzstd1 1.4.8+dfsg-2.1 deb listenablefuture 9999.0-empty-to-avoid-conflict-with-guava java-archive log4j-api 2.17.1 java-archive log4j-core 2.17.1 java-archive login 1:4.8.1-1 deb ... 上面的输出表格之截取了部分，我们可以看到在清单列表中，除了系统包（deb类型）之外，还有java的软件包，其中就包含了 log4j 的包及其版本信息，从这些信息中就可以了解到容器镜像是否包含了存在安全漏洞的依赖和软件包，增强了使用软件镜像来部署应用的安全性。\n上面的信息中还看到了 Syft v0.43.0，这是因为当前的SBOM CLI插件是使用Anchore的Syft项目来进行镜像层的扫描，以后的版本也许会通过其他方法读取SBOM信息。\n我们再尝试输出一个镜像的SPDX格式的SBOM文件：\nWSL - mengz  docker sbom --form spdx-json --output hugo-sbom.json mengzyou/hugo:latest Syft v0.43.0 ✔ Loaded image ✔ Parsed image ✔ Cataloged packages WSL - mengz  cat hugo-sbom.jso { \u0026#34;SPDXID\u0026#34;: \u0026#34;SPDXRef-DOCUMENT\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;mengzyou/hugo-latest\u0026#34;, \u0026#34;spdxVersion\u0026#34;: \u0026#34;SPDX-2.2\u0026#34;, \u0026#34;creationInfo\u0026#34;: { \u0026#34;created\u0026#34;: \u0026#34;2022-05-09T10:55:06.6343529Z\u0026#34;, \u0026#34;creators\u0026#34;: [ \u0026#34;Organization: Anchore, Inc\u0026#34;, \u0026#34;Tool: syft-[not provided]\u0026#34; ], \u0026#34;licenseListVersion\u0026#34;: \u0026#34;3.16\u0026#34; }, \u0026#34;dataLicense\u0026#34;: \u0026#34;CC0-1.0\u0026#34;, \u0026#34;documentNamespace\u0026#34;: \u0026#34;https://anchore.com/syft/image/mengzyou/hugo-latest-162a6a05-379c-49f0-a7f2-b4b738a63d1b\u0026#34;, \u0026#34;packages\u0026#34;: [ { \u0026#34;SPDXID\u0026#34;: \u0026#34;SPDXRef-ed18f2a986e77aab\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;alpine-baselayout\u0026#34;, \u0026#34;licenseConcluded\u0026#34;: \u0026#34;GPL-2.0-only\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Alpine base dir structure and init scripts\u0026#34;, \u0026#34;downloadLocation\u0026#34;: \u0026#34;https://git.alpinelinux.org/cgit/aports/tree/main/alpine-baselayout\u0026#34;, ... } } 由于生成的文件较长，上面只输出了一小部分。\n补充 - SPDX (Software Package Data Exchage)是一个描述SBOM信息的开放标准，其中将包含软件组件、许可版权信息以及相关的安全参考。SPDX 通过为公司和社区提供共享重要数据的通用格式来减少冗余的工作，从而简化和提供合规性。\n总结 这里简单的介绍了 SBOM，以及Docker CLI的实验性子命令 - sbom，可以通过该命令生成r容器镜像多种格式的SBOM信息，让开发人员和需要使用容器镜像来部署服务的运维人员可以容易的获取到镜像的SBOM信息，从而了解到镜像的安全信息，以满足使用的合规性。\n同时，也可以考虑将该工具加入到公司交付应用的CI/CD流水中，作为镜像制品的安全检查工作。\n","date":"May 9, 2022","hero":"/zh/posts/container-tech/docker-sbom/docker-sbom.jpeg","permalink":"/zh/posts/container-tech/docker-sbom/","summary":"在上个月发布的Docker Desktop v4.7.0中，增加了一个新的CLI插件-docker/sbom-cli-plugin，其为Docker CLI增加了一个子命令 - sbom，用于查看Docker容器镜像的软件物料清单（SBOM)。\n什么是SBOM？ 首先介绍下什么是SBOM（Software Bill of Materials），我们称之为软件物料清单，是软件供应链中的术语。软件供应链是用于构建软件应用程序（软件产品）的组件、库和工具的列表，而物料清单则声明这些组件、库的清单，类似于食品的配料清单。软件物料清单可以帮助组织或者个人避免使用有安全漏洞的软件。\nDOCKER SBOM命令 注意: 从Docker Desktop 4.7.0版本开始到现在，docker sbom 命令还是实验性的，该功能也许会在以后版本中删除和更改，当前Linux的Docker CLI还未包含该子命令。\ndocker sbom 命令用于生产一个容器镜像的软件物料清单（SBOM）\nWSL - mengz  docker sbom --help Usage: docker sbom [OPTIONS] COMMAND View the packaged-based Software Bill Of Materials (SBOM) for an image. EXPERIMENTAL: The flags and outputs of this command may change. Leave feedback on https://github.com/docker/sbom-cli-plugin. Examples: docker sbom alpine:latest a summary of discovered packages docker sbom alpine:latest --format syft-json show all possible cataloging details docker sbom alpine:latest --output sbom.","tags":["docker","sbom","security"],"title":"DOCKER SBOM镜像物料清单"},{"categories":["Container"],"contents":"在Docker v19.03版本之前，我们可以使用DOCKER_HOST环境变量来配置和连接远程Docker主机，自从Docker v19.03版本开始，Docker的命令行接口（CLI）增加了一个子命令 - context，用于管理docker客户端连接多个上下文环境。\n通过context命令，可通过配置SSH协议的上下文连接并管理远程多个Docker主机，同时从一台安装了Docker CLI的机器导出上下文环境，并在另一台安装了Docker CLI的机器导入。\ncontext子命令 首先可以通过\u0026ndash;help选项查看命令支持的子命令：\n docker context --help Usage: docker context COMMAND Manage contexts Commands: create Create a context export Export a context to a tar or kubeconfig file import Import a context from a tar or zip file inspect Display detailed information on one or more contexts ls List contexts rm Remove one or more contexts update Update a context use Set the current docker context Run \u0026#39;docker context COMMAND --help\u0026#39; for more information on a command. 这里将演示如何使用使用DOCKER_HOST环境变量的方式，以及context命令的方式来连接远程Docker主机。\n提前准备 首先我们需要准备两台Docker主机，并安装Docker v19.03+ 版本，例如这里\n192.168.0.110(linux-dev) - 我的本地工作主机，Docker version 20.10.12-ce 192.168.0.200(home-boxsrv) - 远程Docker主机，Docker version 20.10.7 为避免输入SSH密码，请提前配置从Docker客户端主机免密访问远程Docker主机。\nDOCKER_HOST环境方式 首先我们在远程Docker主机（home-boxsrv）上运行一个容器，例如名为 dns_masq 的容器\nubuntu@linux-boxsrv:~$ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0597a189d488 jpillora/dnsmasq:1.1 \u0026#34;webproc --config /e…\u0026#34; 2 months ago Up 31 hours 127.0.0.1:53-\u0026gt;53/tcp, 127.0.0.1:53-\u0026gt;53/udp, 192.168.0.200:53-\u0026gt;53/tcp, 192.168.0.200:53-\u0026gt;53/udp, 192.168.31.200:53-\u0026gt;53/tcp, 192.168.31.200:53-\u0026gt;53/udp, 0.0.0.0:8053-\u0026gt;8080/tcp, :::8053-\u0026gt;8080/tcp dns_masq 在本地主机上（linux-dev）配置环境变量\nmengz@linux-dev💻~ ❯ export DOCKER_HOST=ssh://ubuntu@192.168.0.200 然后查看容器\nmengz@linux-dev💻☸~ ❯ docker container ls CONTAINER ID NAMES IMAGE CREATED ago STATUS PORTS COMMAND 0597a189d488 dns_masq jpillora/dnsmasq:1.1 2 months ago ago Up 31 hours 127.0.0.1:53-\u0026gt;53/tcp, 127.0.0.1:53-\u0026gt;53/udp, 192.168.0.200:53-\u0026gt;53/tcp, 192.168.0.200:53-\u0026gt;53/udp, 192.168.31.200:53-\u0026gt;53/tcp, 192.168.31.200:53-\u0026gt;53/udp, 0.0.0.0:8053-\u0026gt;8080/tcp, :::8053-\u0026gt;8080/tcp \u0026#34;webproc --config /e…\u0026#34; 我们可以看到，列出的是运行在远程主机上的容器。\n使用context命令 首先我们在本地主机（linux-dev）上清除上面配置的环境变量\nmengz@linux-dev💻~ ❯ unset DOCKER_HOST 使用context ls命令列出当前客户端配置的上下文环境\nmengz@linux-dev💻~ ❯ docker context ls NAME DESCRIPTION DOCKER ENDPOINT KUBERNETES ENDPOINT ORCHESTRATOR default * Current DOCKER_HOST based configuration unix:///var/run/docker.sock https://k8s1.mengz.lan:6443 (default) swarm 可以看到，当前有一个名为 default 的环境，连接的是本机上Docker引擎。\n现在，我们通过 context create 命令来添加连接（home-boxsrv）的上下文环境\nmengz@linux-dev💻~ ❯ docker context create home-boxsrv --description \u0026#34;Docker Engine on home-boxsrv\u0026#34; --docker \u0026#34;host=ssh://ubuntu@192.168.0.200\u0026#34; home-boxsrv Successfully created context \u0026#34;home-boxsrv\u0026#34; ❯ docker context ls NAME DESCRIPTION DOCKER ENDPOINT KUBERNETES ENDPOINT ORCHESTRATOR default * Current DOCKER_HOST based configuration unix:///var/run/docker.sock https://k8s1.mengz.lan:6443 (default) swarm home-boxsrv Docker Engine on home-boxsrv ssh://ubuntu@192.168.0.200 以成功添加了名为 home-boxsrv 的环境，但是当前激活的还是 default 环境，我们需要通过 context use 命令来设置当前环境\nmengz@linux-dev💻~ ❯ docker context use home-boxsrv home-boxsrv Current context is now \u0026#34;home-boxsrv\u0026#34; 现在我们使用 container ls 看一下\nmengz@linux-dev💻☸~ ❯ docker container ls CONTAINER ID NAMES IMAGE CREATED ago STATUS PORTS COMMAND 0597a189d488 dns_masq jpillora/dnsmasq:1.1 2 months ago ago Up 31 hours 127.0.0.1:53-\u0026gt;53/tcp, 127.0.0.1:53-\u0026gt;53/udp, 192.168.0.200:53-\u0026gt;53/tcp, 192.168.0.200:53-\u0026gt;53/udp, 192.168.31.200:53-\u0026gt;53/tcp, 192.168.31.200:53-\u0026gt;53/udp, 0.0.0.0:8053-\u0026gt;8080/tcp, :::8053-\u0026gt;8080/tcp \u0026#34;webproc --config /e…\u0026#34; 列出的是远程主机（home-boxsrv）上的容器，如果使用 docker info 命令查看，server将是远程主机的信息。\n除了SSH协议的端点方式，如果远程主机通过tcp暴露的docker端点，那我们也可以使用tcp的端点方式，例如下面的名为 home-cappsrv 的环境\n❯ docker context ls NAME DESCRIPTION DOCKER ENDPOINT KUBERNETES ENDPOINT ORCHESTRATOR default Current DOCKER_HOST based configuration unix:///var/run/docker.sock https://k8s1.mengz.lan:6443 (default) swarm home-boxsrv * Docker Engine on home-boxsrv ssh://ubuntu@192.168.0.200 home-cappsrv The docker engine on home-cappsrv tcp://192.168.0.123:2375 总结 通过 context 命令，我们可以方便在一台Docker客户端主机上连接并切换管理多台Docker主机环境，大大提高了运维多主机环境的效率，同时可以方便将管理的Docker上下文环境导出并导入到其他Docker客户端使用，跟多的命令使用方法可参考 官方文档。\n","date":"May 8, 2022","hero":"/zh/posts/container-tech/docker-context-remote/docker-context-remote.png","permalink":"/zh/posts/container-tech/docker-context-remote/","summary":"在Docker v19.03版本之前，我们可以使用DOCKER_HOST环境变量来配置和连接远程Docker主机，自从Docker v19.03版本开始，Docker的命令行接口（CLI）增加了一个子命令 - context，用于管理docker客户端连接多个上下文环境。\n通过context命令，可通过配置SSH协议的上下文连接并管理远程多个Docker主机，同时从一台安装了Docker CLI的机器导出上下文环境，并在另一台安装了Docker CLI的机器导入。\ncontext子命令 首先可以通过\u0026ndash;help选项查看命令支持的子命令：\n docker context --help Usage: docker context COMMAND Manage contexts Commands: create Create a context export Export a context to a tar or kubeconfig file import Import a context from a tar or zip file inspect Display detailed information on one or more contexts ls List contexts rm Remove one or more contexts update Update a context use Set the current docker context Run \u0026#39;docker context COMMAND --help\u0026#39; for more information on a command.","tags":["docker"],"title":"管理远程DOCKER主机"},{"categories":["Linux"],"contents":"openSUSE Leap 15.2自2022年1月4日起已结束生命周（EOL），还在使用该系统的用户未来将不会再收到任何形式的安全与维护更新。\n建议所有用户尽快将系统升级到 - openSUSE Leap 15.3，该系统将获得安全补丁和更新直至2022年11月。下一个版本openSUSE Leap 15.4也将预计在2022年6月发布。\n喜欢滚动更新版的的朋友，也可以借此机会从Leap版本切换到Tumbleweed版本。\nopenSUSE Leap的版本升级可以通过联网在线升级，也可以通过下载最新版本的ISO文件进行线下升级，这里面我们将看看如何在线升级。\nopenSUSE Leap在线升级 使用在线升级的方式有如下优势：\n只需要下载需要更新的软件包，将节省不少带宽 在升级期间，虽然不推荐，但是你任然可以使用系统，只有在升级完成后需要重新启动 因为不需要下载ISO镜像文件，所以不需要DVD驱动器或者刻录USB启动盘，需要的仅仅是网络 当然在线升级也有如下缺点：\n如果由于某些原因，导致升级过程被中断（例如突然断电，网络连接断开），升级进程无法继续，这有可能会留下一个被破坏的系统 如果有多个系统需要升级，那么下载ISO镜像可能会更省带宽 注意，如果你使用的是更旧的Leap版本，例如 15.1，请先升级到15.2之后，再升级到15.3 。\n你可以使用如下命令查看当前版本\n\u0026gt; lsb_release -d Description: openSUSE Leap 15.2 虽然正常的升级不会导致用户数据的丢失，但是为了安全，建议在升级之前备份自己重要的个人数据。\n升级系统之前的准备 首先检查更新源是否存在并更新当前发行版本的软件包 # zypper repos --uri ... 29 | repo-update | 主更新源 | Yes | ( ) No | No | https://mirrors.tuna.tsinghua.edu.cn/opensuse/update/leap/15.2/oss/ 30 | repo-update-non-oss | 主更新源（非开源软件) | Yes | ( ) No | No | https://mirrors.tuna.tsinghua.edu.cn/opensuse/update/leap/15.2/non-oss/ ... 这里我使用了清华大学的镜像源（https://mirrors.tuna.tsinghua.edu.cn/） 如果上面一样已经存在更新源，则进行下一步；如果没有更新源，请添加\n# zypper addrepo --check --refresh --name \u0026#39;openSUSE-Leap-15.2-Update\u0026#39; http://download.opensuse.org/update/leap/15.2/oss/ repo-update 将 /var/cache 移动到一个独立子卷（subvolume） 注意，如果你系统的根文件系统不是 Btrfs 的类型，则可以跳过这一步\n查找根文件系统的设备名 # df / Filesystem 1K-blocks Used Available Use% Mounted on /dev/sdb3 62914560 37723280 24393296 61% / 确定所有其他子卷的父卷 # btrfs subvolume list / 一般来说应该是 @\n挂在指定子卷到临时挂载点 # mount /dev/sdb3 -o subvol=@ /mnt 为了避免数据丢失，备份下缓存 # mv /mnt/var/cache /mnt/var/cache.old 创建一个新的子卷，并将缓存移回 # btrfs subvol create /mnt/var/cache # mv /mnt/var/cache.old/* /mnt/var/cache/ # rm -rf /mnt/var/cache.old 卸载临时挂载点，然后将新的缓存子卷添加到 /etc/fstab # umount /mnt 添加以下条目到 /etc/fstab\nUUID=4f648797-078d-426f-b103-51d9a73dd937 /var/cache btrfs subvol=/@/var/cache 0 0 这里的UUID应该是和根文件系统的一样。\n# mount /var/cache 将当前系统的包更新到最新 # zypper ref # zypper update 执行发行版升级 -\u0026gt; 15.3 更新软件源版本 # sed -i \u0026#39;s/15.2/${releasever}/g\u0026#39; /etc/zypper/repos.d/*.repo 刷新所有软件源 # zypper --releasever=15.3 ref 如果在此期间，发现某些自己添加OBS软件源不可用的话，可以先Disable掉。\n最后就是执行发行版的版本升级 # zypper --releasever15.3 dup 你可能会被询问一些软件包提供商的更改，因为之前我们可能使用其他软件源安装了比较新版本的软件包，我们只需要选择相应选项就可以，最后会得到如下提示\nThe following NEW product is going to be installed: \u0026#34;openSUSE Leap 15.3\u0026#34; The following product is going to be REMOVED: \u0026#34;openSUSE Leap 15.2\u0026#34; The following package requires a system reboot: kernel-preempt-5.3.18-59.10.1 2732 packages to upgrade, 876 to downgrade, 394 new, 75 to remove, 2430 to change vendor. Overall download size: 3.14 GiB. Already cached: 0 B. After the operation, 17.4 MiB will be freed. Note: System reboot required. Continue? [y/n/v/...? shows all options] (y): 敲一个 \u0026lsquo;y\u0026rsquo; 然后回车，就开始了漫长的（却决于你的网络环境和使用的软件源镜像地址）下载和安装过程了，升级过程中你任然可以使用你的系统 ^_^ 。\n完成所有下载和安装之后，会要求你重启你的系统，关闭你的所有应用，然后重启你的系统，你将会得到一个新版本（Leap 15.3）的openSUSE系统。\n\u0026gt; lsb_release -d Description: openSUSE Leap 15.3 之后就可以继续添加你需要使用的OBS软件源（当然对应 15.3 版本）来安装其他软件包了。\n总结 Leap发行版是openSUSE的常规发行版本，基本上12个月会进行一次小版本的更新，例如从 15.2 -\u0026gt; 15.3 -\u0026gt; 15.4; 每36-38个月会进行一次大版本的升级，例如 15.x -\u0026gt; 16.x 。\nopenSUSE还发行了Tumbleweed版本，也就是滚动更新版本，其软件源提供了所有最新的软件包进行滚动升级。\n这里我们看到了如何升级一个Leap版本，过程不算复杂，总结来说就是更新软件源版本，然后通过 zypper dup 进行更新，我使用了openSUSE已经多年了，几乎都是每个版本这样升级上来的。\n说实话，在所有的Linux桌面发行版中，openSUSE不算流行，但是我的使用体验是非常稳定，配合KDE桌面环境，也是非常优雅漂亮的一款Linux发行版，希望喜欢Linux桌面用于办公的小伙伴来尝试。\n","date":"January 15, 2022","hero":"/zh/posts/linux/opensuse-upgrade/opensuse-upgrade.jpg","permalink":"/zh/posts/linux/opensuse-upgrade/","summary":"openSUSE Leap 15.2自2022年1月4日起已结束生命周（EOL），还在使用该系统的用户未来将不会再收到任何形式的安全与维护更新。\n建议所有用户尽快将系统升级到 - openSUSE Leap 15.3，该系统将获得安全补丁和更新直至2022年11月。下一个版本openSUSE Leap 15.4也将预计在2022年6月发布。\n喜欢滚动更新版的的朋友，也可以借此机会从Leap版本切换到Tumbleweed版本。\nopenSUSE Leap的版本升级可以通过联网在线升级，也可以通过下载最新版本的ISO文件进行线下升级，这里面我们将看看如何在线升级。\nopenSUSE Leap在线升级 使用在线升级的方式有如下优势：\n只需要下载需要更新的软件包，将节省不少带宽 在升级期间，虽然不推荐，但是你任然可以使用系统，只有在升级完成后需要重新启动 因为不需要下载ISO镜像文件，所以不需要DVD驱动器或者刻录USB启动盘，需要的仅仅是网络 当然在线升级也有如下缺点：\n如果由于某些原因，导致升级过程被中断（例如突然断电，网络连接断开），升级进程无法继续，这有可能会留下一个被破坏的系统 如果有多个系统需要升级，那么下载ISO镜像可能会更省带宽 注意，如果你使用的是更旧的Leap版本，例如 15.1，请先升级到15.2之后，再升级到15.3 。\n你可以使用如下命令查看当前版本\n\u0026gt; lsb_release -d Description: openSUSE Leap 15.2 虽然正常的升级不会导致用户数据的丢失，但是为了安全，建议在升级之前备份自己重要的个人数据。\n升级系统之前的准备 首先检查更新源是否存在并更新当前发行版本的软件包 # zypper repos --uri ... 29 | repo-update | 主更新源 | Yes | ( ) No | No | https://mirrors.tuna.tsinghua.edu.cn/opensuse/update/leap/15.2/oss/ 30 | repo-update-non-oss | 主更新源（非开源软件) | Yes | ( ) No | No | https://mirrors.","tags":["opensuse"],"title":"升级OPENSUSE LEAP"},{"categories":["Container"],"contents":"我们都知道Docker是C/S模式架构，通过客户端（CLI）访问Docker Daemon来创建和管理容器的。在默认情况下，当daemon终止的时候，会停止所有运行的容器。\n因此我们需要对Docker Daemon进行升级或者某些需要重启的维护操作时，都需要导致运行着的容器跟着重新启动。\nLive Restore 其实，Docker提供了一个特性，可以使得在Daemon不可用的时候，保持容器继续运行，这样就减少了在Daemon进行升级或者出现问题的时候容器的停机时间。那这个特性就叫做Live Restore 。\n通过为Docker Daemon增加以下配置来开启Live Restore特性。在Linux上，默认的配置文件 /etc/docker/daemon.json 里添加\n{ \u0026#34;live-restore\u0026#34;: true } 然后重启docker服务。如果使用systemd管理服务，可以通过reload来避免重启docker服务\nsudo systemctl reload docker.service 其他情况下，可以发送 SIGHUP 信号给dockerd进程。\n对于Windows和MacOS上的Docker Desktop，可以通过Desktop节目的Daemon高级配置来开启Live Restore。\n配置完成后，可以尝试重启Docker Daemon来查看容器是否会保持继续运行。重启之前查看容器的启动时间\n WSL -   mengz  docker container inspect portainer_edge_agent -f \u0026#39;{{ .State.StartedAt }}\u0026#39; 2021-12-18T09:50:59.761725785Z 然后执行 sudo systemctl restart docker.service，在查询一次容器的启动时间，将发现启动时间未发生变化，这说明了容器并没有重启。\nLive Restore的限制 当前的Live Restore特性可以在进行Daemon维护，或者在Daemon发生问题导致不可用的情况，减少容器的停机时间，不过其也有一定的限制。\nDocker版本升级限制 Live Restore仅支持Docker补丁版本升级时可用，也就是 YY.MM.x 最后一位发生变化的升级，而不支持大版本的升级。在进行大版本升级后，可能会导致Daemon无法重新连接到运行中容器的问题，这时候需要手动停止运行的容器。\nDaemon选项变更 也就是说Live Restore仅仅在某些Daemon级别的配置选项不发生改变的情况工作，例如Bridge的IP地址，存储驱动类型等。如果在重启Daemon时候，这些选项发生了改变，则可能会到Daemon无法重新连接运行中的容器，这时也需要手动停止这些容器。\n影响容器的日志输出 如果Daemon长时间停止，会影响运行容器的日志输出。因为默认情况下，日志管道的缓冲区大小为64k，当缓冲写满之后，必须启动Daemon来刷新缓冲区。\n不支持Docker Swarm Live Restore只是独立Docker引擎的特性，而Swarm的服务是由Swarm管理器管理的。当Swarm管理器不可用时，Swarm服务是可以在工作节点上继续运行的，只是不同通过Swarm管理器进行管理，直到Swarm管理恢复工作。\n总结 通过Docker Daemon的 live-restore 特性，我们可以运行无守护进程（daemonless）的容器，这可以减小容器应用在对Docker Daemon进行维护时候的停机时间，不过在使用时也有一定的限制，例如对于升级引擎版本的限制。如果关注无守护进程的容器，可以进一步了解 Podman 。\n以上内容大部分来自Docker的官方文档，更详细的信息请参考 。\n","date":"December 19, 2021","hero":"/zh/posts/container-tech/docker-live-restore/docker-banner.png","permalink":"/zh/posts/container-tech/docker-live-restore/","summary":"我们都知道Docker是C/S模式架构，通过客户端（CLI）访问Docker Daemon来创建和管理容器的。在默认情况下，当daemon终止的时候，会停止所有运行的容器。\n因此我们需要对Docker Daemon进行升级或者某些需要重启的维护操作时，都需要导致运行着的容器跟着重新启动。\nLive Restore 其实，Docker提供了一个特性，可以使得在Daemon不可用的时候，保持容器继续运行，这样就减少了在Daemon进行升级或者出现问题的时候容器的停机时间。那这个特性就叫做Live Restore 。\n通过为Docker Daemon增加以下配置来开启Live Restore特性。在Linux上，默认的配置文件 /etc/docker/daemon.json 里添加\n{ \u0026#34;live-restore\u0026#34;: true } 然后重启docker服务。如果使用systemd管理服务，可以通过reload来避免重启docker服务\nsudo systemctl reload docker.service 其他情况下，可以发送 SIGHUP 信号给dockerd进程。\n对于Windows和MacOS上的Docker Desktop，可以通过Desktop节目的Daemon高级配置来开启Live Restore。\n配置完成后，可以尝试重启Docker Daemon来查看容器是否会保持继续运行。重启之前查看容器的启动时间\n WSL -   mengz  docker container inspect portainer_edge_agent -f \u0026#39;{{ .State.StartedAt }}\u0026#39; 2021-12-18T09:50:59.761725785Z 然后执行 sudo systemctl restart docker.service，在查询一次容器的启动时间，将发现启动时间未发生变化，这说明了容器并没有重启。\nLive Restore的限制 当前的Live Restore特性可以在进行Daemon维护，或者在Daemon发生问题导致不可用的情况，减少容器的停机时间，不过其也有一定的限制。\nDocker版本升级限制 Live Restore仅支持Docker补丁版本升级时可用，也就是 YY.MM.x 最后一位发生变化的升级，而不支持大版本的升级。在进行大版本升级后，可能会导致Daemon无法重新连接到运行中容器的问题，这时候需要手动停止运行的容器。\nDaemon选项变更 也就是说Live Restore仅仅在某些Daemon级别的配置选项不发生改变的情况工作，例如Bridge的IP地址，存储驱动类型等。如果在重启Daemon时候，这些选项发生了改变，则可能会到Daemon无法重新连接运行中的容器，这时也需要手动停止这些容器。\n影响容器的日志输出 如果Daemon长时间停止，会影响运行容器的日志输出。因为默认情况下，日志管道的缓冲区大小为64k，当缓冲写满之后，必须启动Daemon来刷新缓冲区。\n不支持Docker Swarm Live Restore只是独立Docker引擎的特性，而Swarm的服务是由Swarm管理器管理的。当Swarm管理器不可用时，Swarm服务是可以在工作节点上继续运行的，只是不同通过Swarm管理器进行管理，直到Swarm管理恢复工作。","tags":["docker"],"title":"DOCKER LIVERESTORE特性"},{"categories":["Linux"],"contents":"我上一篇文章中介绍的locate文件查找命令，需要依赖updatedb更新索引才能快速查找文件，因此需要定时运行该命令来更新文件索引。我们知道在Linux和类Unix系统上通常使用crontab来创建定时任务。\n在Ubuntu上我们使用apt install mlocate之后，会安装一个脚本文件到 /etc/cron.daily/mlocate，也就是通过Cron机制来每天执行updatedb。然而在我的openSUSE上却并未发现有相关的Crontab配置，但我发现索引文件还是在每天的零点进行了更新，那这个定时任务是谁来执行的呢？\n我通过查找与mlocate相关的文件，发现了以下几个文件：\n❯ locate \u0026#34;mlocate\u0026#34; /etc/systemd/system/timers.target.wants/mlocate.timer /usr/lib/systemd/system/mlocate.service /usr/lib/systemd/system/mlocate.timer 原来在openSUSE系统上，使用的是Systemd的定时单元来实现的。Systemd是一种Linux系统服务管理程序，在我之前的文章在OPENSUSE上使用SYSTEMCTL管理系统服务中介绍过。\n那这里我们将重点介绍下Systemd的定时服务（systemd timer unit）。\nsystemd定时单元 类似与Cron，systemd的定时单元在Linux系统上提供了机制来调度任务，相比于Cron机制，其他具有以下特性（在使用systemd作为初始化和服务管理的系统上）：\n调度的任务可以依赖于其他systemd服务 可以使用systemctl命令来管理定时单元，类似与管理systemd服务 除了类似Cron的循环实时定时任务（realtime）之外，还支持一种基于非时间事件触发的任务（monotonic） 定时单元记录日志到systemd的日志系统（journal），因此方便于统一监控和诊断 systemd定时任务的类型 上面的特性中，我们提到其支持两种类型 - realtime 和 monotonic\nRealtime - 类似于Cron，这种类型的定时任务由定义的绝对时间来触发，在配置文件中通过 OnCalendar 选项来定义 Monotonic - 这种类型的定时任务将会在指定的事件（例如系统启动，服务激活）一定时间后触发，在配置文件中通过 OnBootSec 和 OnUnitActiveSec ，OnStartupSec 等选项来定义，并且该类型的定时任务触发时间不是固定的，在每一次系统重启之后都会被重置 systemd定时任务的配置 在文章开始，我们在寻找mlocate更新文件索引的定时任务时看到，有文件 /usr/lib/systemd/system/mlocate.timer ，没错，就是通过以 .timer 作为扩展名的systemd单元文件来定义systemd的定时单元的\n[Unit] Description=Daily locate database update Documentation=man:updatedb [Timer] OnCalendar=daily AccuracySec=12h Unit=mlocate.service Persistent=true [Install] WantedBy=timers.target 可以看到文件格式与systemd服务的单元文件类似，不过需要 [Timer] 段，在该段定义了如下选项\nOnCalendar=daily，意思是每天触发 AccuracySec=12h，意思是由于某些原因需要推测执行的时间 Unit=mlocate.service，这里就是指定了需要执行的任务服务 Persistent=true，指定如果由于关机等原因到时了为能执行任务的情况下，启动会立即触发该任务 那该定时单元指定了 mlocate.service 作为触发执行的任务，也就是 /usr/lib/systemd/system/mlocate.service 里定义的服务，那服务里就是定义使用 updatedb 命令去更新文件索引。\n对于 OnCalendar ，其值支持的格式为 DayOfWeek Year-Month-Day Hour:Minute:Second，例如\nOnCalendar=--* 5:00:00，指定在每天早上的5点执行 OnCalendar=Fri 18:00:00，指定在每个月的周五下午6点执行 在一个配置文件中，还可以指定多个 OnCalendar ，例如\nOnCalendar=Mon..Fri 10:00 OnCalendar=Sat,Sun 22:00 上面的配置就指定了在周一到周五的每天上午10点，以及在周末两天的晚上10点执行。\n下面我们来举一个使用monotonic类型定时任务的例子，在目录 /etc/systemd/system/ 下服务单元文件 foo.service\n[Unit] Description=\u0026#34;Foo shell script\u0026#34; [Service] ExecStart=/usr/local/bin/foo.sh 同时创建一个定时单元文件 foo.timer\n[Unit] Description=\u0026#34;Run foo shell script\u0026#34; [Timer] OnBootSec=5min OnUnitActiveSec=24h Unit=foo.service [Install] WantedBy=multi-user.target 这里我们看到在 [Timer] 段，我们定义一下选项\nOnBootSec=5min，指定了在系统启动5分钟后触发指定服务的执行 OnUnitActiveSec=24h，指定了在服务在激活之后的24小时执行，也就是每天都会执行一次（但是执行的具体时间取决于开机时间） Unit=foo.service，指定了触发的任务是我们上面定义的foo服务，也就是执行 foo.sh 脚本 管理timer单元 上面的特性中，我们说道timer单元可以通过 systemctl 命令进行管理，类似管理服务单元\nsudo systemctl start foo.timer ，启动指定的定时单元 sudo systemctl enable foo.timer ，开启定时单元自动激活（开机自启） sudo systemctl list-timers ，列出当前系统已激活的定时单元 例如\n❯ sudo systemctl list-timers NEXT LEFT LAST PASSED UNIT ACTIVATES Fri 2021-12-03 17:00:00 CST 22min left Fri 2021-12-03 16:00:03 CST 37min ago snapper-timeline.timer snapper-timeline.service Sat 2021-12-04 00:00:00 CST 7h left Fri 2021-12-03 00:00:03 CST 16h ago logrotate.timer logrotate.service Sat 2021-12-04 00:00:00 CST 7h left Fri 2021-12-03 00:00:03 CST 16h ago mandb.timer mandb.service Sat 2021-12-04 00:00:00 CST 7h left Fri 2021-12-03 00:00:03 CST 16h ago mlocate.timer mlocate.service 我们还可以直接使用 journalctl 来查看相关日志，例如\n❯ sudo journalctl -u mlocate -- Logs begin at Thu 2021-12-02 06:58:59 CST, end at Fri 2021-12-03 16:41:26 CST. -- Dec 03 00:00:03 linux-dev systemd[1]: Starting Update locate database... Dec 03 00:00:03 linux-dev su[864]: (to nobody) root on none Dec 03 00:00:06 linux-dev systemd[1]: Started Update locate database. 同时查看mlocate定时单元和服务的日志。\n关于更多的配置细节，可以参考官方文档 。\n总结 如果你的系统使用的Systemd作为初始化和服务管理系统，并且想使用到我们之前提到的特性，那么我们可以使用systemd的timer单元来定义我们的定时任务。当然大部分系统还是支持Crontab机制的定时任务。\n","date":"December 3, 2021","hero":"/zh/posts/linux/systemd-timer/systemd.webp","permalink":"/zh/posts/linux/systemd-timer/","summary":"我上一篇文章中介绍的locate文件查找命令，需要依赖updatedb更新索引才能快速查找文件，因此需要定时运行该命令来更新文件索引。我们知道在Linux和类Unix系统上通常使用crontab来创建定时任务。\n在Ubuntu上我们使用apt install mlocate之后，会安装一个脚本文件到 /etc/cron.daily/mlocate，也就是通过Cron机制来每天执行updatedb。然而在我的openSUSE上却并未发现有相关的Crontab配置，但我发现索引文件还是在每天的零点进行了更新，那这个定时任务是谁来执行的呢？\n我通过查找与mlocate相关的文件，发现了以下几个文件：\n❯ locate \u0026#34;mlocate\u0026#34; /etc/systemd/system/timers.target.wants/mlocate.timer /usr/lib/systemd/system/mlocate.service /usr/lib/systemd/system/mlocate.timer 原来在openSUSE系统上，使用的是Systemd的定时单元来实现的。Systemd是一种Linux系统服务管理程序，在我之前的文章在OPENSUSE上使用SYSTEMCTL管理系统服务中介绍过。\n那这里我们将重点介绍下Systemd的定时服务（systemd timer unit）。\nsystemd定时单元 类似与Cron，systemd的定时单元在Linux系统上提供了机制来调度任务，相比于Cron机制，其他具有以下特性（在使用systemd作为初始化和服务管理的系统上）：\n调度的任务可以依赖于其他systemd服务 可以使用systemctl命令来管理定时单元，类似与管理systemd服务 除了类似Cron的循环实时定时任务（realtime）之外，还支持一种基于非时间事件触发的任务（monotonic） 定时单元记录日志到systemd的日志系统（journal），因此方便于统一监控和诊断 systemd定时任务的类型 上面的特性中，我们提到其支持两种类型 - realtime 和 monotonic\nRealtime - 类似于Cron，这种类型的定时任务由定义的绝对时间来触发，在配置文件中通过 OnCalendar 选项来定义 Monotonic - 这种类型的定时任务将会在指定的事件（例如系统启动，服务激活）一定时间后触发，在配置文件中通过 OnBootSec 和 OnUnitActiveSec ，OnStartupSec 等选项来定义，并且该类型的定时任务触发时间不是固定的，在每一次系统重启之后都会被重置 systemd定时任务的配置 在文章开始，我们在寻找mlocate更新文件索引的定时任务时看到，有文件 /usr/lib/systemd/system/mlocate.timer ，没错，就是通过以 .timer 作为扩展名的systemd单元文件来定义systemd的定时单元的\n[Unit] Description=Daily locate database update Documentation=man:updatedb [Timer] OnCalendar=daily AccuracySec=12h Unit=mlocate.service Persistent=true [Install] WantedBy=timers.target 可以看到文件格式与systemd服务的单元文件类似，不过需要 [Timer] 段，在该段定义了如下选项\nOnCalendar=daily，意思是每天触发 AccuracySec=12h，意思是由于某些原因需要推测执行的时间 Unit=mlocate.service，这里就是指定了需要执行的任务服务 Persistent=true，指定如果由于关机等原因到时了为能执行任务的情况下，启动会立即触发该任务 那该定时单元指定了 mlocate.service 作为触发执行的任务，也就是 /usr/lib/systemd/system/mlocate.","tags":["systemd","crontab"],"title":"SYSTEMD定时服务"},{"categories":["OpenTool"],"contents":"众所周知，在Linux或者类Unix的文件系统中，想通过文件名关键字查找文件，可以通过find命令。那本文将推荐2款可以快速查找文件的工具，性能比find命令更好，可在某些场景下替换find的使用。\nmlocate 大部分的Linux发行版的都提供了 mlocate 软件包，该软件包包含了一个locate命令用于查找文件，和一个updatedb命令用于更新文件索引供locate使用。\n可直接通过系统的软件包管理工具直接安装\n# CentOS/RHEL $ sudo dnf install mlocate # Debian/Ubuntu $ sudo apt install mlocate 安装完成后，首先需要执行以下命令进行文件索引\nsudo updatedb 索引文件将默认存放在 /var/lib/mlocate/mlocatedb ，也可以修改配置文件 /etc/updatedb.conf 文件，添加某些不需要索引的文件夹，例如\n# Paths which are pruned from updatedb database PRUNEPATHS=\u0026#34;/tmp /var/tmp /var/cache /var/lock /var/run /var/spool /mnt /cdrom /usr/tmp /proc /media /sys /.snapshots /var/run/media\u0026#34; 完成索引之后，就可以使用 locate 命令进行文件查找了，例如\n$ locate mlocate /etc/systemd/system/timers.target.wants/mlocate.timer /usr/bin/rpmlocate /usr/lib/systemd/system/mlocate.service /usr/lib/systemd/system/mlocate.timer /usr/sbin/rcmlocate /usr/share/doc/packages/mlocate /usr/share/doc/packages/mlocate/AUTHORS /usr/share/doc/packages/mlocate/ChangeLog /usr/share/doc/packages/mlocate/NEWS /usr/share/doc/packages/mlocate/README /usr/share/licenses/mlocate /usr/share/licenses/mlocate/COPYING /usr/share/man/man5/mlocate.db.5.gz /var/lib/mlocate /var/lib/mlocate/mlocate.db /var/lib/mlocate/mlocate.db.9O5YsQ /var/lib/systemd/migrated/mlocate /var/lib/systemd/timers/stamp-mlocate.timer 可以使用 -b 选项进行精确匹配，例如下面两个查询的结果区别\n$ locate -b \u0026#39;\\updatedb\u0026#39; /usr/bin/updatedb 注意使用 -b 时，需要在搜索的关键自前使用 \\ 。\n$ locate \u0026#39;updatedb\u0026#39; /etc/updatedb.conf /etc/apparmor.d/usr.bin.updatedb /usr/bin/updatedb /usr/share/augeas/lenses/dist/updatedb.aug /usr/share/man/man5/updatedb.conf.5.gz /usr/share/man/man8/updatedb.8.gz /usr/share/nvim/runtime/ftplugin/updatedb.vim /usr/share/nvim/runtime/syntax/updatedb.vim /usr/share/vim/vim80/ftplugin/updatedb.vim /usr/share/vim/vim80/syntax/updatedb.vim 还可以使用 -r 进行基本的正则表达式模式匹配查找，可以查看 locate --help 或者 man locate 。\n接下来我们将介绍另一款find的替代平 - fd 。\nfd fd是一款由David Peter开发的开源工具，用于在文件系统中查找文件，在大部分情况下可以替换find命令。\nfd可用于多个平台，包括大部分的Linux发行版，MacOS，Windows，具体安装可查看 https://github.com/sharkdp/fd#installation 。\n例如可以使用 HomeBrew/LinuxBrew 进行安装\n$ brew install fd 安装完成后，就可以直接使用，例如查找当前文件夹下以 png 为扩展名的文件\n$ fd -e png go/src/github.com/Go-zh/tour/static/img/gopher.png go/src/github.com/Go-zh/tour/content/img/tree.png go/src/github.com/containous/yaegi/doc/images/yaegi.png ... 注意，fd命令默认的搜索路径是当前目录，可以使用 --base-directory 或者 --search-path 来指定搜索路径，例如我们查找 /etc/ 下，匹配 docker 的常规文件\n$ fd --base-directory /etc/ -t f \u0026#39;docker\u0026#39; audit/rules.d/docker.rules bash_completion.d/docker-compose.bash sysconfig/docker 还可以使用 -x 选项将结果输出给其他命令进行操作（类似find命令的\u0026ndash;exec选项），例如\n$ fd -d 1 -e png -x convert {} {.}.jpg 这将查找当前目录下的所有PNG文件，并转换为JPG文件。上面的命令中使用到了占位符 {} 和 {.} ，看如下示例展示占位符所代表的结果\n❯ fd \u0026#39;recognition.db\u0026#39; -x echo {} Pictures/recognition.db Pictures/Photos/recognition.db ❯ fd \u0026#39;recognition.db\u0026#39; -x echo {.} Pictures/Photos/recognition Pictures/recognition ❯ fd \u0026#39;recognition.db\u0026#39; -x echo {/} recognition.db recognition.db ❯ fd \u0026#39;recognition.db\u0026#39; -x echo {//} Pictures Pictures/Photos ❯ fd \u0026#39;recognition.db\u0026#39; -x echo {/.} recognition recognition 通过占位符，可以很方便的对文件进行相关操作。更多的fd命令选项，可以查看 fd --help 。\n如果想在Windows上使用fd，可以通过 Scoop 包管理器安装，打开PowerShell\n-\u0026gt; scoop install fd 总结 这里介绍了两款在文件系统中通过文件名匹配快速查找文件的工具，mlocate使用了索引文件，所以在全局查找文件时的效率很高。而fd工具提供了很多的功能，在大部分场景下可代替find命令来使用，而性能也高于find命令。补充，fd工具的开发者除了这个工具外，还开发了其他很实用的工具，例如bat - 一个可以代替cat命令的工具，其他支持很多编程语言的语法高亮的形式来输出文件内容，也推荐使用。\n","date":"December 3, 2021","hero":"/zh/posts/opentool/find-files/find-file.png","permalink":"/zh/posts/opentool/find-files/","summary":"众所周知，在Linux或者类Unix的文件系统中，想通过文件名关键字查找文件，可以通过find命令。那本文将推荐2款可以快速查找文件的工具，性能比find命令更好，可在某些场景下替换find的使用。\nmlocate 大部分的Linux发行版的都提供了 mlocate 软件包，该软件包包含了一个locate命令用于查找文件，和一个updatedb命令用于更新文件索引供locate使用。\n可直接通过系统的软件包管理工具直接安装\n# CentOS/RHEL $ sudo dnf install mlocate # Debian/Ubuntu $ sudo apt install mlocate 安装完成后，首先需要执行以下命令进行文件索引\nsudo updatedb 索引文件将默认存放在 /var/lib/mlocate/mlocatedb ，也可以修改配置文件 /etc/updatedb.conf 文件，添加某些不需要索引的文件夹，例如\n# Paths which are pruned from updatedb database PRUNEPATHS=\u0026#34;/tmp /var/tmp /var/cache /var/lock /var/run /var/spool /mnt /cdrom /usr/tmp /proc /media /sys /.snapshots /var/run/media\u0026#34; 完成索引之后，就可以使用 locate 命令进行文件查找了，例如\n$ locate mlocate /etc/systemd/system/timers.target.wants/mlocate.timer /usr/bin/rpmlocate /usr/lib/systemd/system/mlocate.service /usr/lib/systemd/system/mlocate.timer /usr/sbin/rcmlocate /usr/share/doc/packages/mlocate /usr/share/doc/packages/mlocate/AUTHORS /usr/share/doc/packages/mlocate/ChangeLog /usr/share/doc/packages/mlocate/NEWS /usr/share/doc/packages/mlocate/README /usr/share/licenses/mlocate /usr/share/licenses/mlocate/COPYING /usr/share/man/man5/mlocate.db.5.gz /var/lib/mlocate /var/lib/mlocate/mlocate.","tags":["linux","find"],"title":"文件系统查找工具"},{"categories":["DEVOPS"],"contents":"之前我们通过一篇文章入门了使用Terrafrom以声明式配置文件（可版本化的代码）来创建和管理基础设施资源。\n在使用命令terraform apply之前，我们通常使用terraform plan来查看执行计划，输出的执行计划以类似“git diff”的文本方式描述。这里我们将介绍如何以图形可是化的方式来了解执行计划。\nTerrafrom Graph 首先Terraform CLI工具自带了一个子命令 - graph，graph命令用于生产配置和执行计划的图形表示，其输出是DOT格式，可以通过Graphviz转化为图片，例如在Linux终端下\n❯ terraform graph | dot -Tsvg \u0026gt; graph.svg 对于简单的项目（管理的资源对象比较的情况），我们可以通过这个图形了解资源对象的关系。但是如果一个项目管理了大量的资源对象，使用graph生成的图形会显得错中复杂，而且图形文件也比较庞大。\n那接下我们将介绍一款开源的可视化工具。\nRover Rover是一款开源的，可交互的Terraform配置和执行计划可视化工具，其通过Web服务的方式，是我们可以通过浏览器查看生成的图形，并进行一些交互操作。\n使用Rover非常容易，可以从其Github项目的Release下载为各平台编译好的二进制文件（命令）来运行，也可以通过Docker容器的方式运行。\n如果使用下载的二进制文件，将下载好的二进制文件（例如 rover_v0.2.2）放到PATH路径下，例如 /usr/local/bin/rover，接下來在Terraform项目的文件夹下执行\n❯ rover 2021/11/26 16:59:34 Starting Rover... 2021/11/26 16:59:34 Initializing Terraform... 2021/11/26 16:59:35 Generating plan... 2021/11/26 16:59:37 Parsing configuration... 2021/11/26 16:59:37 Generating resource overview... 2021/11/26 16:59:37 Generating resource map... 2021/11/26 16:59:37 Generating resource graph... 2021/11/26 16:59:37 Done generating assets. 2021/11/26 16:59:37 Rover is running on 0.0.0.0:9000 运行rover命令，其将会执行以下操作\n解析目录下的配置文件，并通过Terraform plan生成执行计划文件 解析计划和配置文件，生成3种对象： 资源概览（rso），资源映射图（map），资源图（graph） 使用上面的3中对象，将其转换为可交互的配置和状态视图，以Web服务器运行在本地的 9000 端口 我们可以通过浏览器访问 http://localhost:9000/ 来查看可视化的结果。\n整个页面包含4个部分\nLEGEND - 该部分是对图例的一些说明\nGRAPH - 这部分是整个资源关系和状态的视图，可使用鼠标进行缩放，拖拽，以及选择某一个资源\nRESOURCES - 资源文件的映射列表，现实了资源在配置文件中的定义位置，同时也可以使用鼠标进行选择\nDETAILS - 详细信息视图，当使用鼠标选择了 GRAPH 或者 RESOURCES 视图中的资源对象时，这里将现实其详细的信息\n通过与 terraform graph 生成的图形对比，Rover展示了更加丰富和美观的视图，让我们能以可视化的方式充分理解项目所管理的资源。\n注意的问题 Rover还不支持使用了“remote” - Backend的项目，因为还没办法将执行计划保存到本地 介绍视频 HashiCrop的工程师Tu Nguyen，也就是Rover的开发者，在 Youtube 上发布了他介绍Rover的视频\nVideo by Tu Nguyen From Youtube\n","date":"November 26, 2021","hero":"/zh/posts/devops/tf-rover/tf-plan.png","permalink":"/zh/posts/devops/tf-rover/","summary":"之前我们通过一篇文章入门了使用Terrafrom以声明式配置文件（可版本化的代码）来创建和管理基础设施资源。\n在使用命令terraform apply之前，我们通常使用terraform plan来查看执行计划，输出的执行计划以类似“git diff”的文本方式描述。这里我们将介绍如何以图形可是化的方式来了解执行计划。\nTerrafrom Graph 首先Terraform CLI工具自带了一个子命令 - graph，graph命令用于生产配置和执行计划的图形表示，其输出是DOT格式，可以通过Graphviz转化为图片，例如在Linux终端下\n❯ terraform graph | dot -Tsvg \u0026gt; graph.svg 对于简单的项目（管理的资源对象比较的情况），我们可以通过这个图形了解资源对象的关系。但是如果一个项目管理了大量的资源对象，使用graph生成的图形会显得错中复杂，而且图形文件也比较庞大。\n那接下我们将介绍一款开源的可视化工具。\nRover Rover是一款开源的，可交互的Terraform配置和执行计划可视化工具，其通过Web服务的方式，是我们可以通过浏览器查看生成的图形，并进行一些交互操作。\n使用Rover非常容易，可以从其Github项目的Release下载为各平台编译好的二进制文件（命令）来运行，也可以通过Docker容器的方式运行。\n如果使用下载的二进制文件，将下载好的二进制文件（例如 rover_v0.2.2）放到PATH路径下，例如 /usr/local/bin/rover，接下來在Terraform项目的文件夹下执行\n❯ rover 2021/11/26 16:59:34 Starting Rover... 2021/11/26 16:59:34 Initializing Terraform... 2021/11/26 16:59:35 Generating plan... 2021/11/26 16:59:37 Parsing configuration... 2021/11/26 16:59:37 Generating resource overview... 2021/11/26 16:59:37 Generating resource map... 2021/11/26 16:59:37 Generating resource graph... 2021/11/26 16:59:37 Done generating assets. 2021/11/26 16:59:37 Rover is running on 0.","tags":["terrafrom"],"title":"TF执行计划可视化"},{"categories":["DEVOPS"],"contents":"GitOps的概念以及提出了几年时间了，伴随着DevOps的发展也来越流行。简单地说，GitOps是一套操作和管理软件系统的原则，其源自于现代软件运维，也根植于之前存在和广泛采用的最佳实践。虽然其名字中包含Git，但其所表示的是与版本控制系统相关，而不仅限于Git工具。也可以说其是一个运维框架，它将 DevOps 最佳实践用于应用程序开发，例如版本控制、协作、合规性和 CI/CD 工具，并将它们应用于基础设施自动化。\n去年，来自Amazon和Azure，Codefresh，Github，Redhat，Weaveworks等具有云原生经验公司的工程师们在云原生计算基金会（CNCF）下组建了一个GitOps工作组，并创建了OpenGitOps项目。\nGitOps工作组在上个月（10/09）发布了OpenGitOps原则和术语1.0版本。\nOpenGitOps 1.0 GitOps工作组的联合主席 - Leonardo Murillo说过，“很多人认为他们在做GitOps，因为他们正在使用git，并且使用拉取请求（Pull Request）和推送更改（Push Changes）。而我们希望社区开始看到GitOps不仅仅是使用git的CI/CD流水线，其还包含很多”。\n工作组首先定义了GitOps的核心原则和相关术语，而我们可以用自己的方式自由地解释这些原则，并通过相关的DevOps工具实现最佳实践。\n发布的1.0版本由两个简单的原则和术语文档组成，每一个原则都讨论了系统的需求状态（Desired State）以及它应该如何运行。\nGitOps所管理的系统的需求状态必须满足\n声明式的（Declarative） - GitOps管理的系统必须以声明的方式表达其所需要的状态。 版本化和不可变（Versioned \u0026amp; Immutable） - 系统所需要的状态将以不可变、版本化的方式存储，以及保留完整的版本历史信息。 自动拉取（Pulled Automatically） - 软件代理会自动从源中提取所需的状态声明。 持续调节（Continuously Reconciled） - 软件代理持续观察系统状态并尝试达到系统所需要的状态。 进一步解读 声明式的状态应该很好理解，声明式的描述不应该包含如何达到需求状态的操作，而仅仅描述系统或者应用所需要的状态。例如可使用Terraform来描述和管理一个基础设施，也可以使用Kubernetes的应用部署文件（mainfest）来管理部署的应用。\n版本化和不可变通常理解为是用“git”，其实不仅限于此，这里更加强调的是使用具体的版本标签来定义系统的一个版本状态，而不应该使用例如“latest”之类的标签，因为其不能回退到之前所需要的状态。而其他版本管理系统，只要满足这样的版本标签功能，以及符合团队的协作方式，都可用于GitOps。\n自动拉取意味着我们必须有一个系统内的代理持续观测系统的状态，其需要时时刻刻知道系统所需要的状态和当前状态，而不是在触发确切的更改时才去获取系统得到状态。这里的拉取（Pull）明确地表明了其于CI/CD流水线由条件来触发的不同。\n而持续调节将以上三个部分整合，系统内的代理必须时刻了解管理的系统的实际状态和所需的状态，当发现实际状态和所需的状态由偏差时，或者发现所需状态被改变时（版本改变），其应该尽力时系统状态达到所需求的状态。而这一切都是由系统自动完成的。\n当团队遵循以上原则来管理和维护系统和应用时，才是对GitOps的真正实践。\nOpenGitOps发展 OpenGitOps 1.0仅仅定义了部分实现GitOps的原则，接下来还需要扩展更多的定义，例如如何在GitOps环境中处理事件管理，安全管理，凭证管理等。如果我们的基础设施环境以及我们的应该出现故障，那么在GitOps中应该遵循怎么样的处理原则？\n期待CNCF GitOps工作组的进一步工作。\n","date":"November 25, 2021","hero":"/zh/posts/devops/open-gitops/open-gitops.png","permalink":"/zh/posts/devops/open-gitops/","summary":"GitOps的概念以及提出了几年时间了，伴随着DevOps的发展也来越流行。简单地说，GitOps是一套操作和管理软件系统的原则，其源自于现代软件运维，也根植于之前存在和广泛采用的最佳实践。虽然其名字中包含Git，但其所表示的是与版本控制系统相关，而不仅限于Git工具。也可以说其是一个运维框架，它将 DevOps 最佳实践用于应用程序开发，例如版本控制、协作、合规性和 CI/CD 工具，并将它们应用于基础设施自动化。\n去年，来自Amazon和Azure，Codefresh，Github，Redhat，Weaveworks等具有云原生经验公司的工程师们在云原生计算基金会（CNCF）下组建了一个GitOps工作组，并创建了OpenGitOps项目。\nGitOps工作组在上个月（10/09）发布了OpenGitOps原则和术语1.0版本。\nOpenGitOps 1.0 GitOps工作组的联合主席 - Leonardo Murillo说过，“很多人认为他们在做GitOps，因为他们正在使用git，并且使用拉取请求（Pull Request）和推送更改（Push Changes）。而我们希望社区开始看到GitOps不仅仅是使用git的CI/CD流水线，其还包含很多”。\n工作组首先定义了GitOps的核心原则和相关术语，而我们可以用自己的方式自由地解释这些原则，并通过相关的DevOps工具实现最佳实践。\n发布的1.0版本由两个简单的原则和术语文档组成，每一个原则都讨论了系统的需求状态（Desired State）以及它应该如何运行。\nGitOps所管理的系统的需求状态必须满足\n声明式的（Declarative） - GitOps管理的系统必须以声明的方式表达其所需要的状态。 版本化和不可变（Versioned \u0026amp; Immutable） - 系统所需要的状态将以不可变、版本化的方式存储，以及保留完整的版本历史信息。 自动拉取（Pulled Automatically） - 软件代理会自动从源中提取所需的状态声明。 持续调节（Continuously Reconciled） - 软件代理持续观察系统状态并尝试达到系统所需要的状态。 进一步解读 声明式的状态应该很好理解，声明式的描述不应该包含如何达到需求状态的操作，而仅仅描述系统或者应用所需要的状态。例如可使用Terraform来描述和管理一个基础设施，也可以使用Kubernetes的应用部署文件（mainfest）来管理部署的应用。\n版本化和不可变通常理解为是用“git”，其实不仅限于此，这里更加强调的是使用具体的版本标签来定义系统的一个版本状态，而不应该使用例如“latest”之类的标签，因为其不能回退到之前所需要的状态。而其他版本管理系统，只要满足这样的版本标签功能，以及符合团队的协作方式，都可用于GitOps。\n自动拉取意味着我们必须有一个系统内的代理持续观测系统的状态，其需要时时刻刻知道系统所需要的状态和当前状态，而不是在触发确切的更改时才去获取系统得到状态。这里的拉取（Pull）明确地表明了其于CI/CD流水线由条件来触发的不同。\n而持续调节将以上三个部分整合，系统内的代理必须时刻了解管理的系统的实际状态和所需的状态，当发现实际状态和所需的状态由偏差时，或者发现所需状态被改变时（版本改变），其应该尽力时系统状态达到所需求的状态。而这一切都是由系统自动完成的。\n当团队遵循以上原则来管理和维护系统和应用时，才是对GitOps的真正实践。\nOpenGitOps发展 OpenGitOps 1.0仅仅定义了部分实现GitOps的原则，接下来还需要扩展更多的定义，例如如何在GitOps环境中处理事件管理，安全管理，凭证管理等。如果我们的基础设施环境以及我们的应该出现故障，那么在GitOps中应该遵循怎么样的处理原则？\n期待CNCF GitOps工作组的进一步工作。","tags":["gitops","cncf"],"title":"解读OPEN GITOPS 1.0"},{"categories":["DEVOPS"],"contents":"之前文章我们尝试了在本地环境使用Terraform来创建和管理AWS Lightsail资源，对于管理一些云资源，我们需要在本地安装相应的CLI工具和配置访问相应云资源的凭据（例如AWS CLI， AccessKeyID等），Terraform通过调用本地的CLI工具或者云API来管理云资源的状态，其默认使用的是local类型的Backend，资源的状态文件(.tfstate)也是保存在本地文件目录中的。\n这篇文章我们将尝试使用remote类型的Backend，将项目迁移到Terraform Cloud去执行，并且由Terraform Cloud来管理资源状态。\n什么是Terraform Cloud Terraform Cloud 是一个管理Terraform在一致且可靠的环境中运行的SaaS应用，从而可以替换在本地机器是执行Terraform项目，其存储共享的状态和机密数据，并可以连接到版本控制系统（如 Git)，使得我们可以在团队中将基础设施作为代码进行工作。\nTerraform是一个商业应用，团队和商业使用将会收取费用并提供了更多的高级功能。但对于个人用户，可以免费使用基本的功能。关于费用和功能详情，可以参考 (https://www.hashicorp.com/products/terraform/pricing)。\n首先我们需要注册一个Terraform Cloud的账号，访问 https://app.terraform.io/public/signup/account ，根据提示注册账号\n注册完成后第一次登陆Terraform Cloud，其会询问如何开始一个项目，这里我们选择 Start from scratch，也就是将从一个空模板开始\n接下来我们需要创建一个组织（Organization），例如这里我创建一个名为 learn-terraform 的组织，一个组织就类似要给命名空间，可以管理多个工作空间（Workspace），还可以管理其下工作空间共享的变量和环境变量。\n接下来我们需要在本地环境登录Terraform Cloud，并添加相应的配置来重新初始化项目。\n重新初始项目 完成了Terraform Cloud的账号注册之后，我们需要在本地终端运行 terraform login ，会打开浏览器来登录账号得到一个Token值，将其复制填入终端完成登录\n\u0026gt; terrafrom login Terraform must now open a web browser to the tokens page for app.terraform.io. If a browser does not open this automatically, open the following URL to proceed: https://app.terraform.io/app/settings/tokens?source=terraform-login --------------------------------------------------------------------------------- Generate a token using your browser, and copy-paste it into this prompt. Terraform will store the token in plain text in the following file for use by subsequent commands: /home/mengz/.terraform.d/credentials.tfrc.json Token for app.terraform.io: Enter a value: 接着我们修改项目的配置文件 main.tf ，加入 backend \u0026quot;remote\u0026quot;\nterraform { backend \u0026#34;remtoe\u0026#34; { organization = \u0026#34;learn-terraform\u0026#34; workspaces { name = \u0026#34;mylightsail\u0026#34; } } ... } 执行 terraform init ，Terraform将下载remote的插件，连接至Terraform Cloud 的 learn-terraform/mylightsail 工作空间，并将本地的状态文件迁移到云端\n$ terraform init Initializing the backend...Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \u0026#34;local\u0026#34; backend to the newly configured \u0026#34;remote\u0026#34; backend. No existing state was found in the newly configured \u0026#34;remote\u0026#34; backend. Do you want to copy this state to the new \u0026#34;remote\u0026#34; backend? Enter \u0026#34;yes\u0026#34; to copy and \u0026#34;no\u0026#34; to start with an empty state. Enter a value: yes Releasing state lock. This may take a few moments... Successfully configured the backend \u0026#34;remote\u0026#34;! Terraform will automaticallyuse this backend unless the backend configuration changes.... 浏览器访问Terraform Cloud WebUI，进入相应的工作空间可以查看状态信息。\n完成之后可以将本地的 .terraform/terraform.tfstate 文件删除。本地项目已将Terraform Cloud作为远程后端（remote backend），并且关联了命令行(CLI)驱动的方式，因此后续可以在本地更新资源配置文件，然后在本地运行 plan \u0026amp; apply 命令，这将会触发在远端Cloud上执行具体的状态维护工作。不过要使用Terraform Cloud来执行状态维护，我们还需要将AWS的访问凭据配置到Terraform Cloud上。\n配置工作空间的环境变量 使用Terraform Cloud来维护云资源（例如AWS），我们需要配置相应的访问凭据。这里我们需要配置AWS的 AWS_ACCESS_KEY_ID 和 AWS_SECRET_ACCESS_KEY 作为项目空间的环境变量。\n在工作空间点击 Variables 标签页，点击 + Add Varaible 按钮\n选择 Environment Variables ，然后添加 AWS_ACCESS_KEY_ID 和 AWS_SECRET_ACCESS_KEY 两个环境变量，并设置相应的值。\n完成之后，我们就可以本地控制台运行 terraform plan，terraform apply 将操作发送到Terraform Cloud端去运行，当然我们还是可以在本地项目执行 terraform show 来查看当前的状态，状态将会管理在云端\n\u0026gt; terraform plan Running plan in the remote backend. Output will stream here. Pressing Ctrl-C will stop streaming the logs, but will not stop the plan running remotely. Preparing the remote plan... To view this run in a browser, visit: https://app.terraform.io/app/mengz-infra/my-lightsail/runs/run-LzwFBbihffEKmucd Waiting for the plan to start... Terraform v1.0.11 on linux_amd64 Configuring remote state backend... Initializing Terraform configuration... aws_lightsail_static_ip.outline-sig-ip: Refreshing state... [id=Outline-EIP] aws_lightsail_instance.outline-sig: Refreshing state... [id=Outline-Sig] aws_lightsail_instance_public_ports.outline-sig-public-ports: Refreshing state... [id=Outline-Sig-987241840] aws_lightsail_static_ip_attachment.outline-sig-ip-attache: Refreshing state... [id=Outline-EIP] No changes. Your infrastructure matches the configuration. Terraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed. 可以看到计划是在 remote backend 运行的。\n本版化管理项目 最后，我们可以将项目的配置文件提交到版本控制系统（例如 Gitlab），并配置工作空间的版本控制\n在Terraform Cloud工作空间的设置里，按照提示配置关联相应的版本管理代码仓库。完成之后，我们在本地提交更新的代码后，会自动触发Terraform Cloud去执行维护新的状态。不过这将不会允许在本地终端执行 terraform apply\n\u0026gt; terraform apply │ Error: Apply not allowed for workspaces with a VCS connection │ A workspace that is connected to a VCS requires the VCS-driven workflow to ensure that the VCS remains the single source of truth. 只能通过更新代码，然后提交到远程代码仓库的方式来触发状态维护。这将更加便于与团队共享基础设施代码，以及共同维护基础设施状态，同时也更加趋于GitOps的工作方式。\n总结 本文基于上一篇文章 - 尝试使用Terraform在本地环境管理AWS Lightsail资源，延申了将状态管理的操作迁移到以Terraform Cloud作为远程后端的尝试，除了Terraform Cloud之后，还有其他类型的Backend，可以参考 (https://www.terraform.io/docs/language/settings/backends/index.html)。\n自此，我们初探了使用Terraform作为IaC工具来管理AWS Lightsail资源，当作Terraform学习的一个入门。Hashicrop官方提供了更多的学习资源和文档，想深入学习Terrform，并投入到实际工作中，还请参考官方文档。\n","date":"November 25, 2021","hero":"/zh/posts/devops/tf-cloud/tf-cloud.jpg","permalink":"/zh/posts/devops/tf-cloud/","summary":"之前文章我们尝试了在本地环境使用Terraform来创建和管理AWS Lightsail资源，对于管理一些云资源，我们需要在本地安装相应的CLI工具和配置访问相应云资源的凭据（例如AWS CLI， AccessKeyID等），Terraform通过调用本地的CLI工具或者云API来管理云资源的状态，其默认使用的是local类型的Backend，资源的状态文件(.tfstate)也是保存在本地文件目录中的。\n这篇文章我们将尝试使用remote类型的Backend，将项目迁移到Terraform Cloud去执行，并且由Terraform Cloud来管理资源状态。\n什么是Terraform Cloud Terraform Cloud 是一个管理Terraform在一致且可靠的环境中运行的SaaS应用，从而可以替换在本地机器是执行Terraform项目，其存储共享的状态和机密数据，并可以连接到版本控制系统（如 Git)，使得我们可以在团队中将基础设施作为代码进行工作。\nTerraform是一个商业应用，团队和商业使用将会收取费用并提供了更多的高级功能。但对于个人用户，可以免费使用基本的功能。关于费用和功能详情，可以参考 (https://www.hashicorp.com/products/terraform/pricing)。\n首先我们需要注册一个Terraform Cloud的账号，访问 https://app.terraform.io/public/signup/account ，根据提示注册账号\n注册完成后第一次登陆Terraform Cloud，其会询问如何开始一个项目，这里我们选择 Start from scratch，也就是将从一个空模板开始\n接下来我们需要创建一个组织（Organization），例如这里我创建一个名为 learn-terraform 的组织，一个组织就类似要给命名空间，可以管理多个工作空间（Workspace），还可以管理其下工作空间共享的变量和环境变量。\n接下来我们需要在本地环境登录Terraform Cloud，并添加相应的配置来重新初始化项目。\n重新初始项目 完成了Terraform Cloud的账号注册之后，我们需要在本地终端运行 terraform login ，会打开浏览器来登录账号得到一个Token值，将其复制填入终端完成登录\n\u0026gt; terrafrom login Terraform must now open a web browser to the tokens page for app.terraform.io. If a browser does not open this automatically, open the following URL to proceed: https://app.terraform.io/app/settings/tokens?source=terraform-login --------------------------------------------------------------------------------- Generate a token using your browser, and copy-paste it into this prompt.","tags":["terrafrom","cloud"],"title":"迁移本地项目到TF CLOUD"},{"categories":["DEVOPS"],"contents":"Terraform是一种开源基础设施及代码（IaC）的工具，可提供一致的CLI（命令行接口)工作流来管理数百个云服务，将云API编码为声明性的配置文件进行管理。\n本文创建一个管理AWS Lightsail实例的例子来入门Terraform的使用。\n安装Terraform CLI 要使用Terramform，首先要在本地系统安装Terraform命令行工具。HashiCorp提供了预编译好的二进制分发包，可以通过(https://www.terraform.com/downolads.html) 直接下载相应平台的二进制包，解压后放到相应的执行路径。也可以通过一些软件包管理工具安装，例如在Linux/OS X上通过LinuxBrew/HomeBrew进行安装，在Windows上通过Chocolatey进行安装。\n这里我们示例在Linux上是使用LinuxBrew进行安装\n\u0026gt; brew install terraform 安装完成后，可以查看其版本\n❯ terraform -version Terraform v1.0.11 on linux_amd64 使用-help查看其可用命令，安装成功后，我们就可以使用Terraform来创建相应的基础设施项目了。\nAWS账号准备 本文将通过创建一个管理AWS Lightsial实例的项目来尝试Terraform，因此需要一个AWS账号，以及在本地环境安装和配置好AWS CLI工具的访问凭据。\n安装和配置AWS CLI，请参考其文档 (https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html) 。 配置完成之后，可以在本地命令行终端访问相应的AWS资源。\n创建并初始化Terraform项目 Terraform在本地通过文件夹来管理一个基础设施项目的声明性代码，例如我们在本地创建一个文件夹\n\u0026gt; mkdir mylightsail \u0026gt; cd mylightsail/ 进入文件夹后，创建一个以 .tf 作为后缀的文件，例如 main.tf\n\u0026gt; touch main.tf 然后使用编辑器打开文件进行编辑，写入以下代码块\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 3.65\u0026#34; } } } # Configure the AWS Provider provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-southeast-1\u0026#34; } 其中 terraform/required_providers 块定义了该项目需要的 Provider，Terraform是通过不同的Provider来管理相应的基础设施资源的，可到 (https://registry.terraform.io) 来查找需要的Providers，例如GCP，Azure以及阿里云等的Providers。这里因为我们要管理的资源是AWS Lightsail实例，所以使用了Hashicorp官方提供的 hashicorp/aws 。\nprovider \u0026quot;aws\u0026quot; 部分配置了该Provider的一些可选项，例如这里配置了区域为 ap-southeast-1 ，因此请确保上面配置的AWS访问凭据能够操作该区域的资源。\n也就是这里我们定义了我们需要使用的Provider及其相应的选项配置，接下来我们需要使用 terraform init 命令来初始化项目\n\u0026gt; terraform init Initializing provider plugins... ... Terraform has been successfully initialized! 初始化将会从 (https://registry.terraform.io/) 下载相应的Provider插件到 .terramorm/providers/ 目录，供接下來的命令使用。\n同时，会生成一个 .terraform.lock.hcl 的文件来记录使用的具体Provider版本，其功能类似于NPM的package-lock文件，可将其提交到代码版本管理仓库，其他协作的成员就可以保持使用相同的插件版本。\n创建基础设施资源 完成项目的初始化之后，我们就可以编写需要创建的资源声明性配置，可以直接将相应的配置写入 main.tf 文件，也可以另外创建新的以 .tf 作为后缀的文件，这里我们创建一个新的名为 resources.tf 的文件，并写入我们需要的资源的定义\n## LightSail Resources resource \u0026#34;aws_lightsail_static_ip\u0026#34; \u0026#34;Example-sig-ip\u0026#34; { name = \u0026#34;Example-EIP\u0026#34; } resource \u0026#34;aws_lightsail_instance\u0026#34; \u0026#34;Example-sig\u0026#34; { name = \u0026#34;Example-Sig\u0026#34; availability_zone = \u0026#34;ap-southeast-1c\u0026#34; blueprint_id = \u0026#34;ubuntu_20_04\u0026#34; bundle_id = \u0026#34;nano_2_0\u0026#34; key_pair_name = \u0026#34;LightsailDefaultKeyPair\u0026#34; tags = { Example = \u0026#34;\u0026#34; } } resource \u0026#34;aws_lightsail_static_ip_attachment\u0026#34; \u0026#34;Example-sig-ip-attache\u0026#34; { static_ip_name = aws_lightsail_static_ip.Example-sig-ip.id instance_name = aws_lightsail_instance.Example-sig.id } resource \u0026#34;aws_lightsail_instance_public_ports\u0026#34; \u0026#34;Example-sig-public-ports\u0026#34; { instance_name = aws_lightsail_instance.Example-sig.name port_info { protocol = \u0026#34;tcp\u0026#34; from_port = 0 to_port = 65535 cidrs = [ \u0026#34;0.0.0.0/0\u0026#34; ] } port_info { protocol = \u0026#34;udp\u0026#34; from_port = 0 to_port = 65535 cidrs = [ \u0026#34;0.0.0.0/0\u0026#34; ] } } 定义资源的格式为 resource \u0026quot;[provider_resource _type]\u0026quot; \u0026quot;resource_name\u0026quot;，第一个参数为相应Provider支持的资源类型名称，第二个参数为自己定义的资源名称（可用于其他资源引用）。例如，我们首先定义了一个Lightsail的静态IP资源，其中参数 name 指定了AWS资源的名称。\n上面的定义中，我们声明了以下资源\n一个Lightsail静态IP地址 一个Lightsail计算实例，并绑定名为 LightsailDefaultKeyPair 的SSH密钥 静态IP地址和计算实例的绑定 实例的开放的网络端口组（类似于AWS EC2实例的安全组定义） 保存文件之后，我们可以使用 terraform fmt 命令来格式化文件格式，以及 terraform validate 来检查是否有语法错误。\n定义好我们想要的资源之后，我们先通过命令 terraform plan 命令来执行计划，查看具体的执行更改（plan不会实际操作相应的资源）\n❯ terraform plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_lightsail_instance.Example-sig will be created + resource \u0026#34;aws_lightsail_instance\u0026#34; \u0026#34;Example-sig\u0026#34; { + arn = (known after apply) + availability_zone = \u0026#34;ap-southeast-1c\u0026#34; + blueprint_id = \u0026#34;ubuntu_20_04\u0026#34; + bundle_id = \u0026#34;nano_2_0\u0026#34; + cpu_count = (known after apply) + created_at = (known after apply) + id = (known after apply) + ipv6_address = (known after apply) + ipv6_addresses = (known after apply) + is_static_ip = (known after apply) + key_pair_name = \u0026#34;LightsailDefaultKeyPair\u0026#34; + name = \u0026#34;Example-Sig\u0026#34; + private_ip_address = (known after apply) + public_ip_address = (known after apply) + ram_size = (known after apply) + tags = { + \u0026#34;Example\u0026#34; = \u0026#34;\u0026#34; } + tags_all = { + \u0026#34;Example\u0026#34; = (known after apply) } + username = (known after apply) } # aws_lightsail_instance_public_ports.Example-sig-public-ports will be created + resource \u0026#34;aws_lightsail_instance_public_ports\u0026#34; \u0026#34;Example-sig-public-ports\u0026#34; { + id = (known after apply) + instance_name = \u0026#34;Example-Sig\u0026#34; + port_info { + cidrs = [ + \u0026#34;0.0.0.0/0\u0026#34;, ] + from_port = 0 + protocol = \u0026#34;tcp\u0026#34; + to_port = 65535 } + port_info { + cidrs = [ + \u0026#34;0.0.0.0/0\u0026#34;, ] + from_port = 0 + protocol = \u0026#34;udp\u0026#34; + to_port = 65535 } } # aws_lightsail_static_ip.Example-sig-ip will be created + resource \u0026#34;aws_lightsail_static_ip\u0026#34; \u0026#34;Example-sig-ip\u0026#34; { + arn = (known after apply) + id = (known after apply) + ip_address = (known after apply) + name = \u0026#34;Example-EIP\u0026#34; + support_code = (known after apply) } # aws_lightsail_static_ip_attachment.Example-sig-ip-attache will be created + resource \u0026#34;aws_lightsail_static_ip_attachment\u0026#34; \u0026#34;Example-sig-ip-attache\u0026#34; { + id = (known after apply) + instance_name = (known after apply) + ip_address = (known after apply) + static_ip_name = (known after apply) } Plan: 4 to add, 0 to change, 0 to destroy. +表示将要增加的资源，(know after apply)的意思是要在具体只想（apply）之后，AWS根据定义创建相应资源之后才会返回的具体值。接下来就可以使用 terraform apply 来具体执行操作了，执行成功之后，会生成 .terraform/terraform.state 文件来记录执行后的资源状态，也可以通过命令 terraform show 命令来查看\n❯ terraform show # aws_lightsail_instance.Example-sig: resource \u0026#34;aws_lightsail_instance\u0026#34; \u0026#34;Example-sig\u0026#34; { arn = \u0026#34;arn:aws:lightsail:ap-southeast-1:090767794770:Instance/21cb0ea5-e814-4307-8606-01348d98be15\u0026#34; availability_zone = \u0026#34;ap-southeast-1c\u0026#34; blueprint_id = \u0026#34;ubuntu_20_04\u0026#34; bundle_id = \u0026#34;nano_2_0\u0026#34; cpu_count = 1 created_at = \u0026#34;2021-11-08T05:49:05Z\u0026#34; id = \u0026#34;Example-Sig\u0026#34; ipv6_address = \u0026#34;2406:da18:8ae:4b02:1f2:4ff1:daa1:6a8c\u0026#34; ipv6_addresses = [ \u0026#34;2406:da18:8ae:4b02:1f2:4ff1:daa1:6a8c\u0026#34;, ] is_static_ip = true key_pair_name = \u0026#34;LightsailDefaultKeyPair\u0026#34; name = \u0026#34;Example-Sig\u0026#34; private_ip_address = \u0026#34;172.26.45.249\u0026#34; public_ip_address = \u0026#34;54.220.33.133\u0026#34; ram_size = 0.5 tags = { \u0026#34;Example\u0026#34; = \u0026#34;\u0026#34; } tags_all = { \u0026#34;Example\u0026#34; = \u0026#34;\u0026#34; } username = \u0026#34;ubuntu\u0026#34; } # aws_lightsail_instance_public_ports.Example-sig-public-ports: resource \u0026#34;aws_lightsail_instance_public_ports\u0026#34; \u0026#34;Example-sig-public-ports\u0026#34; { id = \u0026#34;Example-Sig-987241840\u0026#34; instance_name = \u0026#34;Example-Sig\u0026#34; port_info { cidrs = [ \u0026#34;0.0.0.0/0\u0026#34;, ] from_port = 0 protocol = \u0026#34;tcp\u0026#34; to_port = 65535 } port_info { cidrs = [ \u0026#34;0.0.0.0/0\u0026#34;, ] from_port = 0 protocol = \u0026#34;udp\u0026#34; to_port = 65535 } } # aws_lightsail_static_ip.Example-sig-ip: resource \u0026#34;aws_lightsail_static_ip\u0026#34; \u0026#34;Example-sig-ip\u0026#34; { arn = \u0026#34;arn:aws:lightsail:ap-southeast-1:090767794770:StaticIp/3f0298e0-efeb-4429-9574-156fef12a48f\u0026#34; id = \u0026#34;Example-EIP\u0026#34; ip_address = \u0026#34;54.220.33.133\u0026#34; name = \u0026#34;Example-EIP\u0026#34; support_code = \u0026#34;313963776615/54.220.33.133\u0026#34; } # aws_lightsail_static_ip_attachment.Example-sig-ip-attache: resource \u0026#34;aws_lightsail_static_ip_attachment\u0026#34; \u0026#34;exmaple-sig-ip-attache\u0026#34; { id = \u0026#34;Example-EIP\u0026#34; instance_name = \u0026#34;Example-Sig\u0026#34; ip_address = \u0026#34;54.220.33.133\u0026#34; static_ip_name = \u0026#34;Example-EIP\u0026#34; } 之后我们就可以通过更新资源定义文件，然后执行 plan \u0026amp; apply 来更新云上的资源了，如果资源不需要了，我们也可以通过 terraform destroy 一条命令来销毁所有资源。\n总结 本文以管理简单的AWS Lightsail资源为例，展示了如何使用Terraform来管理云上的资源。通过一声明性的代码来定义我们需要的资源，同时资源定义代码可以通过版本管理工具进行版本化管理，进一步实现IaC的工作方式。\n这么我们使用的是Terraform的本地执行模式，除了在本地执行之外，Terraform还提供了执行后端（Backend）的功能，就可以将执行放到云上的一些执行环境，资源状态的管理也将会维护在Backend端，方便实现CI/CD Pipeline，以及GitOps的方式。其中Hashicorp的Terraform Cloud就是一个Backend。下一篇文章将示例将该项目放到Terraform Cloud上去执行。\n","date":"November 23, 2021","hero":"/zh/posts/devops/tf-aws-lightsail/tf-aws.jpg","permalink":"/zh/posts/devops/tf-aws-lightsail/","summary":"Terraform是一种开源基础设施及代码（IaC）的工具，可提供一致的CLI（命令行接口)工作流来管理数百个云服务，将云API编码为声明性的配置文件进行管理。\n本文创建一个管理AWS Lightsail实例的例子来入门Terraform的使用。\n安装Terraform CLI 要使用Terramform，首先要在本地系统安装Terraform命令行工具。HashiCorp提供了预编译好的二进制分发包，可以通过(https://www.terraform.com/downolads.html) 直接下载相应平台的二进制包，解压后放到相应的执行路径。也可以通过一些软件包管理工具安装，例如在Linux/OS X上通过LinuxBrew/HomeBrew进行安装，在Windows上通过Chocolatey进行安装。\n这里我们示例在Linux上是使用LinuxBrew进行安装\n\u0026gt; brew install terraform 安装完成后，可以查看其版本\n❯ terraform -version Terraform v1.0.11 on linux_amd64 使用-help查看其可用命令，安装成功后，我们就可以使用Terraform来创建相应的基础设施项目了。\nAWS账号准备 本文将通过创建一个管理AWS Lightsial实例的项目来尝试Terraform，因此需要一个AWS账号，以及在本地环境安装和配置好AWS CLI工具的访问凭据。\n安装和配置AWS CLI，请参考其文档 (https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html) 。 配置完成之后，可以在本地命令行终端访问相应的AWS资源。\n创建并初始化Terraform项目 Terraform在本地通过文件夹来管理一个基础设施项目的声明性代码，例如我们在本地创建一个文件夹\n\u0026gt; mkdir mylightsail \u0026gt; cd mylightsail/ 进入文件夹后，创建一个以 .tf 作为后缀的文件，例如 main.tf\n\u0026gt; touch main.tf 然后使用编辑器打开文件进行编辑，写入以下代码块\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 3.65\u0026#34; } } } # Configure the AWS Provider provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-southeast-1\u0026#34; } 其中 terraform/required_providers 块定义了该项目需要的 Provider，Terraform是通过不同的Provider来管理相应的基础设施资源的，可到 (https://registry.","tags":["terraform","aws"],"title":"TF管理AWS LIGHTSAIL实例"},{"categories":["Windows"],"contents":"现在Windows （10）是越來越向Linux靠近了，对于开发者开说，特别是在Windows上的Linux子系统非常好用。\nWSL2（Windows Subsystem for Linux ）是Windows 10上的一个工具，允许开发人员在Windows上直接运行Linux环境，使得在Windows系统上进行Linux的原生体验。\n对于WSL2，其底层通过微软的内置虚拟化技术（Hyper-V）实现Linux的环境。本文将一步步知道如何在Windows 10上启用WSL2，并安装一个Ubuntu 20.04分发版本的Linux。\n前提条件 想要在Windows 10上启用WLS2，需要满足以下条件：\nWindows 10 版本 1903 Build 19362，或高于该版本 如果是ARM64的系统，则需要版本2004 Build 19041，或高于该版本 步骤一　- 为WSL启用Windows服务 想要在Windows 10上运行WSL，首先需要启用Windows上的一些服务，这些服务默认是关闭的。\n开始菜单，搜索 PowerShell，右键 PowerShell，选择使用管理员运行。\n在打开的 PowerShell 终端，执行如下命令：\nPS C:\\Windows\\system32\u0026gt; dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart [dism.exe]是Windows的部署映像服务和管理工具，上面的命令开启了WSL的功能。\n以上命令执行成功之后，继续执行如下命令来开启Hyper-V的功能\nPS C:\\Windows\\system32\u0026gt; dism.exe /online /enable-feature /featurename:VirutalMachinePlatform /all /norestart 完成以上操作之后，需要重启Windows操作系统，重启之后再次登陆系统。\n接下来需要从微软下载一个最新的Linux内核升级包并安装，下载安装包 wsl_update_x64.msi，下载完成后直接安装。\n完成之后，以管理员身份运行 PowerShell，执行如下命令来设置wsl使用的默认版本\nPS C:\\Windows\\system32\u0026gt; wsl --set-default-version 2 这里我们将默认设置为 wsl 2 。\n上述步骤就完成了WSL2的启用，接下来将使用WSL2安装基于Linux的发行版本（Ubuntu 20.04）。\n步骤二　- 使用WSL安装Ubuntu 20.04 在开启WSL功能之后，安装一个Linux的分发版很简单，只需要打开Windows应用商店（Microsoft Store），这里我们将安装Ubuntu 20.04分发版。\n打开应用商店之后，直接在应用商店中搜索 Linux ，将看有很多分发版本的选项，这里选择 Ubuntu 20.04，点击 获取 将应用加入账号，然后在点击 安装 按钮进行安装。\n安装完成之后，就可以点击 启动 运行Ubuntu子系统，第一次运行需要一些时间来进行初始化配置，然后会提示输入Linux系统的用户名和密码。\n这里的用户名和秘密不需要与Windows系统的用户名和密码一致，但可以通过sudo来获取管理权限。\n当完成初始化之后，就可以使用该Linux子系统了，当然是以终端的方式。\n步骤三 - 安装Wdindows终端应用（Windows Terminal） 安装的Ubuntu子系统提供了一个默认的终端，不过微软开源了一个Windows上的终端工具 - Windows Terminal，该工具支持很多自定义配置，同时支持Windows的Powershell，也支持Linux子系统，因此可以安装使用。\n直接在应用商店搜索 Terminal ，选择 Windows Terminal 进行安装，安装完成之后可通过开始菜单启动\nWindows Terminal默认是打开Powershell的，不过其支持多标签，点击标题栏上 + 服务旁边的下拉按钮，选择Ubuntu-20.04，新标签就会打开Ubuntu的这个子系统终端\nWindows Termial支持很多自定义配置，具体请参考其文档。\n总结 现在我们在Windows 10上安装了一个全功能的Linux环境，可以使用该环境进行一些Linxu的系统实验，以及进行Linxu应用的开发（配合VSCode的wsl插件更加方便）。除了Ubuntu分发版，微软应用商店里还提供了其他支持的分发版本，包括Debian，openSUSE，Kali等，你也可以选择安装多个环境。\n","date":"June 8, 2021","hero":"/zh/posts/windows/windows10-wsl2/windows10-wsl2.jpg","permalink":"/zh/posts/windows/windows10-wsl2/","summary":"现在Windows （10）是越來越向Linux靠近了，对于开发者开说，特别是在Windows上的Linux子系统非常好用。\nWSL2（Windows Subsystem for Linux ）是Windows 10上的一个工具，允许开发人员在Windows上直接运行Linux环境，使得在Windows系统上进行Linux的原生体验。\n对于WSL2，其底层通过微软的内置虚拟化技术（Hyper-V）实现Linux的环境。本文将一步步知道如何在Windows 10上启用WSL2，并安装一个Ubuntu 20.04分发版本的Linux。\n前提条件 想要在Windows 10上启用WLS2，需要满足以下条件：\nWindows 10 版本 1903 Build 19362，或高于该版本 如果是ARM64的系统，则需要版本2004 Build 19041，或高于该版本 步骤一　- 为WSL启用Windows服务 想要在Windows 10上运行WSL，首先需要启用Windows上的一些服务，这些服务默认是关闭的。\n开始菜单，搜索 PowerShell，右键 PowerShell，选择使用管理员运行。\n在打开的 PowerShell 终端，执行如下命令：\nPS C:\\Windows\\system32\u0026gt; dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart [dism.exe]是Windows的部署映像服务和管理工具，上面的命令开启了WSL的功能。\n以上命令执行成功之后，继续执行如下命令来开启Hyper-V的功能\nPS C:\\Windows\\system32\u0026gt; dism.exe /online /enable-feature /featurename:VirutalMachinePlatform /all /norestart 完成以上操作之后，需要重启Windows操作系统，重启之后再次登陆系统。\n接下来需要从微软下载一个最新的Linux内核升级包并安装，下载安装包 wsl_update_x64.msi，下载完成后直接安装。\n完成之后，以管理员身份运行 PowerShell，执行如下命令来设置wsl使用的默认版本\nPS C:\\Windows\\system32\u0026gt; wsl --set-default-version 2 这里我们将默认设置为 wsl 2 。\n上述步骤就完成了WSL2的启用，接下来将使用WSL2安装基于Linux的发行版本（Ubuntu 20.04）。\n步骤二　- 使用WSL安装Ubuntu 20.","tags":["wsl"],"title":"Windows10上安装WSL2"},{"categories":["Windows"],"contents":"如今虚拟化已经非常流行，当我们使用Linux桌面环境时，可以通过安装libvirt和QEMU直接使用基于内核的虚拟化（KVM）来创建虚拟机并安装其他类型的操作系统。在基于Linux的服务器上，也可以通过oVirt或者PVE等基于KVM的虚拟化方案来实现虚拟机环境。\n当我们想通过官方iso系统镜像安装比较新的Windows（例如Windows 10，Windows Server 2019等），在进入到选择安装磁盘，会发现找不到创建的虚拟磁盘，如下图所示\n这是因为在官方的iso镜像中的Widnows未包含针对KVM的virtio-win驱动，因此我们可以基于Windows的iso镜像，加载virtio-win的相应驱动之后，重新创建一个包含了virtio-win驱动的iso镜像文件。\n关于virtio-win的更多信息，可以参考 https://www.linux-kvm.org/page/WindowsGuestDrivers/Download_Drivers 。\n前提条件 为了创建一个加载virtio-win驱动之后的iso镜像文件，我们需要以下准备：\n具有管理员权限的Windows 10工作系统并安装Windows ADK Windows 10的安装iso文件（这里以Windows 10作为例子） virtio-win驱动的iso文件 (https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/virtio-win-0.1.196-1/virtio-win-0.1.196.iso) UltraISO工具 准备工作 创建工作目录 假设在你的Windows 10系统上有D盘，那我们在D盘创建相应的工作目录，以管理员权限打开PowerShell，并执行\nPS D:\\\u0026gt; mkdir D:\\mnt\\windows_temp,D:\\mnt\\boot,D:\\mnt\\install,D:\\virtio-win 提取Windows安装文件 使用UltraISO工具打开windows 10的iso文件，并将所有文件提取到目录 *D:\\mnt\\windows_temp* 下\n然后给Windows的镜像文件授权读写\nPS D:\\\u0026gt; attrib -r C:\\mnt\\windows_temp\\sources\\*.wim /s 提取virtio驱动文件 使用UltraISO打开下载的virtio-win的iso文件，同样提取到目录 *D:\\virtio-win* 下，然后查看有哪些w10（针对windowns10）的驱动\n我们可以看到在 0.1.196 版本中，包含了以下w10（64位）的驱动，为了方便后面一条命令加载所有驱动，我们把这些驱动重新放到一个目录下\nPS D:\\\u0026gt; cd virtio-win\\ PS D:\\virtio-win\\\u0026gt; mkdir w10 PS D:\\virtio-win\\\u0026gt; cp -r .\\Balloon\\w10\\amd64\\ .\\w10\\Balloon PS D:\\virtio-win\\\u0026gt; cp -r .\\NetKVM\\w10\\amd64\\ .\\w10\\NetKVM PS D:\\virtio-win\\\u0026gt; cp -r .\\pvpanic\\w10\\amd64\\ .\\w10\\pvpanic PS D:\\virtio-win\\\u0026gt; cp -r .\\qemufwcfg\\w10\\amd64\\ .\\w10\\qemufwcfg PS D:\\virtio-win\\\u0026gt; cp -r .\\qemupciserial\\w10\\amd64\\ .\\w10\\qemupciserial PS D:\\virtio-win\\\u0026gt; cp -r .\\qxldod\\w10\\amd64\\ .\\w10\\qxldod PS D:\\virtio-win\\\u0026gt; cp -r .\\sriov\\w10\\amd64\\ .\\w10\\sriov PS D:\\virtio-win\\\u0026gt; cp -r .\\viofs\\w10\\amd64\\ .\\w10\\viofs PS D:\\virtio-win\\\u0026gt; cp -r .\\viogpudo\\w10\\amd64\\ .\\w10\\viogpudo PS D:\\virtio-win\\\u0026gt; cp -r .\\vioinput\\w10\\amd64\\ .\\w10\\vioinput PS D:\\virtio-win\\\u0026gt; cp -r .\\viorng\\w10\\amd64\\ .\\w10\\viorng PS D:\\virtio-win\\\u0026gt; cp -r .\\vioscsi\\w10\\amd64\\ .\\w10\\vioscsi PS D:\\virtio-win\\\u0026gt; cp -r .\\vioserial\\w10\\amd64\\ .\\w10\\vioserial PS D:\\virtio-win\\\u0026gt; cp -r .\\viostor\\w10\\amd64\\ .\\w10\\viostor 如果创建的是其他版本的Windows系统，例如Windows Server 2019，则提取对应目录（2k19/）下的驱动。\n加载virtio驱动 需要加载virtio驱动到Windows镜像文件 boot.wim 和 install.wim 。\n加载驱动到boot.wim 挂载提取出来的 D:\\mnt\\windows_temp\\sources\\boot.wim 文件到目录 *D:\\mnt\\boot* :\n先获取镜像文件的索引\nPS D:\\\u0026gt; Get-WindowsImage -ImagePath D:\\mnt\\windows_temp\\sources\\boot.wim ImageIndex : 1 ImageName : Microsoft Windows PE (x64) ImageDescription : Microsoft Windows PE (x64) ImageSize : 1,657,563,910 bytes ImageIndex : 2 ImageName : Microsoft Windows Setup (x64) ImageDescription : Microsoft Windows Setup (x64) ImageSize : 1,809,575,703 bytes 然后挂载索引2\nPS D:\\\u0026gt; Mount-WindowsImage -Path D:\\mnt\\boot\\ -ImagePath D:\\mnt\\windows_temp\\sources\\boot.wim -Index 2 Path : D:\\mnt\\boot\\ Online : False RestartNeeded : False 接下来就是加载我们之前提取出来的驱动\nPS D:\\\u0026gt; Add-WindowsDriver -Path D:\\mnt\\boot\\ -Driver D:\\virtio-win\\w10\\ -Recurse 可以使用下面的命令来查看驱动是否已经加载进去\nPS D:\\\u0026gt; Get-WindowsDriver -Path D:\\mnt\\boot\\ 确定加载成功后，卸载镜像挂载并保存\nPS D:\\\u0026gt; Dismount-WindowsImage -Path D:\\mnt\\boot\\ -Save 加载驱动到install.wim 同样，首先查看 install.wim 镜像的索引（不同索引指定了同一iso文件里的版本）\nPS D:\\\u0026gt; Get-WindowsImage -ImagePath D:\\mnt\\windows_temp\\sources\\install.wim ImageIndex : 1 ImageName : Windows 10 教育版 ImageDescription : Windows 10 教育版 ImageSize : 15,543,804,395 bytes ImageIndex : 2 ImageName : Windows 10 企业版 ImageDescription : Windows 10 企业版 ImageSize : 15,543,958,390 bytes ImageIndex : 3 ImageName : Windows 10 专业版 ImageDescription : Windows 10 专业版 ImageSize : 15,540,672,790 bytes ImageIndex : 4 ImageName : Windows 10 专业教育版 ImageDescription : Windows 10 专业教育版 ImageSize : 15,543,742,813 bytes ImageIndex : 5 ImageName : Windows 10 专业工作站版 ImageDescription : Windows 10 专业工作站版 ImageSize : 15,543,773,604 bytes 挂载想要加载驱动的版本，例如专业版\nPS D:\\\u0026gt; Mount-WindowsImage -Path D:\\mnt\\install\\ -ImagePath D:\\mnt\\windows_temp\\sources\\install.wim -Index 3 Path : D:\\mnt\\install\\ Online : False RestartNeeded ：False 然后加载驱动\nPS D:\\\u0026gt; Add-WindowsDriver -Path D:\\mnt\\install\\ -Driver D:\\virtio-win\\w10\\ -Recurse 卸载保存\nPS D:\\\u0026gt; Dismount-WindowsImage -Path D:\\mnt\\install\\ -Save 同理，可以继续挂载其他索引（版本），执行以上操作，加载驱动，然后保存。\n创建新的iso镜像文件 这里我们使用Windows ADK带的 oscdimg 工具来创建ISO文件，安装完Windows ADK工具之后，进入目录 *C:\\Program Files (x86)\\Windows Kits\\10\\Assessment and Deployment Kit\\Deployment Tools\\amd64\\Oscdimg*\nPS D:\\\u0026gt; cd \u0026#39;C:\\Program Files (x86)\\Windows Kits\\10\\Assessment and Deployment Kit\\Deployment Tools\\amd64\\Oscdimg\u0026#39; PS C:\\Program Files (x86)\\Windows Kits\\10\\Assessment and Deployment Kit\\Deployment Tools\\amd64\\Oscdimg\u0026gt; .\\oscdimg.exe -lcn_windows_10_virtio -m -u2 -bD:\\mnt\\windows_temp\\boot\\etfsboot.com D:\\mnt\\windows_temp\\ D:\\ISOFiles\\cn_windows_10_virtio_x64.iso 只想如上命令后，就会在目录 *D:\\ISOFiles* 目录下创建出一个名为 cn_windows_10_virtio_x64.iso 的文件。\n最后尝试使用新创建的iso文件来安装虚拟机系统，在选择选择磁盘驱动器时，我们就可以看到我们创建的虚拟磁盘了\n接下来就可以正常安装了。\n总结 本文以Windows 10的iso文件作为示例，演示了如何将virtio-win驱动加载进去，并且创建新的iso文件，使得在基于KVM的虚拟环境也可以顺利加载虚拟机磁盘进行Windows的安装。当然某些虚拟厂家也会直接提供相应的签名驱动，在安装过程中可以通过多增加一个虚拟CD驱动器来加载相应的驱动。\n","date":"June 8, 2021","hero":"/zh/posts/windows/windows10-virtio-image/window10-virtio.jpg","permalink":"/zh/posts/windows/windows10-virtio-image/","summary":"如今虚拟化已经非常流行，当我们使用Linux桌面环境时，可以通过安装libvirt和QEMU直接使用基于内核的虚拟化（KVM）来创建虚拟机并安装其他类型的操作系统。在基于Linux的服务器上，也可以通过oVirt或者PVE等基于KVM的虚拟化方案来实现虚拟机环境。\n当我们想通过官方iso系统镜像安装比较新的Windows（例如Windows 10，Windows Server 2019等），在进入到选择安装磁盘，会发现找不到创建的虚拟磁盘，如下图所示\n这是因为在官方的iso镜像中的Widnows未包含针对KVM的virtio-win驱动，因此我们可以基于Windows的iso镜像，加载virtio-win的相应驱动之后，重新创建一个包含了virtio-win驱动的iso镜像文件。\n关于virtio-win的更多信息，可以参考 https://www.linux-kvm.org/page/WindowsGuestDrivers/Download_Drivers 。\n前提条件 为了创建一个加载virtio-win驱动之后的iso镜像文件，我们需要以下准备：\n具有管理员权限的Windows 10工作系统并安装Windows ADK Windows 10的安装iso文件（这里以Windows 10作为例子） virtio-win驱动的iso文件 (https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/virtio-win-0.1.196-1/virtio-win-0.1.196.iso) UltraISO工具 准备工作 创建工作目录 假设在你的Windows 10系统上有D盘，那我们在D盘创建相应的工作目录，以管理员权限打开PowerShell，并执行\nPS D:\\\u0026gt; mkdir D:\\mnt\\windows_temp,D:\\mnt\\boot,D:\\mnt\\install,D:\\virtio-win 提取Windows安装文件 使用UltraISO工具打开windows 10的iso文件，并将所有文件提取到目录 *D:\\mnt\\windows_temp* 下\n然后给Windows的镜像文件授权读写\nPS D:\\\u0026gt; attrib -r C:\\mnt\\windows_temp\\sources\\*.wim /s 提取virtio驱动文件 使用UltraISO打开下载的virtio-win的iso文件，同样提取到目录 *D:\\virtio-win* 下，然后查看有哪些w10（针对windowns10）的驱动\n我们可以看到在 0.1.196 版本中，包含了以下w10（64位）的驱动，为了方便后面一条命令加载所有驱动，我们把这些驱动重新放到一个目录下\nPS D:\\\u0026gt; cd virtio-win\\ PS D:\\virtio-win\\\u0026gt; mkdir w10 PS D:\\virtio-win\\\u0026gt; cp -r .\\Balloon\\w10\\amd64\\ .\\w10\\Balloon PS D:\\virtio-win\\\u0026gt; cp -r .\\NetKVM\\w10\\amd64\\ .\\w10\\NetKVM PS D:\\virtio-win\\\u0026gt; cp -r .","tags":["virtualization"],"title":"加载virtio驱动的Windows10安装镜像"},{"categories":["Container"],"contents":"在我们的本地开发环境（Windows, Mac or Linux），我们可以很容易地使用Docker容器的方式，跑一个MQTT的Broker起来，方便我们的应用开发调试MQTT相关的功能。这里我们以Emqx作为MQTT的Broker来做示例。\n环境准备 首先需要在我们本地的Workstation上安装Docker环境:\nWindows - 如果是Windows 10，推荐使用Docker Desktop或者WSL2，如果是10一下版本，可以使用Docker Toolbox，或者在Windows上跑一个Linux虚拟机，直接在虚拟机里安装Docker Mac - 使用Docker Desktop Linux - 直接安装Docker引擎 如果选择使用Docker Desktop，可以参考 Windows上的Docker Desktop 。\nDocker运行emqx容器 Emqx的开发者已经构建了可用的容器映像，放在Docker Hub上，所以我们这里不需要自己构建映像，而是直接从Docker Hub上拉取\n$ docker image pull emqx/emqx:v4.0.5 如果是在Windows上使用Docker Desktop，则上面的命令是在PowerShell里执行。\n使用如下的命令直接启动一个emqx的容器\n$ docker image container run --name dev_emqx -d -p 18083:18083 -p 1883:1883 emqx/emqx:v4.0.5 启动成功后，我们可以使用命令查看运行情况\n$ docker container ls docker container ls CONTAINER ID NAMES IMAGE CREATED ago STATUS PORTS COMMAND 7c93940c07a3 dev_emqx emqx/emqx:v4.0.5 39 minutes ago ago Up 39 minutes 4369/tcp, 5369/tcp, 6369/tcp, 8080/tcp, 8083-8084/tcp, 8883/tcp, 0.0.0.0:1883-\u0026gt;1883/tcp, 0.0.0.0:18083-\u0026gt;18083/tcp, 11883/tcp \u0026#34;/usr/bin/docker-ent…\u0026#34; 我们可以看到映射了两个端口到我们的主机上\n1883 : mqtt端口，用户mqtt客户端连接 18083 : emqx的Web管理界面端口，可以访问 http://localhost:18083/ ，默认的用户名 admin，密码 public 这样就运行了一个单节点的MQTT Broker，下面我们使用[MQTT.js](https://www.npmjs.com/package/mqtt]的命令行客户端尝试连接，订阅/发布消息。\n客户端连接 首先使用npm安装MQTT.js，当然如果没有Node环境，也可以选择其他客户端\n$ npm install mqtt -g 连接一个客户端订阅一个主题（topic/hello）\n$ mqtt sub -h localhsot -p 1883 -t \u0026#39;topic/hello\u0026#39; -v 然后打开另一个终端向主题发布消息\n$ mqtt pub -h localhsot -p 1883 -t \u0026#39;topic/hello\u0026#39; -m \u0026#34;{\\\u0026#34;message\\\u0026#34;:\\\u0026#34;Hello\\\u0026#34;,\\\u0026#34;timestamp\\\u0026#34;:\\\u0026#34;$(date +\u0026#39;%s\u0026#39;)\\\u0026#34;}\u0026#34; 查看另一个订阅的终端，会收到该消息\n└❯ mqtt sub -h localhost -p 1883 -t \u0026#39;topic/hello\u0026#39; -v topic/hello {\u0026#34;message\u0026#34;:\u0026#34;Hello\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;1585189813\u0026#34;} 我们在订阅的时候，使用了-v选项，所以消息会打印出主题信息。\nMQTT客户端 Java client MQTTFx - 一个Java编写的MQTT图像界面客户端 MQTTX - 跨平台MQTT桌面客户端工具 ","date":"March 10, 2021","hero":"/zh/posts/container-tech/docker-mqtt-broker/docker-mqtt.webp","permalink":"/zh/posts/container-tech/docker-mqtt-broker/","summary":"在我们的本地开发环境（Windows, Mac or Linux），我们可以很容易地使用Docker容器的方式，跑一个MQTT的Broker起来，方便我们的应用开发调试MQTT相关的功能。这里我们以Emqx作为MQTT的Broker来做示例。\n环境准备 首先需要在我们本地的Workstation上安装Docker环境:\nWindows - 如果是Windows 10，推荐使用Docker Desktop或者WSL2，如果是10一下版本，可以使用Docker Toolbox，或者在Windows上跑一个Linux虚拟机，直接在虚拟机里安装Docker Mac - 使用Docker Desktop Linux - 直接安装Docker引擎 如果选择使用Docker Desktop，可以参考 Windows上的Docker Desktop 。\nDocker运行emqx容器 Emqx的开发者已经构建了可用的容器映像，放在Docker Hub上，所以我们这里不需要自己构建映像，而是直接从Docker Hub上拉取\n$ docker image pull emqx/emqx:v4.0.5 如果是在Windows上使用Docker Desktop，则上面的命令是在PowerShell里执行。\n使用如下的命令直接启动一个emqx的容器\n$ docker image container run --name dev_emqx -d -p 18083:18083 -p 1883:1883 emqx/emqx:v4.0.5 启动成功后，我们可以使用命令查看运行情况\n$ docker container ls docker container ls CONTAINER ID NAMES IMAGE CREATED ago STATUS PORTS COMMAND 7c93940c07a3 dev_emqx emqx/emqx:v4.0.5 39 minutes ago ago Up 39 minutes 4369/tcp, 5369/tcp, 6369/tcp, 8080/tcp, 8083-8084/tcp, 8883/tcp, 0.","tags":["docker","mqtt"],"title":"本地环境运行MQTT容器"},{"categories":["DEVOPS"],"contents":"前言 匆匆忙忙结束了2020年，想着还是要在2021年的第一个月要写一篇文章。在看了[Gitlab]推送的文章之后，自己也比较认同在DevOps实践中如果使用一个一体化的CI/CD平台或者工具所能带来的好处。\n不过每个组织或者团队有自身的一些考量和实际情况，就比如的当前我在工作中也使用了多个工具来实现整个CI/CD流程，似乎是分离了CI和CD的过程，因为公司更加考量了做交付的稳定性和安全性考量，并没有做到完全的持续发布。\n所以这篇文章仅仅是探讨一些一体化CI/CD平台可以带来的优势。\nCI/CD 持续继承和交付（CI/CD）改变了我们构建，测试和部署软件应用的方式。CI/CD工具自动化这些流程，减少错误率，并且优化了整个工作流。代码在整个开发阶段中进行自动化测试，已确保在错误到达生产之前就将其捕获并修复。\nCI/CD工具的使用将持续增加并改善软件开发的方式，应的部署也不再需要是每年一次，每个季度一次，甚至每个月一次。通过CI/CD，开发运维团队可以一天内部署多次。\n10个CI/CD的优点 上面说了什么是CI/CD，那接下来我们看看其有哪些优点。\n1. 更少的交接\n在开发管道中更多的交接，那将带来更多的故障点和更多的复杂性。\n2. 增加开发速度\n通过CI/CD，开发的所有阶段都更快，整个过程中更快的迭代速度使每个团队的工作效率更高，开发人员也可以更有信心地转移到其他项目。\n3. 更多的部署\n每两周或更长时间发生一次的发布，现在可以每天推送多次。\n4. 更快的测试\n开发工作流程中一个比较耗时的部分被移除，开发人员可以从事其他高价值的项目。自动化测试让团队可以更快速地获得反馈，并尽早失败，而不是在生产中产生这些错误，或者更糟糕地将错误带到最终的发布版本中。\n5. 更少的代码缺陷\n通过开发流程中引入自动化测试，在代码缺陷发生时就能及时捕获，并避免代码合并入主分支。这样可以确保整体上的具有更高的代码质量，并且每个发布版本都能按预期工作。\n6. 增强合规性\n合规性任务可以纳入开发生命周期，减少发布不合规应用的风险。自动化合规检查使得更容易完成审核，并可以防止高代价的错误。\n7. 更多的创新时间\n花费在集成维护的时间越少，IT支出不变，也就意味着可以更多的时间投入到其他地方。\n8. 更开心的开发者 开发人员能够更有自信地工作和快速地修复缺陷，而不是等上几个星期来了解错误发生在哪里。\n9. 减少管理费用\n组织中开发人员的时间是有限的，开发时间是可计费时间的很多倍，而手动测试和部署代码可能会破坏整个IT预算。自动化的工作流程减少了手动任务，可以使预算更加有效。\n. 流程一致性\n开发流程中具有更高的自动化程度意味着没人会忘记该过程中执行的步骤，也使得构建更加一致。这样更容易培训新的开发人员，组织也可以更加容易控制如何构建，以及何时进行发布。\n一体式CI/CD优势 上面概述了CI/CD可以带来的好处，然而在实际的实践中，如何真正地衡量生产力速度，需要评估整个软件开发生命周期（SDLC），而不仅仅是点点滴滴。\n无缝地持续集成和交付应具有使应用程序为部署做好准备所需的一切，不幸的是，很多使用CI/CD的组织由于使用的工具而陷入了无法达到最佳工作流程的困境。\n一个可以在整个软件开发生命周期提供可见性的应用程序是确保和优化每个开发阶段的最佳方法，当一切都在一个屋檐下时，就容易发现流程的瓶颈，以及评估影响开发速度的每一个元素。\n在单个DevOps平台下的每个步骤都提供了一条信息，可就是否准备好合并代码做出明智的决定。当然，所有这些步骤可以在没有一体式CI/CD的解决方案的情况下执行，但是功能全面的工具可以创建单一的事实来源，在此情况下，可以更快地监视每个步骤，并减少遗漏错误的可能性。\n一体式CI/CD平台带来以下好处：\n整个软件开发生命周期的可见性 所有开发阶段的单一真实来源 无需登陆多个应用程序 在一个界面上导航更简单 只需要安装，维护，扩展，备份一个应用程序 更容易的授权管理 降低运营费用 因此，推荐在组织或团队中构建单一的CI/CD平台来加速应用创新，例如选择使用 Gitlab，Github，以及各大公有云提供商发行的DevOps平台及工具来优化整个CI/CD流水线。\n","date":"January 25, 2021","hero":"/zh/posts/devops/single-application-cicd/single-application-cicd.png","permalink":"/zh/posts/devops/single-application-cicd/","summary":"前言 匆匆忙忙结束了2020年，想着还是要在2021年的第一个月要写一篇文章。在看了[Gitlab]推送的文章之后，自己也比较认同在DevOps实践中如果使用一个一体化的CI/CD平台或者工具所能带来的好处。\n不过每个组织或者团队有自身的一些考量和实际情况，就比如的当前我在工作中也使用了多个工具来实现整个CI/CD流程，似乎是分离了CI和CD的过程，因为公司更加考量了做交付的稳定性和安全性考量，并没有做到完全的持续发布。\n所以这篇文章仅仅是探讨一些一体化CI/CD平台可以带来的优势。\nCI/CD 持续继承和交付（CI/CD）改变了我们构建，测试和部署软件应用的方式。CI/CD工具自动化这些流程，减少错误率，并且优化了整个工作流。代码在整个开发阶段中进行自动化测试，已确保在错误到达生产之前就将其捕获并修复。\nCI/CD工具的使用将持续增加并改善软件开发的方式，应的部署也不再需要是每年一次，每个季度一次，甚至每个月一次。通过CI/CD，开发运维团队可以一天内部署多次。\n10个CI/CD的优点 上面说了什么是CI/CD，那接下来我们看看其有哪些优点。\n1. 更少的交接\n在开发管道中更多的交接，那将带来更多的故障点和更多的复杂性。\n2. 增加开发速度\n通过CI/CD，开发的所有阶段都更快，整个过程中更快的迭代速度使每个团队的工作效率更高，开发人员也可以更有信心地转移到其他项目。\n3. 更多的部署\n每两周或更长时间发生一次的发布，现在可以每天推送多次。\n4. 更快的测试\n开发工作流程中一个比较耗时的部分被移除，开发人员可以从事其他高价值的项目。自动化测试让团队可以更快速地获得反馈，并尽早失败，而不是在生产中产生这些错误，或者更糟糕地将错误带到最终的发布版本中。\n5. 更少的代码缺陷\n通过开发流程中引入自动化测试，在代码缺陷发生时就能及时捕获，并避免代码合并入主分支。这样可以确保整体上的具有更高的代码质量，并且每个发布版本都能按预期工作。\n6. 增强合规性\n合规性任务可以纳入开发生命周期，减少发布不合规应用的风险。自动化合规检查使得更容易完成审核，并可以防止高代价的错误。\n7. 更多的创新时间\n花费在集成维护的时间越少，IT支出不变，也就意味着可以更多的时间投入到其他地方。\n8. 更开心的开发者 开发人员能够更有自信地工作和快速地修复缺陷，而不是等上几个星期来了解错误发生在哪里。\n9. 减少管理费用\n组织中开发人员的时间是有限的，开发时间是可计费时间的很多倍，而手动测试和部署代码可能会破坏整个IT预算。自动化的工作流程减少了手动任务，可以使预算更加有效。\n. 流程一致性\n开发流程中具有更高的自动化程度意味着没人会忘记该过程中执行的步骤，也使得构建更加一致。这样更容易培训新的开发人员，组织也可以更加容易控制如何构建，以及何时进行发布。\n一体式CI/CD优势 上面概述了CI/CD可以带来的好处，然而在实际的实践中，如何真正地衡量生产力速度，需要评估整个软件开发生命周期（SDLC），而不仅仅是点点滴滴。\n无缝地持续集成和交付应具有使应用程序为部署做好准备所需的一切，不幸的是，很多使用CI/CD的组织由于使用的工具而陷入了无法达到最佳工作流程的困境。\n一个可以在整个软件开发生命周期提供可见性的应用程序是确保和优化每个开发阶段的最佳方法，当一切都在一个屋檐下时，就容易发现流程的瓶颈，以及评估影响开发速度的每一个元素。\n在单个DevOps平台下的每个步骤都提供了一条信息，可就是否准备好合并代码做出明智的决定。当然，所有这些步骤可以在没有一体式CI/CD的解决方案的情况下执行，但是功能全面的工具可以创建单一的事实来源，在此情况下，可以更快地监视每个步骤，并减少遗漏错误的可能性。\n一体式CI/CD平台带来以下好处：\n整个软件开发生命周期的可见性 所有开发阶段的单一真实来源 无需登陆多个应用程序 在一个界面上导航更简单 只需要安装，维护，扩展，备份一个应用程序 更容易的授权管理 降低运营费用 因此，推荐在组织或团队中构建单一的CI/CD平台来加速应用创新，例如选择使用 Gitlab，Github，以及各大公有云提供商发行的DevOps平台及工具来优化整个CI/CD流水线。","tags":["gitlab","cicd"],"title":"一体化CI/CD平台"},{"categories":["Kubernetes"],"contents":"我们都知道分布式系统非常难以管理，很大的一个原因是要是整个系统的可用性，是需要所有的部件（服务）都正常工作。如果一个小的部件不可用，系统应该可以检测到，绕过该部件，然后修复它，而且这样的行为应该可以自动进行。\n健康检查就是一个简单方法，使系统可以知道应用（服务）的一个实例是否正常工作。如果一个实例能正常工作，那其他服务不应该访问它或者向它发送请求，请求应该发送到健康的实例。而系统应该恢复应用的监控状态。\n当我们使用 Kubernetes 来运行和管理我们的应用（服务），默认情况下当一个Pod里的所有容器都启动后，就向该Pod发送相应的流量，并且当容器崩溃的时候重启容器。在一般情况下，这个行为也是可以接受的，不过k8s还提供了对容器的健康检查机制，可以让我们的部署更加健壮。\n在演示如何具体配置K8S的健康检查之前，让我们来看看什么健康探测模式（Health Probe Pattern）?\n健康探测模式 当我们设计一个关键任务，高可用的应用时，弹性是我们需要考虑的最重要方面之一。当一个应用能快速从失败中恢复，那个这个应用就是具有弹性的。\n为了保证基于k8s部署的应用是高可用的，在设计集群时，我们需要遵从特定的设计模式。而健康探测就是其中的一种模式，它定义了应用如何向系统（k8s）报告它自己的健康状态。\n这里所谓的健康状态不仅仅是Pod是否启动及运行，还应包括其是否可以正常处理请求并响应，这样k8s就可以更加合理地进行流量路由以及负载均衡。\nKubernetes的健康探测 我们都知道，k8s通过各种控制器对象（Deployment, StatefulSets等）来监控Pod的状态，如果一个控制器检测到Pod由于某些原因崩溃，它就会尝试重新启动Pod，或者把Pod调度到其他节点上进行启动。然而，Pod是可以报告自己的状态的，例如一个使用Nginx作为web服务器的应用，通过Deployment部署到集群里并正常启动，这个时候检测到Pod的的状态是运行着的，但是可能由于某些原因导致访问web服务时确返回500（内部服务错误），对请求者来说该服务是不可用的状态。\n默认情况下，k8s的kubelet继续地探测容器的进程，当探测到进程退出，它会重启容器的Pod，有些情况下重启可以让容器恢复正常。但像上面的例子，容器的进行正常运行，而应用却返回500错误，并不能正确地探测到应用的健康状态。\n因此，k8s提供了两个类型的探测： 存活探测（Liveness Probe），就绪探测（Readiness Probe）。\n存活探测（Liveness Probe） 很多应用在长时间运行，或者遇到某种错误进入死锁状态，除非重启，否则无法恢复。因此k8s提供存活探测（Liveness Probe）来发现并恢复这种状态，当然存活探测检查到错误时，kubelet将对Pod采取重启操作来恢复应用\n就绪探测（Readiness Probe） 有时应用会暂时性的不能对外提供网络服务，例如在负载比较大的是偶，或者在应用启动的时候可能需要加载大量数据或做一些初始化的动作，需要一定时间来准备对外提供服务。\n这样的情况下，系统探测到应用实例不可用时，不应该杀死应用重启，而是应该分配流量，不往该实例发送请求（通过配置服务负载）。\n因此，k8s提供了就绪探测来发现并处理这种情况，发现Pod里的容器为就绪时，会设置应用的service(k8s资源对行)移除该实例的Endpoint（服务端点），使得流量然过该不可用的服务实例，等待探测起就绪后，再将其端点添加回相应的服务。当然如果应用是第一次启动，则会等待就绪探测成功后才会将其添加到服务端点。\nKubernetes探测方法 那系统如何来探测容器的健康状态呢？k8s支持配置三种探测方法： 执行命令，TCP，HTTP 。\n三种方法都可以应用到存活和就绪探测。\n执行命令 通过在容器内执行命令来判断容器的状态，如果命令返回值为 0，则认为容器是健康的；如果返回值为非 0，则认为容器是不健康的。\n这种方式一般在容器运行的应用没有提供HTTP的服务，也没有任何TCP端口启动来提供服务的情况，而可以通过运行一个命令来确定应用是否健康。\n下面是配置一个Pod使用命令\napiVersion: v1 kind: Pod metadata: name: app spec: containers: - image: example/app:v1 name: app livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 上面的示例就使用命令 cat /tmp/healthy 来进行存货探测，如果容器里不存在 /tmp/healthy 文件，则命令返回非0值，k8s则认为容器不健康，这里使用了存活探测，因此会重启该Pod。\nTCP 那TCP方式，就是通过尝试向容器监听的端口建立TCP连接来确定其是否健康，如果能成功建立连接，则认为健康，否则不健康。\napiVersion: v1 kind: Pod metadata: name: app spec: containers: - image: example/app:v1 name: app ports: - containerPort: 3000 protocol: TCP livenessProbe: tcpSocket: port: 3000 initialDelaySeconds: 15 periodSeconds: 20 上面示例了通过配置TCP方式的存活探测，k8s会通过检查Pod端口3000来确认容器是否健康，上面的配置中有两个参数：\ninitialDelaySeconds - Pod启动时等待多少时间（15秒）后开始进行检查，该参数用于应用需要一定时间启动的情况，避免一开始就检查失败而导致Pod重启 periodSeconds - 进行端口检查的周期时间，也就是在Pod运行时，每20秒进行一次检查 HTTP 如果应用是一个HTTP服务（或者实现了一个HTTP服务的API来报告健康状态），则可以通过配置HTTP的方式来进行探测\napiVersion: v1 kind: Pod metadata: name: app spec: containers: - image: exmaple/app:v1 name: app ports: - containerPort: 8080 protocol: TCP livenessProbe: httpGet: path: / port: 3000 initialDelaySeconds: 5 periodSeconds: 20 HTTP方式通过向容器的端口和指定路径发送http请求，如果请求的返回值是 200 - 300 之间，则认为是成功的，如果返回值为其他值，例如500，则认为是失败的。\n健康探测还可以配置更多参数来做判定，详细信息可参考 Kubernetes文档 。\n总结 在云原生应用之前，主要通过应用日志来监控和分析应用的健康状态，然而当应用不可用时没有什么方法来自动恢复应用。当然日志依然非常有用的，应次也需要日志聚合系统来收集所有应用的日志进行观测和分析应用程序行为。\n对于分布式系统，以及基于容器的微服务架构云原生应用，我们需要更加快速地将应用从失败中恢复，保证系统的健壮性。因此在进行应用设计的时候，我们应该考虑合适的健康探测模式，在通过Kubernetes进行应用编排的时候，也要尽可能地对容器进行健康状态的探测，是的k8s在发现应用实例不可用的时候，采取保证系统高可用的操作。\n参考：\n配置存活、就绪和启动探测器 Kubernetes best practices: Setting up health checks with readiness and liveness probes ","date":"December 8, 2020","hero":"/zh/posts/k8s/health-checks/kubernetes-health-checks.png","permalink":"/zh/posts/k8s/health-checks/","summary":"我们都知道分布式系统非常难以管理，很大的一个原因是要是整个系统的可用性，是需要所有的部件（服务）都正常工作。如果一个小的部件不可用，系统应该可以检测到，绕过该部件，然后修复它，而且这样的行为应该可以自动进行。\n健康检查就是一个简单方法，使系统可以知道应用（服务）的一个实例是否正常工作。如果一个实例能正常工作，那其他服务不应该访问它或者向它发送请求，请求应该发送到健康的实例。而系统应该恢复应用的监控状态。\n当我们使用 Kubernetes 来运行和管理我们的应用（服务），默认情况下当一个Pod里的所有容器都启动后，就向该Pod发送相应的流量，并且当容器崩溃的时候重启容器。在一般情况下，这个行为也是可以接受的，不过k8s还提供了对容器的健康检查机制，可以让我们的部署更加健壮。\n在演示如何具体配置K8S的健康检查之前，让我们来看看什么健康探测模式（Health Probe Pattern）?\n健康探测模式 当我们设计一个关键任务，高可用的应用时，弹性是我们需要考虑的最重要方面之一。当一个应用能快速从失败中恢复，那个这个应用就是具有弹性的。\n为了保证基于k8s部署的应用是高可用的，在设计集群时，我们需要遵从特定的设计模式。而健康探测就是其中的一种模式，它定义了应用如何向系统（k8s）报告它自己的健康状态。\n这里所谓的健康状态不仅仅是Pod是否启动及运行，还应包括其是否可以正常处理请求并响应，这样k8s就可以更加合理地进行流量路由以及负载均衡。\nKubernetes的健康探测 我们都知道，k8s通过各种控制器对象（Deployment, StatefulSets等）来监控Pod的状态，如果一个控制器检测到Pod由于某些原因崩溃，它就会尝试重新启动Pod，或者把Pod调度到其他节点上进行启动。然而，Pod是可以报告自己的状态的，例如一个使用Nginx作为web服务器的应用，通过Deployment部署到集群里并正常启动，这个时候检测到Pod的的状态是运行着的，但是可能由于某些原因导致访问web服务时确返回500（内部服务错误），对请求者来说该服务是不可用的状态。\n默认情况下，k8s的kubelet继续地探测容器的进程，当探测到进程退出，它会重启容器的Pod，有些情况下重启可以让容器恢复正常。但像上面的例子，容器的进行正常运行，而应用却返回500错误，并不能正确地探测到应用的健康状态。\n因此，k8s提供了两个类型的探测： 存活探测（Liveness Probe），就绪探测（Readiness Probe）。\n存活探测（Liveness Probe） 很多应用在长时间运行，或者遇到某种错误进入死锁状态，除非重启，否则无法恢复。因此k8s提供存活探测（Liveness Probe）来发现并恢复这种状态，当然存活探测检查到错误时，kubelet将对Pod采取重启操作来恢复应用\n就绪探测（Readiness Probe） 有时应用会暂时性的不能对外提供网络服务，例如在负载比较大的是偶，或者在应用启动的时候可能需要加载大量数据或做一些初始化的动作，需要一定时间来准备对外提供服务。\n这样的情况下，系统探测到应用实例不可用时，不应该杀死应用重启，而是应该分配流量，不往该实例发送请求（通过配置服务负载）。\n因此，k8s提供了就绪探测来发现并处理这种情况，发现Pod里的容器为就绪时，会设置应用的service(k8s资源对行)移除该实例的Endpoint（服务端点），使得流量然过该不可用的服务实例，等待探测起就绪后，再将其端点添加回相应的服务。当然如果应用是第一次启动，则会等待就绪探测成功后才会将其添加到服务端点。\nKubernetes探测方法 那系统如何来探测容器的健康状态呢？k8s支持配置三种探测方法： 执行命令，TCP，HTTP 。\n三种方法都可以应用到存活和就绪探测。\n执行命令 通过在容器内执行命令来判断容器的状态，如果命令返回值为 0，则认为容器是健康的；如果返回值为非 0，则认为容器是不健康的。\n这种方式一般在容器运行的应用没有提供HTTP的服务，也没有任何TCP端口启动来提供服务的情况，而可以通过运行一个命令来确定应用是否健康。\n下面是配置一个Pod使用命令\napiVersion: v1 kind: Pod metadata: name: app spec: containers: - image: example/app:v1 name: app livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 上面的示例就使用命令 cat /tmp/healthy 来进行存货探测，如果容器里不存在 /tmp/healthy 文件，则命令返回非0值，k8s则认为容器不健康，这里使用了存活探测，因此会重启该Pod。\nTCP 那TCP方式，就是通过尝试向容器监听的端口建立TCP连接来确定其是否健康，如果能成功建立连接，则认为健康，否则不健康。","tags":["kubernetes","kubectl"],"title":"K8S健康检查最佳实践"},{"categories":["Linux"],"contents":"什么是 WireGuard ？ 其官方宣称是快速、现代以及安全的VPN隧道（Fast, Modern, Secure VPN Tunnel）。\nWireGuard使用了最先进的加密技术，相比 IPSec 更简单更精简，而且拥有几乎超越 OpenVPN 的性能。其最初是针对Linux内核发布的，但是现在已经跨平台（Windows, MacOS, BSD, Android, iOS等）可部署。\n接下来这篇How To系列文章，就来一步步在Ubuntu (Linux)上安装和配置WireGuard VPN，其中一台云主机运行Ubuntu-20.04用作VPN服务器，另一台本地的linux桌面环境作为VPN客户端。\n服务器端安装WireGuard 这里我们的服务器使用的是操作系统为Ubuntu 20.04的云主机，对于如何创建并配置一台云主机，可以选择 [DigitalOcean]（https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-20-04）。\n这里我们我们已经配置好一台Ubuntu 20.04的云主机，并且可以通过SSH访问。首先对系统进行安全更新\n$ sudo apt update $ sudo apt upgrade 接下来直接使用APT安装WireGuard软件包\n$ sudo apt install wireguard 会同时安装 wireguard-tools 软件包，我们需要使用其工具进行相关的配置。\n配置WireGuard服务端 进入root权限进行操作，为服务端生产私有/公共密钥对\n$ sudo -i \\# cd /etc/wireguard/ \\# umask 077 \\# wg genkey | tee privatekey | wg pubkey \u0026gt; publickey 执行完上述命令后，我们会在目录 /etc/wireguard/ 下生产两个密钥文件 privatekey 和 publickey 。\n接下来我们需要创建一个接口配置文件，命名为 wg0.conf，编辑并添加如下内容\n[Interface] ## VPN server private IP address ## Address = 192.168.6.1/24 ## VPN server port ## ListenPort = 4114 ## VPN server\u0026#39;s private key i.e. /etc/wireguard/privatekey ## PrivateKey = xxxxxxxxxxxxxxxxxxxxxxxxxxxx 其中 Address 指定该网络接口的IP地址，可自己设定一个子网的地址，ListenPort 指定WG的UDP监听端口（客户端将需要配置该端口进行连接），PrivateKey 就是上面生成的私有密钥的内容。\n如果你的系统开启了UFW防火墙，记住要配置ufw允许以上端口（4114），例如\n$ sudo ufw status Status: active $ sudo ufw allow 4114/udp 我们这里使用的是云主机，则需要在云控制台配置相应的安全组/防火墙开放 4114/udp 端口的访问。\n接下来，执行以下命令启动WireGuard服务\n$ sudo systemctl enable wg-quick@wg0 $ sudo systemctl start wg-quick@wg0 $ sudo systemcl status wg-quick@wg0 最后一个命令是查看wg服务的运行状态的，例如返回如下\n● wg-quick@wg0.service - WireGuard via wg-quick(8) for wg0 Loaded: loaded (/lib/systemd/system/wg-quick@.service; enabled; vendor preset: enabled) Active: active (exited) since Thu 2020-12-03 06:48:02 UTC; 5s ago Docs: man:wg-quick(8) man:wg(8) https://www.wireguard.com/ https://www.wireguard.com/quickstart/ https://git.zx2c4.com/wireguard-tools/about/src/man/wg-quick.8 https://git.zx2c4.com/wireguard-tools/about/src/man/wg.8 Process: 3606 ExecStart=/usr/bin/wg-quick up wg0 (code=exited, status=0/SUCCESS) Main PID: 3606 (code=exited, status=0/SUCCESS) Tasks: 0 (limit: 682) Memory: 800.0K CGroup: /system.slice/system-wg\\x2dquick.slice/wg-quick@wg0.service 而且系统里增加了一个虚拟的网络接口\n3: wg0: \u0026lt;POINTOPOINT,NOARP,UP,LOWER_UP\u0026gt; mtu 1380 qdisc noqueue state UNKNOWN group default qlen 1000 link/none inet 192.168.40.1/24 scope global wg0 valid_lft forever preferred_lft forever 还可以通过wg命令显示接口信息\n$ sudo wg interface: wg0 public key: rQYpYVpgPYdYYVrlgF52S/M8vrht+rkqSmn5ayVAG0I= private key: (hidden) listening port: 4114 配置客户端连接WG 完成服务器断的配置，并成功启动之后，我们需要配置本地客户端来连接该VPN，我本地使用的是 [openSUSE] 桌面环境，首先需要安装 wireguard-tools，其他 Linux 系统也是类似的\n❯ sudo zypper in wireguard-tools 正在加载软件源数据... 正在读取已安装的软件包... 正在解决软件包依赖关系... 将安装以下 1 个新软件包： wireguard-tools 1 个软件包将新装. 总下载大小：72.4 KiB。已缓存：0 B。 操作完成后，将使用额外的 140.7 KiB。 继续吗？ [y/n/v/...? 显示全部选项] (y): y 一样地需要创建私有/公共密钥对\n$ sudo -i \\# cd /etc/wireguard/ \\# umask 077 \\# wg genkey | tee privatekey | wg pubkey \u0026gt; publickey 创建 /etc/wireguard/wg0.conf 文件，配置如下\n[Interface] ## This Desktop/client\u0026#39;s private key ## PrivateKey = \u0026lt;client private key\u0026gt; ## Client ip address ## Address = 192.168.40.10/24 [Peer] ## public key ## PublicKey = \u0026lt;Server public key\u0026gt; ## set ACL ## AllowedIPs = 192.168.40.0/24 ## Your Ubuntu 20.04 LTS server\u0026#39;s public IPv4/IPv6 address and port ## Endpoint = 35.220.179.202:4114 ## Key connection alive ## PersistentKeepalive = 15 注意需要分别配置接口（也就是客户端自己）的私有密钥，还要配置连接服务端（peer）的公共密钥，还有连接服务端的IP和端口信息。\n启动连接\n❯ sudo systemctl start wg-quick@wg0.service ❯ sudo wg interface: wg0 public key: 1abmCvigQqhXYLOkvjrU864dOyJHN9bf6Ya0GP4tXzs= private key: (hidden) listening port: 54147 peer: rQYpYVpgPYdYYVrlgF52S/M8vrht+rkqSmn5ayVAG0I= endpoint: 35.220.179.202:4114 allowed ips: 192.168.40.0/24 latest handshake: 4 seconds ago transfer: 92 B received, 180 B sent persistent keepalive: every 15 seconds 这个时候还不能通过VPN直接访问服务器端，想要客户端和服务端可以通过VPN的私有网络互相访问，这个时候需要到服务端添加[Peer]的配置，服务端向文件 /etc/wireguard/wg0.conf 添加以下内容\n[Peer] PublicKey = +pztufezkYV8ujhJI2N2Q5SW5yuuTXzHmytrGdJjziE= AllowedIPs = 192.168.40.10/32 重启服务器，客户端服务\n$ sudo systemctl restart wg-quick@wg0 这个时候就建立了一个P-to-P的VPN网络，可以使用私有IP互相访问\n❯ ping 192.168.40.1 PING 192.168.40.1 (192.168.40.1) 56(84) bytes of data. 64 bytes from 192.168.40.1: icmp_seq=2 ttl=64 time=51.8 ms 64 bytes from 192.168.40.1: icmp_seq=3 ttl=64 time=55.0 ms 64 bytes from 192.168.40.1: icmp_seq=4 ttl=64 time=58.6 ms ... 客户端还不能通过VPN网络访问互联网，要想所有VPN的客户端都可以通过该服务器来访问互联网，还需要在服务器上配置相应的NAT，IP转发规则。\n总结 这里仅仅通过示例，在Linux上使用 WireGuard 创建了一个端到端（客户端/服务器）VPN网络，让客户端和服务器可以通过VPN私有IP地址互相访问。想了解更多关于WireGuard的信息，请访问其官网 https://www.wireguard.com/ 。\n","date":"December 3, 2020","hero":"/zh/posts/linux/wireguard-vpn/linux-wireguard.png","permalink":"/zh/posts/linux/wireguard-vpn/","summary":"什么是 WireGuard ？ 其官方宣称是快速、现代以及安全的VPN隧道（Fast, Modern, Secure VPN Tunnel）。\nWireGuard使用了最先进的加密技术，相比 IPSec 更简单更精简，而且拥有几乎超越 OpenVPN 的性能。其最初是针对Linux内核发布的，但是现在已经跨平台（Windows, MacOS, BSD, Android, iOS等）可部署。\n接下来这篇How To系列文章，就来一步步在Ubuntu (Linux)上安装和配置WireGuard VPN，其中一台云主机运行Ubuntu-20.04用作VPN服务器，另一台本地的linux桌面环境作为VPN客户端。\n服务器端安装WireGuard 这里我们的服务器使用的是操作系统为Ubuntu 20.04的云主机，对于如何创建并配置一台云主机，可以选择 [DigitalOcean]（https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-20-04）。\n这里我们我们已经配置好一台Ubuntu 20.04的云主机，并且可以通过SSH访问。首先对系统进行安全更新\n$ sudo apt update $ sudo apt upgrade 接下来直接使用APT安装WireGuard软件包\n$ sudo apt install wireguard 会同时安装 wireguard-tools 软件包，我们需要使用其工具进行相关的配置。\n配置WireGuard服务端 进入root权限进行操作，为服务端生产私有/公共密钥对\n$ sudo -i \\# cd /etc/wireguard/ \\# umask 077 \\# wg genkey | tee privatekey | wg pubkey \u0026gt; publickey 执行完上述命令后，我们会在目录 /etc/wireguard/ 下生产两个密钥文件 privatekey 和 publickey 。","tags":["wireguard","vpn"],"title":"在LINUX上配置WIREGUARD"},{"categories":["Linux"],"contents":"在Linux系统上，我们可以通过 ip , netstat 或者 [ethtool] 命令显示网络接口丢弃数据包的统计信息。接下来我们看看如何使用每个命令。\n使用netstat按接口显示数据包 其实 netstat 命令已经过时，可使用命令 ip 和 ss 来代替。但是 netstat 依然在一些旧的Linux分发版本上可用，因此在 ip/ss 不可用的情况，我们可以使用netstat，其语法如下\nnetstat -i netstat --interfaces 例如\n~$ netstat -i Kernel Interface table Iface MTU Met RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flg docker0 1500 0 188180 0 0 0 151852 0 0 0 BMRU eth0 1500 0 472368 0 0 0 375351 0 0 0 BMRU lo 65536 0 51687 0 0 0 51687 0 0 0 LRU vethc8f46ea 1500 0 136984 0 0 0 79587 0 0 0 BMRU 如果想显示每种协议的概要统计信息，可以执行\nnetstat -s netstat --statistics 例如\n$ netstat -s Ip: 527622 total packets received 19 with invalid addresses 329762 forwarded 0 incoming packets discarded 191137 incoming packets delivered 568337 requests sent out Icmp: 8 ICMP messages received 8 input ICMP message failed. ICMP input histogram: destination unreachable: 7 timeout in transit: 1 5 ICMP messages sent 0 ICMP messages failed ICMP output histogram: destination unreachable: 5 IcmpMsg: InType3: 7 InType11: 1 OutType3: 5 Tcp: 2509 active connections openings 26 passive connection openings 748 failed connection attempts 14 connection resets received 4 connections established 182968 segments received 241886 segments send out 72 segments retransmited 279 bad segments received. 1844 resets sent InCsumErrors: 279 Udp: 8067 packets received 5 packets to unknown port received. 0 packet receive errors 11440 packets sent 只显示tcp的信息\nnetstat -s -t netstat --statistics --tcp 只显示udp的信息\nnetstat -s -u netstat --statistics --udp 使用ip命令显示网络接口数据包信息 如果要显示所有接口的统计信息，命令如下\nip -s link 如果要显示某一个接口的，则制定接口名\nip -s link show {interface} 例如\n$ ip -s link show eth0 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 00:16:3e:02:c8:e3 brd ff:ff:ff:ff:ff:ff RX: bytes packets errors dropped overrun mcast 377786943 473945 0 0 0 0 TX: bytes packets errors dropped carrier collsns 266024587 377467 0 0 0 0 RX指示了接收的数据包，TX指示了发送的数据包。\n使用ethtool命令查询指定网络接口的信息 可以使用 -S 或者 \u0026ndash;statistics 选项来显示统计信息，语法如下\nethtool -S {device} 例如\n❯ ethtool -S wlan1 NIC statistics: rx_packets: 487703 rx_bytes: 207474712 rx_duplicates: 180 rx_fragments: 487682 rx_dropped: 19952 tx_packets: 141579 tx_bytes: 34804215 tx_filtered: 0 tx_retry_failed: 0 tx_retries: 19541 sta_state: 4 txrate: 400000000 rxrate: 360000000 signal: 201 channel: 0 noise: 18446744073709551615 ch_time: 18446744073709551615 ch_time_busy: 18446744073709551615 ch_time_ext_busy: 18446744073709551615 ch_time_rx: 18446744073709551615 ch_time_tx: 18446744073709551615 还可以直接使用cat或者column命令来查询 /proc/net/dev 文件，例如\n❯ column -t /proc/net/dev Inter-| Receive | Transmit face |bytes packets errs drop fifo frame compressed multicast|bytes packets errs drop fifo colls carrier compressed lo: 230352757 1201722 0 0 0 0 0 0 230352757 1201722 0 0 0 0 0 0 eth0: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 wlan1: 1346770664 2865963 0 14 0 0 0 0 282983658 1154942 0 0 0 0 0 0 br-13cb4d22d1c8: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 br-44561b4ee062: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 br-70b0dad49865: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 docker0: 6824830 44848 0 0 0 0 0 0 133304965 47104 0 0 0 0 0 0 vetheb8b528: 2360070 13321 0 0 0 0 0 0 60431688 18817 0 0 0 0 0 0 vetha4dc663: 461283 2464 0 0 0 0 0 0 2981558 2302 0 0 0 0 0 0 补充：如何诊断数据包丢弃的原因 发现网络数据有被丢弃的请，想找出原因，这里介绍一个工具 dropwath。\n首先使用需要自己编译安装该工具，下面示例在Ubuntu上编译安装：\nsudo apt-get install libpcap-dev libnl-3-dev libnl-genl-3-dev binutils-dev libreadline6-dev autoconf libtool pkg-config build-essential git clone https://github.com/nhorman/dropwatch.git cd dropwatch ./autogen.sh ./configure make make install 然后可以运行dropwatch进行监控\n$ dropwatch -l kas Initializing kallsyms db dropwatch\u0026gt; help Command Syntax: exit - Quit dropwatch help - Display this message set: alertlimit \u0026lt;number\u0026gt; - capture only this many alert packets alertmode \u0026lt;mode\u0026gt; - set mode to \u0026#34;summary\u0026#34; or \u0026#34;packet\u0026#34; trunc \u0026lt;len\u0026gt; - truncate packets to this length. Only applicable when \u0026#34;alertmode\u0026#34; is set to \u0026#34;packet\u0026#34; queue \u0026lt;len\u0026gt; - queue up to this many packets in the kernel. Only applicable when \u0026#34;alertmode\u0026#34; is set to \u0026#34;packet\u0026#34; sw \u0026lt;true | false\u0026gt; - monitor software drops hw \u0026lt;true | false\u0026gt; - monitor hardware drops start - start capture stop - stop capture show - show existing configuration stats - show statistics dropwatch\u0026gt; 还可以通过 tcpdump 进行网络抓包，然后使用 wireshark 来进行分析。\n","date":"December 2, 2020","hero":"/zh/posts/linux/network-packets/linux-network.jpg","permalink":"/zh/posts/linux/network-packets/","summary":"在Linux系统上，我们可以通过 ip , netstat 或者 [ethtool] 命令显示网络接口丢弃数据包的统计信息。接下来我们看看如何使用每个命令。\n使用netstat按接口显示数据包 其实 netstat 命令已经过时，可使用命令 ip 和 ss 来代替。但是 netstat 依然在一些旧的Linux分发版本上可用，因此在 ip/ss 不可用的情况，我们可以使用netstat，其语法如下\nnetstat -i netstat --interfaces 例如\n~$ netstat -i Kernel Interface table Iface MTU Met RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flg docker0 1500 0 188180 0 0 0 151852 0 0 0 BMRU eth0 1500 0 472368 0 0 0 375351 0 0 0 BMRU lo 65536 0 51687 0 0 0 51687 0 0 0 LRU vethc8f46ea 1500 0 136984 0 0 0 79587 0 0 0 BMRU 如果想显示每种协议的概要统计信息，可以执行","tags":["network","linux"],"title":"LINUX上统计网络接口数据包"},{"categories":["Container"],"contents":"在进行应用容器化的实践中，我们可以使用多种方式来创建容器镜像，而使用Dockerfile是我们最常用的方式。 而且在实现CI/CD Pipeline的过程中，使用Dockerfile来构建应用容器也是必须的。\n本文不具体介绍Dockerfile的指令和写法，仅仅是在实践中积累的一些写好一个Dockerfile的小提示，体现在一下几个方面：\n减少构建时间 减小镜像大小 镜像可维护性 重复构建一致性 安全性 减小构建时间 首先来看看下面这个Dockerfile\nFROM ubuntu:18.04 COPY . /app RUN apt-get update RUN apt-get -y install ssh vim openjdk-8-jdk CMD [“java”,”-jar”,”/app/target/app.jar”] 要减小构建的时间，那我们可以例如Docker构建的缓存特性，尽量保留不经常改变的层，而在Dockerfile的指令中， COPY和RUN都会产生新的层，而且缓存的有效是与命令的顺序有关系的。\n在上面的Dockerfile中，COPY . /app在RUN apt-get ...之前，而COPY是经常改变的部分，所以每次构建都会到导致RUN apt-get ...缓存失效。\nTip-1 : 合理利用缓存，而执行命令的顺序是会影响缓存的可用性的。\n要减小构建时间，另一方面是应该仅仅COPY需要的东西，对于上面这个Dockerfile的目的，应该仅仅需要COPY Java应用的jar文件。\nTip-2 : 构建过程中仅仅COPY需要的东西。\n上面的Dockerfile对apt-get命令分别使用了两个RUN指令，会生成两个不同的层。\nTip-3 : 尽量合并最终镜像的层数。\n还有对于这个示例，我们最终是想要一个JRE环境来运行Java应用，因此可以选择一个jre的镜像来作为基础镜像，这样不用花时间再去安装jdk。\nTip-4 : 选择合适的基础镜像\n这样我们可以把Dockerfile写成：\nFROM ubuntu:18.04 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get y install ssh vim openjdk-8-jdk COPY target/app.jar /app CMD [“java”,”-jar”,”/app/app.jar”] 减小镜像大小 进一步，我们如何尽量减小最终应用镜像的大小，来加速我们的CI构建，以及减小镜像在网络上传输的效率。\n在上例中，ssh, vim应该都是不必要的软件包，它们会咱用镜像的空间。\nTip-5 : 移除不必要的软件包安装（包括一些debug工具）。\n其次，类似apt-get之类的系统包管理工具会产生缓存数据，我们也应该清除。\nTip-6 : 在使用系统包管理工具安装软件包后清理缓存数据。\n另外我们应该使用Docker提供的多阶段构建特性来减小最终的镜像大小，我们在后面介绍。\n我们进一步改进Dockerfile：\nFROM ubuntu:18.04 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get y install –no-install-recommends openjdk-8-jdk \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* COPY target/app.jar /app CMD [“java”,”-jar”,”/app/app.jar”] 镜像的可维护性 我们看看上面的Dockerfile，使用了一个ubuntu的镜像来安装jdk包，而在安装jdk包的不同时间点，可能会导致不同的版本，这样就导致了镜像的不易维护。\nTip-7 : 尽可能使用应用（语言）运行时的官方基础镜像，并指定Tag版本\n一般来说，官方会维护一些变种镜像来提供多样性，例如基于 alpine的，还有 -slim 精简版本的，其次对于像Java应用，最终我们需要的应该只是JRE，因此应该选择jre的镜像，这样既保证了可维护性，同时也可以减小镜像的大小。\nFROM openjdk:8-jre-alpine COPY target/app.jar /app CMD [“java”,”-jar”,”/app/app.jar”] 重复构建一致性 我们应该保障我们的镜像构建在任何时候，以及任何构建服务器上是一致的，但是我们看上面的Dockerfile，是将jar文件COPY到容器中，但是这个jar文件是在什么环境构建的呢？\nTip-8 : 在一致的环境中从源代码构建\n同样，Docker的多阶段构建提供了最好的解决方案，将源码编译构建放到构建阶段，将最终生成的软件包COPY放到运行是阶段。\nTip-9 : 使用多阶段构建\nFROM maven:3.6-jdk-11 AS builder WORKDIR /app COPY pom.xml . RUN mvn -e -B dependency:resolve COPY src ./src RUN mvn -e -B package FROM openjdk:11-jre-slim COPY –from=builder /app/target/app.jar / CMD [“java”,”-jar”,”/app.jar”] 例如上面的Dockerfile，我们使用了maven的镜像来构建代码，使用openjdk:jre的镜像来运行。\n安全性 最后我们来看看安全性，如何使我们的应用容器更加安全。首先，容器里包含的软件包越少，那可能的漏洞就会越少，所以这也是 Tip-5 所强调的。\nTip-10 : 使用非root用户运行容器应用进程\n其次，我们应该使用非root用户来运行我们的应用，默认情况下容器都是使用root用户来执行，我们可以使用以下两种方法来使用非root用户来运行。\n使用USER指令，记得在使用USER指令前创建相应的用户 在CMD或者ENTRYPOINT中使用su-exec , gosu等工具来启动应用 我推荐使用第二种方法，因为第一种方式，在启动容器后进入容器会默认使用非root用户，这样不便于安装某些调试工具来执行调试（当然也可以通过配置sudo）。\n而第二种方式需要安装su-exec等工具，我建议自己基于官方的基础镜像维护一些自己的运行时基础镜像，这样避免在每次构建应用镜像的时候都进行一次安装。\nFROM gradle:6.4-jdk11 as builder WORKDIR /code COPY . . RUN gradle assemble FROM mengzyou/openjdk:11-jre-alpine ENV APP_HOME=\u0026#34;/opt/app\u0026#34; \\ APP_USER=\u0026#34;appuser\u0026#34; \\ JAR_OPTS=\u0026#34;--spring.profiles.active=prod\u0026#34; RUN addgroup ${APP_USER} \u0026amp;\u0026amp; \\ adduser -D -h ${APP_HOME} -S -G ${APP_USER} ${APP_USER} COPY --from=builder --chown=${APP_USER}:${APP_USER} /code/build/libs/*.jar ${APP_HOME}/app.jar EXPOSE 8080/tcp WORKDIR ${APP_HOME} CMD su-exec app java ${JAVA_OPTS} -jar ${APP_HOME}/app.jar ${JAR_OPTS} # CMD [“su-exec”,”appuser”,”sh -c”,”java -jar /opt/app/app.jar\u0026#34;] 在来一个golang的示例\nFROM golang:1.14-alpine AS builder RUN apk add --no-cache git \u0026amp;\u0026amp; \\ mkdir -p $GOPATH/src/app \\ WORKDIR $GOPATH/src/app COPY . $GOPATH/src/app RUN go mod tidy \\ \u0026amp;\u0026amp; go build -o /go/bin/app FROM mengzyou/alpine:3.12 ENV APP_HOME=/opt/app \\ APP_USER=appuser RUN addgroup ${APP_USER} \u0026amp;\u0026amp; \\ adduser -D -h ${APP_HOME} -S -G ${APP_USER} ${APP_USER} COPY --from=builder --chown=${APP_USER}:${APP_USER} /go/bin/app ${APP_HOME}/ EXPOSE 8080/tcp WORKDIR ${APP_HOME} CMD su-exec ${APP_USER} ${APP_HOME}/app 总结 这里仅仅是在Dockerfile实践中的一些提示，要写好Dockerfile，还有很多方面需要注意的地方，可参考Docker官方的Best practices for writing Dockerfiles。\n","date":"July 15, 2020","hero":"/zh/posts/container-tech/dockerfile-best/docker-bp.jpg","permalink":"/zh/posts/container-tech/dockerfile-best/","summary":"在进行应用容器化的实践中，我们可以使用多种方式来创建容器镜像，而使用Dockerfile是我们最常用的方式。 而且在实现CI/CD Pipeline的过程中，使用Dockerfile来构建应用容器也是必须的。\n本文不具体介绍Dockerfile的指令和写法，仅仅是在实践中积累的一些写好一个Dockerfile的小提示，体现在一下几个方面：\n减少构建时间 减小镜像大小 镜像可维护性 重复构建一致性 安全性 减小构建时间 首先来看看下面这个Dockerfile\nFROM ubuntu:18.04 COPY . /app RUN apt-get update RUN apt-get -y install ssh vim openjdk-8-jdk CMD [“java”,”-jar”,”/app/target/app.jar”] 要减小构建的时间，那我们可以例如Docker构建的缓存特性，尽量保留不经常改变的层，而在Dockerfile的指令中， COPY和RUN都会产生新的层，而且缓存的有效是与命令的顺序有关系的。\n在上面的Dockerfile中，COPY . /app在RUN apt-get ...之前，而COPY是经常改变的部分，所以每次构建都会到导致RUN apt-get ...缓存失效。\nTip-1 : 合理利用缓存，而执行命令的顺序是会影响缓存的可用性的。\n要减小构建时间，另一方面是应该仅仅COPY需要的东西，对于上面这个Dockerfile的目的，应该仅仅需要COPY Java应用的jar文件。\nTip-2 : 构建过程中仅仅COPY需要的东西。\n上面的Dockerfile对apt-get命令分别使用了两个RUN指令，会生成两个不同的层。\nTip-3 : 尽量合并最终镜像的层数。\n还有对于这个示例，我们最终是想要一个JRE环境来运行Java应用，因此可以选择一个jre的镜像来作为基础镜像，这样不用花时间再去安装jdk。\nTip-4 : 选择合适的基础镜像\n这样我们可以把Dockerfile写成：\nFROM ubuntu:18.04 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get y install ssh vim openjdk-8-jdk COPY target/app.jar /app CMD [“java”,”-jar”,”/app/app.","tags":["docker"],"title":"DOCKERFILE构建最佳实践"},{"categories":null,"contents":"这是一个示例帖子，旨在测试以下内容：\n不同的帖子作者。 目录 降价内容渲染。 数学渲染。 表情符号渲染。 Markdown语法渲染 标题 The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nMarkdown Syntax Rendering Headings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Inline Markdown In Table italics bold strikethrough code Code Blocks Code block with backticks html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nMath Rendering Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\nEmoji Rendering 🙈 🙈 🙉 🙉 🙊 🙊\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"June 8, 2020","hero":"/zh/posts/markdown-sample/hero.svg","permalink":"/zh/posts/markdown-sample/","summary":"这是一个示例帖子，旨在测试以下内容：\n不同的帖子作者。 目录 降价内容渲染。 数学渲染。 表情符号渲染。 Markdown语法渲染 标题 The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nMarkdown Syntax Rendering Headings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo.","tags":null,"title":"Markdown示例"},{"categories":null,"contents":"这是一个示例帖子，旨在测试以下内容：\n默认英雄形象。 不同的短代码。 告警 该主题可用以下告警。\n这是 type=\u0026quot;success\u0026quot; 的告警。 这是 type=\u0026quot;danger\u0026quot; 的告警。 这是 type=\u0026quot;warning\u0026quot; 的告警。 这是 type=\u0026quot;info\u0026quot; 的告警。 这是 type=\u0026quot;dark\u0026quot; 的告警。 这是 type=\u0026quot;primary\u0026quot; 的告警。 这是 type=\u0026quot;secondary\u0026quot; 的告警。 图像 没有任何属性的示例图像 {{\u0026lt; img src=\u0026quot;/posts/shortcodes/boat.jpg\u0026quot; title=\u0026ldquo;海上的一艘船\u0026rdquo; \u0026gt;}}\n设置高宽属性的示例图像 {{\u0026lt; img src=\u0026quot;/posts/shortcodes/boat.jpg\u0026quot; height=\u0026ldquo;400\u0026rdquo; width=\u0026ldquo;600\u0026rdquo; title=\u0026ldquo;海上的一艘船\u0026rdquo; \u0026gt;}} 设置高宽属性中间对齐的图像 {{\u0026lt; img src=\u0026quot;/posts/shortcodes/boat.jpg\u0026quot; height=\u0026ldquo;400\u0026rdquo; width=\u0026ldquo;600\u0026rdquo; align=\u0026ldquo;center\u0026rdquo; title=\u0026ldquo;海上的一艘船\u0026rdquo; \u0026gt;}} 带有float属性的图像 {{\u0026lt; img src=\u0026quot;/posts/shortcodes/boat.jpg\u0026quot; height=\u0026ldquo;200\u0026rdquo; width=\u0026ldquo;500\u0026rdquo; float=\u0026ldquo;right\u0026rdquo; title=\u0026ldquo;海上的一艘船\u0026rdquo; \u0026gt;}}\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies. Praesent tellus risus, eleifend vel efficitur ac, venenatis sit amet sem. Ut ut egestas erat. Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat. Suspendisse nec ipsum eu erat finibus dictum. Morbi volutpat nulla purus, vel maximus ex molestie id. Nullam posuere est urna, at fringilla eros venenatis quis.\nFusce vulputate dolor augue, ut porta sapien fringilla nec. Vivamus commodo erat felis, a sodales lectus finibus nec. In a pulvinar orci. Maecenas suscipit eget lorem non pretium. Nulla aliquam a augue nec blandit. Curabitur ac urna iaculis, ornare ligula nec, placerat nulla. Maecenas aliquam nisi vitae tempus vulputate.\n分列 此主题支持根据需要将页面拆分为任意数量的列。\n两列 {{\u0026lt;split 6 6\u0026gt;}} -\u0026ndash; {{\u0026lt; /split \u0026gt;}}\n左边列 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies. 右边列 Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat. 三列 {{\u0026lt; split 4 4 4 \u0026gt;}} \u0026mdash; \u0026mdash; {{\u0026lt; /split \u0026gt;}}\n左边列 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies. 中间列 Aenean dignissim dictum ex. Donec a nunc vel nibh placerat interdum. 右边列 Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat. 垂直空间 两行之间的垂直空间。\n{{\u0026lt; vs 4\u0026gt;}}\n第一行。 这是第二行。与上一行将有4rem的垂直空间。\n视频 {{\u0026lt; video src=\u0026quot;/videos/sample.mp4\u0026quot; \u0026gt;}}\nVideo by Rahul Sharma from Pexels.\nMermaid 这里是一些使用mermaid短代码的示例。\n图形:\n\\{\\{\u0026lt; mermaid align=\u0026#34;left\u0026#34; \u0026gt;}} graph LR; A[Hard edge] --\u0026gt;|Link text| B(Round edge) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result one] C --\u0026gt;|Two| E[Result two] \\{\\{\u0026lt; /mermaid \u0026gt;}} graph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] 序列图:\n\\{\\{\u0026lt; mermaid \u0026gt;}} sequenceDiagram participant Alice participant Bob Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! \\{\\{\u0026lt; /mermaid \u0026gt;}} sequenceDiagram participant Alice participant Bob Alice-\u003e\u003eJohn: Hello John, how are you? loop Healthcheck John-\u003e\u003eJohn: Fight against hypochondria end Note right of John: Rational thoughts prevail! John--\u003e\u003eAlice: Great! John-\u003e\u003eBob: How about you? Bob--\u003e\u003eJohn: Jolly good! 甘特图:\n\\{\\{\u0026lt; mermaid \u0026gt;}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d \\{\\{\u0026lt; /mermaid \u0026gt;}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d 类图:\n\\{\\{\u0026lt; mermaid \u0026gt;}} classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 \u0026lt;--\u0026gt; C2: Cool label \\{\\{\u0026lt; /mermaid \u0026gt;}} classDiagram Class01 \u003c|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --\u003e C2 : Where am i? Class09 --* C3 Class09 --|\u003e Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 \u003c--\u003e C2: Cool label Git图:\n\\{\\{\u0026lt; mermaid background=\u0026#34;black\u0026#34; align=\u0026#34;right\u0026#34; \u0026gt;}} gitGraph: options { \u0026#34;nodeSpacing\u0026#34;: 150, \u0026#34;nodeRadius\u0026#34;: 10 } end commit branch newbranch checkout newbranch commit commit checkout master commit commit merge newbranch \\{\\{\u0026lt; /mermaid \u0026gt;}} gitGraph: options { \"nodeSpacing\": 150, \"nodeRadius\": 10 } end commit branch newbranch checkout newbranch commit commit checkout master commit commit merge newbranch ER图:\n\\{\\{\u0026lt; mermaid \u0026gt;}} erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses \\{\\{\u0026lt; /mermaid \u0026gt;}} erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses Gist {{\u0026lt; gist hossainemruz 4ad86c9b6378677e14eff12713e75e44 \u0026gt;}}\n","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"/zh/posts/shortcodes/","summary":"这是一个示例帖子，旨在测试以下内容：\n默认英雄形象。 不同的短代码。 告警 该主题可用以下告警。\n这是 type=\u0026quot;success\u0026quot; 的告警。 这是 type=\u0026quot;danger\u0026quot; 的告警。 这是 type=\u0026quot;warning\u0026quot; 的告警。 这是 type=\u0026quot;info\u0026quot; 的告警。 这是 type=\u0026quot;dark\u0026quot; 的告警。 这是 type=\u0026quot;primary\u0026quot; 的告警。 这是 type=\u0026quot;secondary\u0026quot; 的告警。 图像 没有任何属性的示例图像 {{\u0026lt; img src=\u0026quot;/posts/shortcodes/boat.jpg\u0026quot; title=\u0026ldquo;海上的一艘船\u0026rdquo; \u0026gt;}}\n设置高宽属性的示例图像 {{\u0026lt; img src=\u0026quot;/posts/shortcodes/boat.jpg\u0026quot; height=\u0026ldquo;400\u0026rdquo; width=\u0026ldquo;600\u0026rdquo; title=\u0026ldquo;海上的一艘船\u0026rdquo; \u0026gt;}} 设置高宽属性中间对齐的图像 {{\u0026lt; img src=\u0026quot;/posts/shortcodes/boat.jpg\u0026quot; height=\u0026ldquo;400\u0026rdquo; width=\u0026ldquo;600\u0026rdquo; align=\u0026ldquo;center\u0026rdquo; title=\u0026ldquo;海上的一艘船\u0026rdquo; \u0026gt;}} 带有float属性的图像 {{\u0026lt; img src=\u0026quot;/posts/shortcodes/boat.jpg\u0026quot; height=\u0026ldquo;200\u0026rdquo; width=\u0026ldquo;500\u0026rdquo; float=\u0026ldquo;right\u0026rdquo; title=\u0026ldquo;海上的一艘船\u0026rdquo; \u0026gt;}}\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies.","tags":null,"title":"短代码示例"},{"categories":["Container"],"contents":"在本系列的Docker入门中，我们介绍了容器的基本概念，以及如何在Ubuntu（Linux）上安装Docker引擎来进行容器化引用的开发。\n本篇我们介绍如何在Windows系统上安装和使用Docker，这里主要介绍在Windows 10上安装和使用Docker Desktop，对于Windows 10以下的版本，可以使用Docker Toolbox，这里就不做介绍了．\n安装Docker Desktop Docker Desktop　- The fastest way to containerize applications on your desktop，　这是Docker官方的定义，Docker Desktop为Windows和Mac提供了一个桌面化的容器开发环境，在Windows 10上，Docker Desktop使用了Windows的Hyper-V虚拟化技术，因此你需要一台打开了硬件虚化化的电脑并且安装的是Windows 10专业版以上的系统，还需要打开Hyper-V功能，如何在Windows 10上打开Hyper-V，参考这里．\n补充　：　Docker Desktop支持Windows 10 64位: 专业版，企业版，教育版 (Build 15063 或以上).\n首先在满足条件的Windows系统上下载Docker Desktop的安装包 - https://hub.docker.com/editions/community/docker-ce-desktop-windows　．安装过程是简单的，直接双击下载的安装，更具提示安装就好了，一开始我们选择使用Linux容器（之后可以其他换到使用Windows容器的方式，会单独写一篇来介绍使用Windows容器）．安装过程中安装程序会检查系统是否满足，如果不满足，安装程序会报错并结束安装．\n安装完成之后，打开 开始 菜单，然后选择 Docker Desktop 启动．\n查看状态栏上的Docker图标，一开始会显示 starting 装，等到显示Docker Desktop is running，就可以通过终端（例如 PowerSheel）来使用Docker的相关命令了，下面我们将使用Windows 10的PowerShell作为终端来进行操作．\n构建和运行容器 我们将使用一个简单Node应用来示例如何在Windows上构建容器镜像和启动一个容器．首先我们需要将代码库下载到我们的环境中，这里可以使用Git来克隆代码库或者直接下载代码包．\n在Windows上，可以使用Git for Windows，也可以使用Windows 10的WSL安装一个Ubuntu子系统，然后在Ubuntu子系统终端里安装Git，并直接使用Git克隆代码，这里我使用的是在Ubuntu子系统终端里克隆代码库到本地目录．\n如上图所示，我们把代码克隆到了D:\\gitrepos\\hellonode\\目录，然后切换到PowerShell终端，进入该目录.　用你喜欢的文本编辑器打开hellonode\\Dockerfile（推荐时候用VS Code，内容如下\nFROM node:12.2-alpine MAINTAINER Mengz You \u0026lt;mengz.you@outlook.com\u0026gt; WORKDIR /app COPY package*.json ./ RUN npm install COPY . . EXPOSE 3000 CMD [\u0026#34;npm\u0026#34;,\u0026#34;start\u0026#34;] 构建镜像 使用这个简单的Dockerfile，用于构建示例的Node应用，在Powershell中执行如下命令\ndocker image build -t hellnode:local . docker image ls 如下图所示，将会看到构建的容器镜像\n运行容器 接着我们使用构建好的镜像来启动一个应用容器，在Powershell中执行如下命令\ndocker container run --name hellonode -d -p 3000:3000 hellonode:local docker container ls 这样我们就启动了一个容器，并且可以使用docker contianer ls查看当前的容器状态，同时我们也可以使用Docker Desktop的Dashboard来图形化查看容器状态并进行一些操作，要打开Dashboard，点击状态栏的Docker Desktop图标，选择Dashboard打开，如下图所示\n在Dashboard上，我们可以UI操作的方式查看容器相关的信息，例如查看容器的日志，停止／启动／重启容器，还可以进入容器CLI等操作．\n在运行容器时，我们使用-p 3000:3000指定了将容器应用的3000端口映射到了本地的3000端口，所以我们可以直接访问本地3000端口来访问应用\n之后，我们可以使用一下命令停止和删除掉容器\ndocker container stop hellonode docker contianer rm hellonode 推送镜像到Docker Hub 如何你需要将构建的镜像推送到Docker Hub，首先需要登录你的Docker Hub帐号，点击状态栏的Docker Desktop图标，选择Sign in/Create Docker ID..，打开登录窗口进行登录\n然后我们在PowerShell里执行\ndocker image tag hellonode:local mengzyou/hellonode:v1.0 docker image push mengzyou/hellonode:v1.0 这样就会将镜像推送到Docker Hub上你的仓库中了．\n总结 这里是简单介绍了下如何在Windows 10上使用Docker Desktop来进行容器话应用的开发，Docker Desktop为Windows用户提供了很好的Docker容器化工具，除了可以使用Docker引擎之外，Docker Desktop还提供了Kubenetes功能，可以在Windows上运行一个单机的K8S环境，更多信息可以阅读官方文档．\n虽然Docker Desktop为Windows提供了一个可视化的管理工具，不过我还是推荐直接在Linux桌面上直接使用Docker引擎，在Linux环境中，如果你也想要一个UI的管理工具，我推荐使用Portainer - 一个基于Web的容器管理工具．\n","date":"March 18, 2020","hero":"/zh/posts/container-tech/docker-desktop-windows/docker-desktop-windows.jpg","permalink":"/zh/posts/container-tech/docker-desktop-windows/","summary":"在本系列的Docker入门中，我们介绍了容器的基本概念，以及如何在Ubuntu（Linux）上安装Docker引擎来进行容器化引用的开发。\n本篇我们介绍如何在Windows系统上安装和使用Docker，这里主要介绍在Windows 10上安装和使用Docker Desktop，对于Windows 10以下的版本，可以使用Docker Toolbox，这里就不做介绍了．\n安装Docker Desktop Docker Desktop　- The fastest way to containerize applications on your desktop，　这是Docker官方的定义，Docker Desktop为Windows和Mac提供了一个桌面化的容器开发环境，在Windows 10上，Docker Desktop使用了Windows的Hyper-V虚拟化技术，因此你需要一台打开了硬件虚化化的电脑并且安装的是Windows 10专业版以上的系统，还需要打开Hyper-V功能，如何在Windows 10上打开Hyper-V，参考这里．\n补充　：　Docker Desktop支持Windows 10 64位: 专业版，企业版，教育版 (Build 15063 或以上).\n首先在满足条件的Windows系统上下载Docker Desktop的安装包 - https://hub.docker.com/editions/community/docker-ce-desktop-windows　．安装过程是简单的，直接双击下载的安装，更具提示安装就好了，一开始我们选择使用Linux容器（之后可以其他换到使用Windows容器的方式，会单独写一篇来介绍使用Windows容器）．安装过程中安装程序会检查系统是否满足，如果不满足，安装程序会报错并结束安装．\n安装完成之后，打开 开始 菜单，然后选择 Docker Desktop 启动．\n查看状态栏上的Docker图标，一开始会显示 starting 装，等到显示Docker Desktop is running，就可以通过终端（例如 PowerSheel）来使用Docker的相关命令了，下面我们将使用Windows 10的PowerShell作为终端来进行操作．\n构建和运行容器 我们将使用一个简单Node应用来示例如何在Windows上构建容器镜像和启动一个容器．首先我们需要将代码库下载到我们的环境中，这里可以使用Git来克隆代码库或者直接下载代码包．\n在Windows上，可以使用Git for Windows，也可以使用Windows 10的WSL安装一个Ubuntu子系统，然后在Ubuntu子系统终端里安装Git，并直接使用Git克隆代码，这里我使用的是在Ubuntu子系统终端里克隆代码库到本地目录．\n如上图所示，我们把代码克隆到了D:\\gitrepos\\hellonode\\目录，然后切换到PowerShell终端，进入该目录.　用你喜欢的文本编辑器打开hellonode\\Dockerfile（推荐时候用VS Code，内容如下\nFROM node:12.2-alpine MAINTAINER Mengz You \u0026lt;mengz.you@outlook.com\u0026gt; WORKDIR /app COPY package*.","tags":["docker","windows"],"title":"WINDOWS上的DOCKER DESKTOP"},{"categories":["DEVOPS"],"contents":"这篇文章以一个简单的Nodejs应用为例，示例如何使用Github Actions来自动构建，测试和部署一个应用．\n什么是Github Actions 首先简单介绍下什么是Github Actions？　Github Actions是Github官方提供的一个与Github集成在一起的CI/CD工具，使用Github Actions可以非常容易地自动化你的所有软件工作流程，包括持续集成（CI）和持续发布（CD）．\n不过要使用Github Actions，你需要将你的项目代码库放在Github上，然后为代码库配置相应的工作流（Workflows）．　Actions Runner 使用Github Actions来执行工作流任务，还需要一个可执行的环境，Actions Runner就是提供这样的环境，Github Actions支持两种类型的Runner:\nGithub-Hosted Runner : 由Github官方提供和维护的Runner服务器，不需要用户自己维护和更新，有支持Linnux，Windows，macOS环境的构建 Self-Hosted Runner : 用户自己使用本地机器，云服务器安装Actions应用，用户可以自定义硬件，软件等需求 Actions 在Github Actions中有一个Action的概念，Actions是一个独立的任务，你可以组合这些任务成为要完成一个工作的步骤.　在工作步骤中，你可以自己写执行命令组成Action，也可以直接使用Github社区提供的针对一个写公共任务的Actions，可以到Github市场查找社区或者其他开发人员编写的Actions．　例如一个最常用的Action - checkout，可用来检出代码库：\n- uses: actions/checkout@v2 除了以上概念之外，Github Actions还有其他概念需要了解，具体可参考　(https://help.github.com/en/actions/getting-started-with-github-actions/overview)\nNodejs应用示例 接下来，我们就那个简单的nodejs应用来看看如何使用Github Actions创建CI/CD的流程．\n首先，你的项目代码库需要放在Github上，例如　https://github/mengzyou/hellonode/ ，访问你的代码库主页，然后点击 Actions 进入Actions页面．\n根据你的代码库的语言类型，Github推荐了一些Workflow的模板，这里我们将使用Nodejs的模板　直接点击 Set up this workflow 来应用这个模板，然后Github会直接打来Web编辑器来编辑这个模板文件\n你可以直接使用该文件，也可以修改，添加需要的Actions，完成之后可以点击　Start commit 按钮来提交Workflow文件，Github会自动为代码库创建目录　.github/workflows/，以及把该文件放在该目录下，例如　.github/workflows/nodejs.yml .　提交之后，Github Actions就会根据Workflow的内容开始运行相应的工作．\n创建一个执行测试CI工作流 其实我们也可以直接编辑本地代码库，添加目录　.github/workflows/｀，以及创建相应的Workflows配置文件，例如我们创建一个　.github/workflows/nodejs.yml`　name: Node.js CI on: push: branches: - master jobs: build: runs-on: ubuntu-latest container: node:12.16-alpine steps: - name: Checkout repository code uses: actions/checkout@v2 - name: Cache node modules uses: actions/cache@v1 env: cache-name: cache-node-modules with: path: ./node_modules key: ${{ runner.os }}-build-${{ env.cache-name }}-${{ hashFiles(\u0026#39;**/package-lock.json\u0026#39;) }} restore-keys: | ${{ runner.os }}-build-${{ env.cache-name }}- ${{ runner.os }}-build- ${{ runner.os }}- - name: Install Dependencies run: npm install - name: Test run: npm test env: CI: true 我们就定义了一个Workflow，并命名为 Node.js CI，将在master分支发生push时执行．并且定义了一个名为　build 的工作（job），该工作将使用Github-Hosted Runner（ubuntu-latest，Github-Hosted Runner会虚拟化一个相应的操作系统环境），我们也会使用容器来执行相应的步骤，这里使用了Docker容器镜像node:12.16-alpine.　setps定义将要执行的没有个步骤，可以给每个步骤命名，也可以直接调用相应的Actions，或者直接使用　－run 来执行命令．　上面的步骤中，有使用　actions/checkout@v2 来拉去代码库，也有使用　actions/cache@v1 来创建使用缓存（使用缓存可加速工作流的执行），然后使用直接行命令　npm install 和　npm test　的步骤．　当我们编写好之后，创建一个提交，然后Push到Github上master分支，就会出发一次定义的CI流程，如下图所示\n我们可以通过Github代码库的Actions页面查看Workflow执行的状态和结果，也可以查看执行的日志．\n在上面的示例中，我们的工作流里只有一个Job，就是测试代码，我们可以添加更多的Job来只想其他任务．例如打包我们的应用，部署我们的应用．\n下面我们就添加更多工作来完成我们的整个CI/CD工作流．\n添加一个工作打包应用的Docker镜像 我们编辑　.github/workflows/nodejs.yml 文件，添加以下内容　package: runs-on: ubuntu-latest needs: build steps: - name: Checkout repository code uses: actions/checkout@v2 - name: Build \u0026amp; Push container image uses: mr-smithers-excellent/docker-build-push@v2 with: image: mengzyou/hellonode tag: latest registry: docker.io username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} 我们添加两一个job - package，这个job将运行在Github-Hosted Runner（ubuntu-latest）上，needs 表示该job需要 build　job成功执行完成之后才执行．\n包括了两个步骤，拉取代码库，然后是使用 mr-smithers-excellent/docker-build-push@v2 Action来执行容器镜像的构建和推送．　这个Action中，我们还使用到了Github Actions的Secrets，Secrets可以配置一些敏感的变量，到Github代码库设置的Secrets进行配置\n这里我们配置了推送Docker镜像到Dokcer Hub需要的用户名和访问凭证. 对于Docker Hub，推荐使用Docker Hub Access Token作为访问密码．\n我们将Workflow提交到master分支后，在地一个build工作之后，就发触发这个job，打包镜像并推送到Docker Hub.　添加一个工作来自动部署 接下来，为了完成整个CI/CD的演示，我们再添加一个job，名为　deploy．　对于这个job的执行，我们需要使用一个Self-Hosted的Runner．\n访问代码库的Github界面，点击Settings，然后点击左侧边栏的Actions，再点击`Add runner｀按钮\n按照弹出来的指导，在需要部署应用的服务器上安装 action-runner，这里我们选择的是在一个Linux上安装，成功启动runner应用之后，会连接Github服务，在Gihub的Self-hosted runners下能看到添加的runner的状态，这样添加的一个Self-hosted runner会具有标签[\u0026lsquo;self-hosted\u0026rsquo;,\u0026rsquo;linux\u0026rsquo;,\u0026lsquo;x86\u0026rsquo;]，这些标签将会用于job选择合适runner来执行．\n添加了Self-hosted　Runner之后，我们就可以在Workflow里添加相应的job了　deploy: runs-on: [self-hosted,linux] needs: package env: CONTAINER_IMAGE: mengzyou/hellonode CONTAINER_NAME: webapps_hellonode steps: - name: Docker pull image run: docker image pull $CONTAINER_IMAGE - name: Docker stop container run: docker container stop $CONTAINER_NAME - name: Docker remove container if: always() run: docker container rm -f $CONTAINER_NAME - name: Docker run container if: always() run: docker container run --name $CONTAINER_NAME -d -p 8000:3000 $CONTAINER_IMAGE - name: Prune images if: always() run: docker image prune -f # remove the dangling images 这里，我们将 runs-on　设置为　[self-hosted,linux]　来选择运行的Runner服务器，然后该job需要　package job 成功完成之后才会执行．　我们还配置了两个Job（．job.env）范围内的变量来使用我们的容器镜像和名字．然后包含了以下执行步骤：　从Docker Hub拉取上一个Job构建出来的容器镜像 停止运行的容器 删除之前的应用容器 使用新的镜像运行一个新的应用容器 清理旧的容器镜像 提交代码，Push到master分支，这样一个包含３个Jobs的Workflow就会自动执行，如果在任何一个Job中只想错误，都会停止后续的Jobs，并且给出反馈．\n总结 上面通过为一个简单的nodejs应用添加了Github Actions来完成了一个构建，打包，部署的CI/CD流程．通过使用Github Actions，可以很容易地为我们放在Github上的项创建CI/CD流程，管理软件开发的生命周期（SDLC）．　Gihhub Actions类似于Gitlab的Gitlab-CI，以及［Jinkens］(https://jenkins.io/zh/)，还有很多第三方的CI/CD服务．\nGithub Actions是Github原生支持的，因此对于代码库管理在Github上的组织和用户，更容易使用．\n参考\nhttps://help.github.com/en/actions ","date":"March 5, 2020","hero":"/zh/posts/devops/github-actions/github-actions.png","permalink":"/zh/posts/devops/github-actions/","summary":"这篇文章以一个简单的Nodejs应用为例，示例如何使用Github Actions来自动构建，测试和部署一个应用．\n什么是Github Actions 首先简单介绍下什么是Github Actions？　Github Actions是Github官方提供的一个与Github集成在一起的CI/CD工具，使用Github Actions可以非常容易地自动化你的所有软件工作流程，包括持续集成（CI）和持续发布（CD）．\n不过要使用Github Actions，你需要将你的项目代码库放在Github上，然后为代码库配置相应的工作流（Workflows）．　Actions Runner 使用Github Actions来执行工作流任务，还需要一个可执行的环境，Actions Runner就是提供这样的环境，Github Actions支持两种类型的Runner:\nGithub-Hosted Runner : 由Github官方提供和维护的Runner服务器，不需要用户自己维护和更新，有支持Linnux，Windows，macOS环境的构建 Self-Hosted Runner : 用户自己使用本地机器，云服务器安装Actions应用，用户可以自定义硬件，软件等需求 Actions 在Github Actions中有一个Action的概念，Actions是一个独立的任务，你可以组合这些任务成为要完成一个工作的步骤.　在工作步骤中，你可以自己写执行命令组成Action，也可以直接使用Github社区提供的针对一个写公共任务的Actions，可以到Github市场查找社区或者其他开发人员编写的Actions．　例如一个最常用的Action - checkout，可用来检出代码库：\n- uses: actions/checkout@v2 除了以上概念之外，Github Actions还有其他概念需要了解，具体可参考　(https://help.github.com/en/actions/getting-started-with-github-actions/overview)\nNodejs应用示例 接下来，我们就那个简单的nodejs应用来看看如何使用Github Actions创建CI/CD的流程．\n首先，你的项目代码库需要放在Github上，例如　https://github/mengzyou/hellonode/ ，访问你的代码库主页，然后点击 Actions 进入Actions页面．\n根据你的代码库的语言类型，Github推荐了一些Workflow的模板，这里我们将使用Nodejs的模板　直接点击 Set up this workflow 来应用这个模板，然后Github会直接打来Web编辑器来编辑这个模板文件\n你可以直接使用该文件，也可以修改，添加需要的Actions，完成之后可以点击　Start commit 按钮来提交Workflow文件，Github会自动为代码库创建目录　.github/workflows/，以及把该文件放在该目录下，例如　.github/workflows/nodejs.yml .　提交之后，Github Actions就会根据Workflow的内容开始运行相应的工作．\n创建一个执行测试CI工作流 其实我们也可以直接编辑本地代码库，添加目录　.github/workflows/｀，以及创建相应的Workflows配置文件，例如我们创建一个　.github/workflows/nodejs.yml`　name: Node.js CI on: push: branches: - master jobs: build: runs-on: ubuntu-latest container: node:12.","tags":["github","cicd"],"title":"GITHUB ACTIONS工作流"},{"categories":["Container"],"contents":"今天借助Github用户huan的盒装微信项目，在我的openSUSE Leap系统上使用Docker成功地运行封装的Windows上的微信客户端。\n安装Docker 在Linux系统上安装Docker引擎是很容器的，请参考Docker入门，如果你也使用的是openSUSE Leap，执行如下命令安装Docker引擎:\n$ sudo zypper ref $ sudo zypper in docker 启动微信客户端 注意： 在启动之前，需要设置主机系统的X服务的访问控制，使用如下的命令禁用主机上X服务的访问控制，允许所有客户端链接服务：\n$ xhost + 关于[xhost]的更多信息，可参考(https://www.computerhope.com/unix/xhost.htm)。\nhuan/docker-wechat提供了一个启动脚本dochat.sh来执行容器镜像的下载，以及启动，可直接执行如下操作：\n$ curl -sL https://raw.githubusercontent.com/huan/docker-wechat/master/dochat.sh | bash 当然也可以克隆Git代码库，然后执行dochat.sh脚本。\n成功启动后如下图所示，使用手机扫描登录。\n使用Docker Compose启动 dochat.sh是直接使用了docker run命令启动容器，也可以编写一个compose文件来使用docker-compose管理应用容器。例如我在目录 ~/dockerapp/ 下创建了一个 dochat.yml 文件。\nversion: \u0026#39;2.4\u0026#39; services: dochat: image: zixia/wechat container_name: dockerapps_dochat network_mode: bridge devices: - \u0026#34;/dev/video0:/dev/video0\u0026#34; - \u0026#34;/dev/snd:/dev/snd\u0026#34; volumes: - \u0026#34;/etc/localtime:/etc/localtime:ro\u0026#34; - \u0026#34;$HOME/.dochat/appdata:/home/user/.wine/drive_c/user/Application Data/\u0026#34; - \u0026#34;$HOME/.dochat/wechatfiles:/home/user/WeChat Files/\u0026#34; - \u0026#34;/tmp/.X11-unix:/tmp/.X11-unix\u0026#34; environment: - \u0026#34;DISPLAY=unix$DISPLAY\u0026#34; - \u0026#34;XMODIFIERS=@im=fcitx\u0026#34; - \u0026#34;GTK_IM_MODULE=fcitx\u0026#34; - \u0026#34;QT_IM_MODULE=fcitx\u0026#34; - \u0026#34;AUDIO_GID=492\u0026#34; - \u0026#34;VIDEO_GID=484\u0026#34; - \u0026#34;GID=100\u0026#34; - \u0026#34;UID=1000\u0026#34; - \u0026#34;DOCHAT_DEBUG=true\u0026#34; ipc: host privileged: true 首次启动时使用命令docker-compose -f ~/dockerapp/dochat.yml up -d，在关闭应用之后，再次启动时使用docker-compose -f ~/dockerapp/dochat.yml start。\n也可以创建一个桌面快捷方式，编写一个Desktop文件放在桌面文件夹下 ~/desktop/dochat.desktop :\n[Desktop Entry] Categories=Network;Utility;Chat; Comment[en_US]=Docker run windows wechat client on Linux. Comment=Docker run windows wechat client on Linux. Exec=/usr/local/bin/docker-compose -f /home/mengz/dockerapp/dochat.yml start GenericName[en_US]= GenericName= Icon=/home/mengz/dockerapp/dochat.png MimeType= Name=Docker WeChat Path= StartupNotify=true Terminal=false TerminalOptions= Type=Application X-DBUS-ServiceName= X-DBUS-StartupType= X-KDE-SubstituteUID=false X-KDE-Username= 这样在桌面双击快捷方式就可以启动微信了，是不是很棒！\n补充 如果不想使用Docker来运行封装的Windows版本的微信桌面客户端，这里也可以推荐你使用一个用Electronic封装的Web版本微信客户端geeeeeeeeek/electronic-wechat。\n","date":"February 25, 2020","hero":"/zh/posts/container-tech/docker-wechat/docker-wechat.png","permalink":"/zh/posts/container-tech/docker-wechat/","summary":"今天借助Github用户huan的盒装微信项目，在我的openSUSE Leap系统上使用Docker成功地运行封装的Windows上的微信客户端。\n安装Docker 在Linux系统上安装Docker引擎是很容器的，请参考Docker入门，如果你也使用的是openSUSE Leap，执行如下命令安装Docker引擎:\n$ sudo zypper ref $ sudo zypper in docker 启动微信客户端 注意： 在启动之前，需要设置主机系统的X服务的访问控制，使用如下的命令禁用主机上X服务的访问控制，允许所有客户端链接服务：\n$ xhost + 关于[xhost]的更多信息，可参考(https://www.computerhope.com/unix/xhost.htm)。\nhuan/docker-wechat提供了一个启动脚本dochat.sh来执行容器镜像的下载，以及启动，可直接执行如下操作：\n$ curl -sL https://raw.githubusercontent.com/huan/docker-wechat/master/dochat.sh | bash 当然也可以克隆Git代码库，然后执行dochat.sh脚本。\n成功启动后如下图所示，使用手机扫描登录。\n使用Docker Compose启动 dochat.sh是直接使用了docker run命令启动容器，也可以编写一个compose文件来使用docker-compose管理应用容器。例如我在目录 ~/dockerapp/ 下创建了一个 dochat.yml 文件。\nversion: \u0026#39;2.4\u0026#39; services: dochat: image: zixia/wechat container_name: dockerapps_dochat network_mode: bridge devices: - \u0026#34;/dev/video0:/dev/video0\u0026#34; - \u0026#34;/dev/snd:/dev/snd\u0026#34; volumes: - \u0026#34;/etc/localtime:/etc/localtime:ro\u0026#34; - \u0026#34;$HOME/.dochat/appdata:/home/user/.wine/drive_c/user/Application Data/\u0026#34; - \u0026#34;$HOME/.dochat/wechatfiles:/home/user/WeChat Files/\u0026#34; - \u0026#34;/tmp/.X11-unix:/tmp/.X11-unix\u0026#34; environment: - \u0026#34;DISPLAY=unix$DISPLAY\u0026#34; - \u0026#34;XMODIFIERS=@im=fcitx\u0026#34; - \u0026#34;GTK_IM_MODULE=fcitx\u0026#34; - \u0026#34;QT_IM_MODULE=fcitx\u0026#34; - \u0026#34;AUDIO_GID=492\u0026#34; - \u0026#34;VIDEO_GID=484\u0026#34; - \u0026#34;GID=100\u0026#34; - \u0026#34;UID=1000\u0026#34; - \u0026#34;DOCHAT_DEBUG=true\u0026#34; ipc: host privileged: true 首次启动时使用命令docker-compose -f ~/dockerapp/dochat.","tags":["docker","wechat","opensuse"],"title":"DOCKER运行微信桌面客户端"},{"categories":["Container"],"contents":"这是Docker快速开始系列的第二篇，在对我们的应用进行容器化之前，请先阅读第一篇安装好Docker环境。\n介绍 我们在开发主机（开发环境）上安装好Docker之后，我们就可以开始发开容器化应用，通常按照以下步骤：\n为应用的每个组件创建Docker镜像，然后通过镜像运行容器并测试． 编写 docker stack 文件或者Kubernetes的 YMAL　文件，将容器和支持的基础设施集装到一个完整应用程序. 测试，分享和部署你的整个容器化的应用程序． 在这个快速的教程里，我们将专注在第一个步骤：创建容器将基于的镜像．\n准备Dockerfile 我们将使用Docker的一个培训项目示例docker-training/node-bulletin-board，按照如下步骤\n从Github克隆示例代码（首先你需要在环境中安装好Git） $ git clone -b v1 https://github.com/docker-training/node-bulletin-board $ cd node-bulletin-board/bulletin-board-app/ 这是一个简单的公告板应用示例代码，使用node.js编写．现在，我们需要容器化该应用．\n在代码目录下，有一个Dockerfile文件，该文件描述了如何为一个容器封装一个私有文件系统，以及包含一些描述如何运行容器的元数据，文件内容如下 FROM node:8.9.4-alpine WORKDIR /usr/src/app COPY package.json . RUN npm install COPY . . CMD [ \u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34; ] 为应用编写Dockerfile是容器化应用的第一步，你可以认为Dockerfile里的命令是构建镜像的一步步指令\n首先从一个已经存在的基础镜像开始，FROM node:8.9.4-alpine，该基础镜像是一个官方的镜像，在开始构建时，如果本地没有该镜像，将会Docker Hub自动拉取该镜像． 然后通过WORKDIR指令设置工作目录，之后的操作都将基于该工作目录． 通过COPY指令，将当前目录下的package.json文件拷贝到容器的当前目录下（/usr/src/app/），也就是/usr/src/app/package.json．　RUN指令是执行相关的命令，示例中在/usr/src/app/目录下执行npm install命令，该命令将根据package.json文件安装应用相关的依赖包． 接着将剩余的代码从主机拷贝到镜像的文件系统中． 最后的CMD命令配置了镜像的元数据，描述使用该镜像运行容器时，如何启动应用程序，这里启动容器时将运行npm start. 以上只是一个简单的Dockerfile示例，更多的指令请参考官方文档.\n构建和测试镜像 现在我们拥有了源代码和Dockerfile，我们可以开始构建应用的镜像了．\n首先确保当前目录是 node-bulletin-board/bulletin-board-app/　，通过如下的命令构建镜像 $ docker image build -t bulletinboard:1.0 . 你将看到Docker按照Dockerfile里的指令进行构建，当构建成功后，可通过如下命令查看到构建出来的镜像\n$ docker image ls bulletinboard 1.0 866d1f004027 About a minutes ago 82.3MB 我们通过-t选项指明了镜像的名字和标签，命令的最有一个.意思是构建的上下文是当前目录，将在当前目录里寻找Dockerfile．\n基于镜像启动一个容器 docker container run --name bb --publish 8000:8080 --detach bulletinboard:1.0 该命令将基于镜像bulletinboard:1.0启动一个容器实例，我们使用如下的一些命令选项\n\u0026ndash;name : 将容器命名为bb，如果不指定，Docker将自动为容器命名. \u0026ndash;publish : 将容器的里的端口8080映射到主机上的8000款口，也就是发往主机8000款口的流量将会转发到容器的8080端口． \u0026ndash;detach : Docker将容器运行在后台． 由于构建镜像的时候通过CMD指定了容器启动的命令npm start，因此容器在启动时候将自动通过该命令来启动应用进程．\n访问应用 由于容器启动时，制定了将服务端口映射到了主机的8000端口，因此我们可以通过浏览器访问 http://localhost:8000/ 访问应用．\n当验证应用运行正常，可将容器在主机环境中删除掉 $ docker container rm --force bb 总结 本文通过一个简单的node.js应用，示例了如何通过Dockerfile来容器化应用程序，也就是将应用的依赖，可只想代码封装到容器镜像，再通过镜像在开发环境上运行应用容器．　这是容器话应用的地一个阶段，在构架了应用的容器之后，我们就可以将镜像分发到其他运行环境（测试环境，生产环境）进行应用的部署了．当然对于复杂的应用，可能由很多的服务组件构成，那每一各服务都将会被构建成相应的镜像，部署的时候就会运行很多的容器来组成整个应用程序，因此我们需要容器的编排工具（Orchestrator）来自动管理和部署相应的容器服务．\n","date":"February 13, 2020","hero":"/zh/posts/container-tech/docker-container-app/docker-banner.png","permalink":"/zh/posts/container-tech/docker-container-app/","summary":"这是Docker快速开始系列的第二篇，在对我们的应用进行容器化之前，请先阅读第一篇安装好Docker环境。\n介绍 我们在开发主机（开发环境）上安装好Docker之后，我们就可以开始发开容器化应用，通常按照以下步骤：\n为应用的每个组件创建Docker镜像，然后通过镜像运行容器并测试． 编写 docker stack 文件或者Kubernetes的 YMAL　文件，将容器和支持的基础设施集装到一个完整应用程序. 测试，分享和部署你的整个容器化的应用程序． 在这个快速的教程里，我们将专注在第一个步骤：创建容器将基于的镜像．\n准备Dockerfile 我们将使用Docker的一个培训项目示例docker-training/node-bulletin-board，按照如下步骤\n从Github克隆示例代码（首先你需要在环境中安装好Git） $ git clone -b v1 https://github.com/docker-training/node-bulletin-board $ cd node-bulletin-board/bulletin-board-app/ 这是一个简单的公告板应用示例代码，使用node.js编写．现在，我们需要容器化该应用．\n在代码目录下，有一个Dockerfile文件，该文件描述了如何为一个容器封装一个私有文件系统，以及包含一些描述如何运行容器的元数据，文件内容如下 FROM node:8.9.4-alpine WORKDIR /usr/src/app COPY package.json . RUN npm install COPY . . CMD [ \u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34; ] 为应用编写Dockerfile是容器化应用的第一步，你可以认为Dockerfile里的命令是构建镜像的一步步指令\n首先从一个已经存在的基础镜像开始，FROM node:8.9.4-alpine，该基础镜像是一个官方的镜像，在开始构建时，如果本地没有该镜像，将会Docker Hub自动拉取该镜像． 然后通过WORKDIR指令设置工作目录，之后的操作都将基于该工作目录． 通过COPY指令，将当前目录下的package.json文件拷贝到容器的当前目录下（/usr/src/app/），也就是/usr/src/app/package.json．　RUN指令是执行相关的命令，示例中在/usr/src/app/目录下执行npm install命令，该命令将根据package.json文件安装应用相关的依赖包． 接着将剩余的代码从主机拷贝到镜像的文件系统中． 最后的CMD命令配置了镜像的元数据，描述使用该镜像运行容器时，如何启动应用程序，这里启动容器时将运行npm start. 以上只是一个简单的Dockerfile示例，更多的指令请参考官方文档.\n构建和测试镜像 现在我们拥有了源代码和Dockerfile，我们可以开始构建应用的镜像了．\n首先确保当前目录是 node-bulletin-board/bulletin-board-app/　，通过如下的命令构建镜像 $ docker image build -t bulletinboard:1.0 . 你将看到Docker按照Dockerfile里的指令进行构建，当构建成功后，可通过如下命令查看到构建出来的镜像\n$ docker image ls bulletinboard 1.","tags":["docker"],"title":"DOCKER构建容器化应用"},{"categories":["Container"],"contents":"Docker Docker是一个为开发者和运维工程师（系统管理员）以容器的方式构建，分享和运行应用的平台。使用容器进行应用部署的方式，我们成为容器化。\n容器化应用具有一下特性，使得容器化日益流行：\n灵活：　再复杂的应用都可以进行容器化． 轻量：　容器使用使用和共享主机的内核，在系统资源的利用比虚拟机更加高效． 可移植：　容器可以本地构建，部署到云上，运行在任何地方． 松耦合：　容器是高度自封装的，可以在不影响其他容器的情况下替换和升级容器． 可扩展：　可以在整个数据中心里增加和自动分发容器副本． 安全：　容器约束和隔离应用进程，而无需用户进行任何配置． 镜像和容器 其实，容器就是运行的进程，附带一些封装的特性，使其与主机上和其他容器的进程隔离．每个容器都只访问它自己私有的文件系统，这是容器隔离很重要的一方面．而Docker镜像就提供了这个文件系统，一个镜像包含运行该应用所有需求 - 代码或者二进制文件，运行时，依赖库，以及其他需要的文件系统对象．\n通过与虚拟机对比，虚拟机（VM）通过一个虚拟机管理（Hypervisor）运行了完整的操作系统来访问主机资源．通常虚拟机会产生大量的开销，超过了应用本身所需要的开销．\n容器编排 容器化过程的可移植性和可重复性意味着我们有机会跨云和数据中心移动和扩展容器化的应用程序，容器有效地保证应用程序可以在任何地方以相同的方式运行，这使我们可以快速有效地利用所有这些环境．当我们扩展我们的应用，我们需要一些工具来帮助自动维护这些应用，在容器的生命周期里，可以自动替换失败的容器，管理滚动升级，以及重新配置．　容器编排器（Orchestrator）就是管理，扩展和维护容器化应用的工具，当前最常见的例子就是 Kubernetes 和 Docke Swarm ．Docker Desktop 工具可以在开发环境提供这两个编排工具．当前，Docker Desktop 仅支持在Windows和OSX系统上安装，本文接下来主要介绍如何在Linux上安装Docker，以及运行一个容器．\n安装Docker 如果你使用的是Windows或者Mac OS系统，请参考上面的链接安装和使用 Docker Desktop，下面我们将已Ubuntu 18.04系统为例来安装Docker的社区版本（docker-ce）．　配置软件源 更新apt包索引 $ sudo apt update 安装需要的软件包 $ sudo apt install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common 添加Docker官方的GPG信息 $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 使用如下命令添加Docker的安装源 $ sudo add-apt-repository \\ \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026#34; stable意思为添加的是稳定版本的源．\n安装 更新软件源 $ sudo apt update 安装最新版本的Docker引擎和containerd $ sudo apt install docker-ce docker-ce-cli containerd.io 验证安装 $ sudo docker run hello-world 该命令会下载一个测试镜像并运行一个容器，该容器会打印一些信息后退出．　补充：　为了运行docker命令不需要使用sudo，可以将当前用户加入docker\n$ sudo usermod -aG \u0026#34;docker\u0026#34; \u0026lt;username\u0026gt; 在重新登录用户后，可以直接使用docker命令进行操作．\n运行容器 在上面安装docker引擎的最后一个验证步骤中，其实我们已经运行了一个容器，这里再示例如何运行一个ubuntu操作系统的基础容器．\n以下命令假设已经将当前用户添加到了docker组．\n拉取（Pull）镜像 $ docker image pull ubuntu:18.04 该命令将从Docker官方的镜像仓库（Docker Hub）上下载一个官方维护的Ubuntu的基础镜像，标签为18.04．　运行容器 $ docker container run --rm -it ubuntu:18.04 root@451a1f6ed7c9:/# 上面的命令，将用ubuntu:18.04的镜像运行一个容器，--rm选项指示在退出或停止容器时自动删除该容器，-it选项表示创建一个tty并与其交互，因为该容器默认运行的进程是bash，这样我们可直接在容器里进行交互．\n在容器运行时，可以打开另一个终端，使用如下命令查看当前运行在该主机上的容器\ndocker container ls CONTAINER ID NAMES IMAGE CREATED ago STATUS PORTS COMMAND 451a1f6ed7c9 musing_diffie ubuntu:18.04 4 minutes ago ago Up 4 minutes \u0026#34;/bin/bash\u0026#34; 当我们在容器的只能高端里执行exit，或者在主机的另一个终端里执行docker container stop musing_diffie，都将停止并删除该容器．\n总结 这里我们介绍了容器的基本概念，以及在Ubuntu系统上安装Docker，运行了一个简单的容器．该系列接下来的文章中我会介绍如何容器化我们的应用程序．\n","date":"February 13, 2020","hero":"/zh/posts/container-tech/docker-intro/docker-banner.png","permalink":"/zh/posts/container-tech/docker-intro/","summary":"Docker Docker是一个为开发者和运维工程师（系统管理员）以容器的方式构建，分享和运行应用的平台。使用容器进行应用部署的方式，我们成为容器化。\n容器化应用具有一下特性，使得容器化日益流行：\n灵活：　再复杂的应用都可以进行容器化． 轻量：　容器使用使用和共享主机的内核，在系统资源的利用比虚拟机更加高效． 可移植：　容器可以本地构建，部署到云上，运行在任何地方． 松耦合：　容器是高度自封装的，可以在不影响其他容器的情况下替换和升级容器． 可扩展：　可以在整个数据中心里增加和自动分发容器副本． 安全：　容器约束和隔离应用进程，而无需用户进行任何配置． 镜像和容器 其实，容器就是运行的进程，附带一些封装的特性，使其与主机上和其他容器的进程隔离．每个容器都只访问它自己私有的文件系统，这是容器隔离很重要的一方面．而Docker镜像就提供了这个文件系统，一个镜像包含运行该应用所有需求 - 代码或者二进制文件，运行时，依赖库，以及其他需要的文件系统对象．\n通过与虚拟机对比，虚拟机（VM）通过一个虚拟机管理（Hypervisor）运行了完整的操作系统来访问主机资源．通常虚拟机会产生大量的开销，超过了应用本身所需要的开销．\n容器编排 容器化过程的可移植性和可重复性意味着我们有机会跨云和数据中心移动和扩展容器化的应用程序，容器有效地保证应用程序可以在任何地方以相同的方式运行，这使我们可以快速有效地利用所有这些环境．当我们扩展我们的应用，我们需要一些工具来帮助自动维护这些应用，在容器的生命周期里，可以自动替换失败的容器，管理滚动升级，以及重新配置．　容器编排器（Orchestrator）就是管理，扩展和维护容器化应用的工具，当前最常见的例子就是 Kubernetes 和 Docke Swarm ．Docker Desktop 工具可以在开发环境提供这两个编排工具．当前，Docker Desktop 仅支持在Windows和OSX系统上安装，本文接下来主要介绍如何在Linux上安装Docker，以及运行一个容器．\n安装Docker 如果你使用的是Windows或者Mac OS系统，请参考上面的链接安装和使用 Docker Desktop，下面我们将已Ubuntu 18.04系统为例来安装Docker的社区版本（docker-ce）．　配置软件源 更新apt包索引 $ sudo apt update 安装需要的软件包 $ sudo apt install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common 添加Docker官方的GPG信息 $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 使用如下命令添加Docker的安装源 $ sudo add-apt-repository \\ \u0026#34;deb [arch=amd64] https://download.","tags":["docker"],"title":"DOCKER入门"},{"categories":["Linux"],"contents":"在使用Ubuntu Linux系统时，经常发现在升级Linux内核之后，旧版本的内核包依旧保留在系统中，占据了一定的磁盘空间。系统并不会自动删除掉旧版本的内核，是因为保证在使用新内核无法启动时，还可以选择使用旧版本的内核来启动系统。因此在使用新版本内核启动成功之后，我们需要手动来删除掉一些旧版本的内核包，以释放一定的磁盘空间。那我们如何安全的删除旧版本的内核呢？\n删除旧的内核映像 以下是在Ubunut上安全删除旧的Linux内核映像步骤，你不必须以root用户执行命令，或者使用sudo．　步骤一　- 启动到新的内核　首先启动到新安装的内核版本，可以使用如下的命令来验证当前内核版本：\n\u0026gt; uname -mrs \u0026gt; uname -a 输出样例如下：\nLinux 4.4.0-117-generic x86_64\n使用以下命令显示当前系统中已经安装的所有Linux内核映像：\n# dpkg --list | egrep -i --color \u0026#39;linux-image|linux-headers\u0026#39; 输出可能如下：\nii linux-headers-4.15.0-45 4.15.0-45.48 all Header files related to Linux kernel version 4.15.0 ii linux-headers-4.15.0-45-generic 4.15.0-45.48 amd64 Linux kernel headers for version 4.15.0 on 64 bit x86 SMP ii linux-headers-generic 4.15.0.45.47 amd64 Generic Linux kernel headers ii linux-image-4.15.0-45-generic 4.15.0-45.48 amd64 Signed kernel image generic ii linux-image-generic 4.15.0.45.47 amd64 Generic Linux kernel image rc linux-headers-4.15.0-22 4.15.0-22.24 all Header files related to Linux kernel version 4.15.0 rc linux-headers-4.15.0-22-generic 4.15.0-22.24 amd64 Linux kernel headers for version 4.15.0 on 64 bit x86 SMP rc linux-headers-generic 4.15.0.22.23 amd64 Generic Linux kernel headers rc linux-image-4.15.0-22-generic 4.15.0-22.24 amd64 Signed kernel image generic rc linux-image-generic 4.15.0.22.23 amd64 Generic Linux kernel image 步骤二　- 删除不想要或者不使用的内核映像 然后可以使用如下命令一个一个地删除掉旧的内核包：\n# apt-get --purge remove linux-image-4.15.0-22-generic 或者　$ sudo apt-get --purge remove linux-image-4.15.0-22-generic 对于较新的Ubunu发行版的提示 在较新的系统上，所有过期的内核和相关头文件包会自动标记为不再使用的状态，因此可以使用以下一条命令直接清除：\n$ sudo apt --purge autoremove 直接使用一个脚本来删除　可以考虑使用如下的一个功夫脚本来清除旧的内核包：\n#!/usr/bin/env bash v=\u0026#34;$(uname -r | awk -F \u0026#39;-virtual\u0026#39; \u0026#39;{ print $1}\u0026#39;)\u0026#34; i=\u0026#34;linux-headers-virtual|linux-image-virtual|linux-headers-${v}|linux-image-$(uname -r)\u0026#34; apt-get --purge remove $(dpkg --list | egrep -i \u0026#39;linux-image|linux-headers\u0026#39; | awk \u0026#39;/ii/{ print $2}\u0026#39; | egrep -v \u0026#34;$i\u0026#34;) ","date":"December 8, 2019","hero":"/zh/posts/linux/ubuntu-old-kernel/ubuntu-kernel.jpg","permalink":"/zh/posts/linux/ubuntu-old-kernel/","summary":"在使用Ubuntu Linux系统时，经常发现在升级Linux内核之后，旧版本的内核包依旧保留在系统中，占据了一定的磁盘空间。系统并不会自动删除掉旧版本的内核，是因为保证在使用新内核无法启动时，还可以选择使用旧版本的内核来启动系统。因此在使用新版本内核启动成功之后，我们需要手动来删除掉一些旧版本的内核包，以释放一定的磁盘空间。那我们如何安全的删除旧版本的内核呢？\n删除旧的内核映像 以下是在Ubunut上安全删除旧的Linux内核映像步骤，你不必须以root用户执行命令，或者使用sudo．　步骤一　- 启动到新的内核　首先启动到新安装的内核版本，可以使用如下的命令来验证当前内核版本：\n\u0026gt; uname -mrs \u0026gt; uname -a 输出样例如下：\nLinux 4.4.0-117-generic x86_64\n使用以下命令显示当前系统中已经安装的所有Linux内核映像：\n# dpkg --list | egrep -i --color \u0026#39;linux-image|linux-headers\u0026#39; 输出可能如下：\nii linux-headers-4.15.0-45 4.15.0-45.48 all Header files related to Linux kernel version 4.15.0 ii linux-headers-4.15.0-45-generic 4.15.0-45.48 amd64 Linux kernel headers for version 4.15.0 on 64 bit x86 SMP ii linux-headers-generic 4.15.0.45.47 amd64 Generic Linux kernel headers ii linux-image-4.15.0-45-generic 4.15.0-45.48 amd64 Signed kernel image generic ii linux-image-generic 4.","tags":["ubuntu","kernel"],"title":"在UBUNTU上删除旧的内核"},{"categories":["OpenTool"],"contents":"Rclone是一个命令行云存储同步工具，可以在文件系统和云存储服务之间或者多个云存储服务之间访问和同步文件，支持很多云存储服务后端。\nRclone支持的云存储服务 rclone的当前版本为v1.49.1，支持以下云存储服务：\n1Fichier Alibaba Cloud (Aliyun) Object Storage System (OSS) Amazon Drive (See note) Amazon S3 Backblaze B2 Box Ceph C14 DigitalOcean Spaces Dreamhost Dropbox FTP Google Cloud Storage Google Drive Google Photos HTTP Hubic Jottacloud IBM COS S3 Koofr Memset Memstore Mega Microsoft Azure Blob Storage Microsoft OneDrive Minio Nextcloud OVH OpenDrive Openstack Swift Oracle Cloud Storage ownCloud pCloud premiumize.me put.io QingStor Rackspace Cloud Files rsync.net Scaleway SFTP Wasabi WebDAV Yandex Disk The local filesystem 当前对于Google Photos的支持有一定的限制，具体请看文档。\nRcline的功能：\n始终检查MD5/SHA1哈希值保证文件完整性 保留文件时间戳 在整个文件上支持部分同步 复制模式仅仅复制新的/修改的文件 检查模式检查文件的哈希相等 同步 (单一方向)使文件目录保持一致 可以同步到或者从网络同步 支持后端加密 支持后端缓存 支持后端联合 可选的FUSE挂载 多线程下载到本地磁盘 可以通过HTTP/WebDav/FTP/SFTP/dlna为本地或者远端文件提供服务 实验性的基于Web的图形界面 安装Rclone 对于Linux环境，由于rclone是由Golang开发，因此安装非常简单。管发提供了编译好的二进制可执行程序，以及RPM和DEB包，可根据自己系统的情况选择。以下演示直接下载二进制可执行文件的方式。\n下载地址： https://downloads.rclone.org/v1.49.1/rclone-v1.49.1-linux-amd64.zip\n❯ wget https://downloads.rclone.org/v1.49.1/rclone-v1.49.1-linux-amd64.zip ❯ unzip -qq rclone-v1.49.1-linux-amd64.zip ❯ sudo cp rclone-v1.49.1-linux-amd64/rclone /usr/local/bin/ ❯ sudo chmod +x /usr/local/bin/rclone 配置云存储服务 Rclone支持很多云存储服务，以下举例配置两个服务Microsoft OneDrive和Dropbox。\n对于其他的云存储服务，可参考官方文档。\nMicrosoft OneDrive 在命令行终端执行rclone config :\n❯ rclone config 2019/08/30 11:21:13 NOTICE: Config file \u0026#34;/home/mengz/.config/rclone/rclone.conf\u0026#34; not found - using defaults No remotes found - make a new one n) New remote s) Set configuration password q) Quit config n/s/q\u0026gt; n name\u0026gt; onedrive 这里使用n添加新的远程服务，然后给服务设置一个名字，这里使用了onedrive，接下来需要选择存储服务：\nType of storage to configure. Enter a string value. Press Enter for the default (\u0026#34;\u0026#34;). Choose a number from below, or type in your own value ... ... Storage\u0026gt; onedrive ... 我们选择onedrive，接下来是询问client_id和client_secret，这里直接回车，后面会通过打开浏览器来授权：\nMicrosoft App Client Id Leave blank normally. Enter a string value. Press Enter for the default (\u0026#34;\u0026#34;). client_id\u0026gt; Microsoft App Client Secret Leave blank normally. Enter a string value. Press Enter for the default (\u0026#34;\u0026#34;). client_secret\u0026gt; Edit advanced config? (y/n) y) Yes n) No y/n\u0026gt; n Remote config Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine y) Yes n) No y/n\u0026gt; y If your browser doesn\\\u0026#39;t open automatically go to the following link: http://127.0.0.1:53682/auth Log in and authorize rclone for access Waiting for code... 会自动打开浏览器，请求服务授权，授权成功后返回终端继续：\nGot code Choose a number from below, or type in an existing value ... Your choice\u0026gt; 1 Found 1 drives, please select the one you want to use: 0: OneDrive (business) id=b!Eqwertyuiopasdfghjklzxcvbnm-7mnbvcxzlkjhgfdsapoiuytrewqk Chose drive to use:\u0026gt; 0 Found drive \u0026#39;root\u0026#39; of type \u0026#39;business\u0026#39;, URL: https://org-my.sharepoint.com/personal/you/Documents Is that okay? y) Yes n) No y/n\u0026gt; y -------------------- ... -------------------- y) Yes this is OK e) Edit this remote d) Delete this remote y/e/d\u0026gt; y Current remotes: Name Type ==== ==== onedrive onedrive e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q\u0026gt; q 这样就配置好了OneDrive的访问，现在在终端可以使用rclone的操作命令来查看远程存储的内容了：\n❯ rclone lsd onedrive: ❯ rclone copy ./file onedrive:Temp/ ❯ rclone ls onedrive:Temp/ 1452 file 关于OneDrive的详细详细，参考官方文档。\nDropbox 其实Dropbox的配置方式与上面几乎一样，就不做具体的展示了，只是要说明的一点是，对于像Dropbox，Google Drive之类的服务，需要你终端配置相应的代理（也就是可以科学上网），否则会无法访问相应的服务。\n当前配置完成之后，运行命令 rclone config 可以看到当前配置了的远程服务：\n❯ rclone config Current remotes: Name Type ==== ==== dropbox dropbox gdrive drive onedrive onedrive 相应的配置文件位置在 ~/.config/rclone/rclone.conf 。\n基于Web的图形界面 基于Web的图形管理界面是实验性的功能，可通过以下命令打开：\n❯ rclone rcd --rc-web-gui\n其会自动打开浏览器并自动登录，\nDashboard ： 显示Rclone的状态，全局状态、速度和允许设置的带宽 配置 ： 允许添加新的后端配置、更新或者删除现有的配置 Explorer ： 允许浏览配置的Rclone远程后端，并执行文件操作 Backend ： 版本信息和各种后端设置 总结 Rclone提了大量的云存储服务支持，供用户选择，使得用户方便在命令行终端操作云存储的文件和目录，同时提供了一个实验性的基于Web的图像界面，使用方便，有兴趣的朋友可以尝试一下。\n更多的功能请参考官方网站。\n","date":"August 30, 2019","hero":"/zh/posts/opentool/rclone-sync/rclone.png","permalink":"/zh/posts/opentool/rclone-sync/","summary":"Rclone是一个命令行云存储同步工具，可以在文件系统和云存储服务之间或者多个云存储服务之间访问和同步文件，支持很多云存储服务后端。\nRclone支持的云存储服务 rclone的当前版本为v1.49.1，支持以下云存储服务：\n1Fichier Alibaba Cloud (Aliyun) Object Storage System (OSS) Amazon Drive (See note) Amazon S3 Backblaze B2 Box Ceph C14 DigitalOcean Spaces Dreamhost Dropbox FTP Google Cloud Storage Google Drive Google Photos HTTP Hubic Jottacloud IBM COS S3 Koofr Memset Memstore Mega Microsoft Azure Blob Storage Microsoft OneDrive Minio Nextcloud OVH OpenDrive Openstack Swift Oracle Cloud Storage ownCloud pCloud premiumize.me put.io QingStor Rackspace Cloud Files rsync.net Scaleway SFTP Wasabi WebDAV Yandex Disk The local filesystem 当前对于Google Photos的支持有一定的限制，具体请看文档。","tags":["cloud","rclone","linux"],"title":"RCLOUD与云存储同步"},{"categories":["Linux"],"contents":"我们都知道在Linux上默认都会有一个名为root的超级用户，该用户可以修改系统上的任何文件和目录，那我们怎么创建一些不能被删除/修改的文件和目录呢？\n那在Linux系统中有一个命令chattr可以用来修改文件和目录的属性，通过该命令就可以设置文件和目录不可删除，甚至包括root也不能操作。\n创建不可删除的文件 例如，我们在系统上新建一个名为undeletable-file的文件，通过名了chattr设置其属性为不可修改的：\n❯ echo \u0026#34;some contents\u0026#34; ❯ ~/undeletable-file ❯ sudo chattr +i -V ~/undeletable-file chattr 1.43.8 (1-Jan-2018) Flags of /home/mengz/undeletable-file set as ----i-------------- ❯ rm -f ~/undeletable-file rm: cannot remove \u0026#39;undeletable-file\u0026#39;: Operation not permitted ❯ sudo rm -f ~/undeletable-file rm: cannot remove \u0026#39;undeletable-file\u0026#39;: Operation not permitted ❯ echo \u0026#34;change\u0026#34; ❯❯ ~/undeletable-file bash: undeletable-file: Operation not permitted 注意 ： 是用命令chattr修改属性的时候需要root权限，因此这里使用了sudo 。\n我们也可以通过命令lsattr来查看当前文件的属性：\n❯ lsattr ~/undeletable-file ----i-------------- /home/mengz/undeletable-file 设置目录不可修改 针对目录，同样是用命令chattr，是用-R选项可以递归地修改目录和其文件的属性：\n❯ mkdir -p immutable-dir/{dir1,dir2} ❯ touch immutable-dir/dir1/file1 ❯ touch immutable-dir/dir2/file2 ❯ sudo chattr +i -RV immutable-dir/ chattr 1.43.8 (1-Jan-2018) Flags of immutable-dir/ set as ----i-------------- Flags of immutable-dir//dir1 set as ----i-------------- Flags of immutable-dir//dir1/file1 set as ----i-------------- Flags of immutable-dir//dir2 set as ----i-------------- Flags of immutable-dir//dir2/file2 set as ----i-------------- ❯ rm -rf immutable-dir/ rm: cannot remove \u0026#39;immutable-dir/dir1/file1\u0026#39;: Operation not permitted rm: cannot remove \u0026#39;immutable-dir/dir2/file2\u0026#39;: Operation not permitted ❯ sudo rm -f immutable-dir/dir1/file1 rm: cannot remove \u0026#39;immutable-dir/dir1/file1\u0026#39;: Operation not permitted 要使文件或者目录可修改，是用命令chattr加上选项-i ：\n❯ sudo chattr -i -RV immutable-dir/ chattr 1.43.8 (1-Jan-2018) Flags of immutable-dir/ set as ------------------- Flags of immutable-dir//dir1 set as ------------------- Flags of immutable-dir//dir1/file1 set as ------------------- Flags of immutable-dir//dir2 set as ------------------- Flags of immutable-dir//dir2/file2 set as ------------------- ❯ rm -rf immutable-dir/ ","date":"August 24, 2019","hero":"/zh/posts/linux/make-undeletable/linux-commands.jpg","permalink":"/zh/posts/linux/make-undeletable/","summary":"我们都知道在Linux上默认都会有一个名为root的超级用户，该用户可以修改系统上的任何文件和目录，那我们怎么创建一些不能被删除/修改的文件和目录呢？\n那在Linux系统中有一个命令chattr可以用来修改文件和目录的属性，通过该命令就可以设置文件和目录不可删除，甚至包括root也不能操作。\n创建不可删除的文件 例如，我们在系统上新建一个名为undeletable-file的文件，通过名了chattr设置其属性为不可修改的：\n❯ echo \u0026#34;some contents\u0026#34; ❯ ~/undeletable-file ❯ sudo chattr +i -V ~/undeletable-file chattr 1.43.8 (1-Jan-2018) Flags of /home/mengz/undeletable-file set as ----i-------------- ❯ rm -f ~/undeletable-file rm: cannot remove \u0026#39;undeletable-file\u0026#39;: Operation not permitted ❯ sudo rm -f ~/undeletable-file rm: cannot remove \u0026#39;undeletable-file\u0026#39;: Operation not permitted ❯ echo \u0026#34;change\u0026#34; ❯❯ ~/undeletable-file bash: undeletable-file: Operation not permitted 注意 ： 是用命令chattr修改属性的时候需要root权限，因此这里使用了sudo 。\n我们也可以通过命令lsattr来查看当前文件的属性：\n❯ lsattr ~/undeletable-file ----i-------------- /home/mengz/undeletable-file 设置目录不可修改 针对目录，同样是用命令chattr，是用-R选项可以递归地修改目录和其文件的属性：","tags":["linux","command"],"title":"LINUX上创建不可删除文件"},{"categories":["Linux"],"contents":"如今有很多笔记本电脑都配备了双显卡，一块集成的 Intel 显卡，一块性能更好一些的 NVIDIA 显卡。\n可是在平时的使用中可能根本用不上 NVIDIA 的那块显卡，那么为了使这样的笔记本电脑更省电，那么在平时的使用中可以禁用 NVIDIA 的显卡，而只使用集成的显卡。 Bumblebee 就是一个开源项目，在 Linux 上实现了 NVIDIA 的 Optimus 技术，在需要的时候使用 NVIDIA 的显卡。\n我的电脑是 Lenovo ThinkPad T440s ，配备了如下的两块显卡：\n1. 00:02.0 VGA compatible controller: Intel Corporation Haswell-ULT Integrated Graphics Controller (rev 09) 2. VGA compatible controller: NVIDIA Corporation GK208M [GeForce GT 730M] (rev ff) 而我使用的系统是 openSUSE Leap 42.1，下面就看看如何在该系统上禁用 NVIDIA 的显卡。\n安装所需的软件包 首先添加如下安装源：\nsudo zypper ar -r http://download.opensuse.org/repositories/X11:/Bumblebee/openSUSE_Leap_42.1/X11:Bumblebee.repo\n刷新后，安装如下软件包：\nbumblebee nvidia-bumblebee bbswitch bbswitch-kmp-default\n如果你是64位系统，还请安装\nnvidia-bumblebee-32bit\n之后还需要将用户添加入 bumblebee 和 video 两个组：\nsudo usermod -a bumblebee username\nsudo usermod -a video username\n启动服务 安装完以上软件包之后，需要启动连个服务：\nsudo systemctl enable bumblebeed\nsudo systemctl enable dkms\n然后检查 /etc/modprobe.d/50-blacklist.conf 文件，确保 blacklist nouveau 存在（除非你确定要使用 nouveau 驱动）。 之后\nmkinitrd\n重启系统。\n查看状态 重启之后，使用如下命令查看状态\noptirun \u0026ndash;status\n如果显示的结果是：\nBumblebee status: Ready (3.2.1). X inactive. Discrete video card is off.\n那么恭喜，你已经配置成功并且禁用了 NVIDIA 的显卡。 如果你想测试你的 NVIDIA 的显卡，可以运行如下命令：\noptirun glxsheres\n这是将会启用 NVIDIA 显卡来运行该程序，所以如果你想使用 NVIDIA 的显卡来运行某个应用程序，请使用 optirun 命令。\n排除问题 如果你在 \u0026ldquo;optirun \u0026ndash;status\u0026rdquo; 中得到的结果是：\nBumblebee status: Ready (3.2.1). X inactive. Discrete video card is on.\n那么首先检查文件 /etc/modprobe.d/50-bbswitch.conf ，看是有如下内容：\noptions bbswitch load_state=0\n然后检查\nsudo systemctl -l status bumblebeed\n看是否有类似如下的信息\n/dev/dri/card0: failed to set DRM interface version 1.4: Permission denied\n如果有，那么往文件 /etc/bumblebee/xorg.conf.nvidia 里添加如下内容：\nSection \u0026#34;Screen\u0026#34; Identifier \u0026#34;Default Screen\u0026#34; Device \u0026#34;DiscreteNvidia\u0026#34; EndSection 然后重启该服务。\n在其次，检查\ndmesg | grep -C 10 bbswitch\n看是否有类似如下内容：\nACPI Warning: _SB_.PCI0.PEG_.VID_._DSM: Argument #4 type mismatch - Found [Buffer], ACPI requires [Package] (20150410/nsarguments-95)\n这样需要在启动内核参数添加 \u0026ldquo;acpi_osi=!Windows 2013\u0026rdquo; 后重启系统。\n如果想了解更多的情况，请查看考 Github 上的 Bumblebee 和 bbswitch 两个项目。\n","date":"November 25, 2015","hero":"/zh/posts/linux/bumlebee-nvidia/bumblebee-nvidia.png","permalink":"/zh/posts/linux/bumlebee-nvidia/","summary":"如今有很多笔记本电脑都配备了双显卡，一块集成的 Intel 显卡，一块性能更好一些的 NVIDIA 显卡。\n可是在平时的使用中可能根本用不上 NVIDIA 的那块显卡，那么为了使这样的笔记本电脑更省电，那么在平时的使用中可以禁用 NVIDIA 的显卡，而只使用集成的显卡。 Bumblebee 就是一个开源项目，在 Linux 上实现了 NVIDIA 的 Optimus 技术，在需要的时候使用 NVIDIA 的显卡。\n我的电脑是 Lenovo ThinkPad T440s ，配备了如下的两块显卡：\n1. 00:02.0 VGA compatible controller: Intel Corporation Haswell-ULT Integrated Graphics Controller (rev 09) 2. VGA compatible controller: NVIDIA Corporation GK208M [GeForce GT 730M] (rev ff) 而我使用的系统是 openSUSE Leap 42.1，下面就看看如何在该系统上禁用 NVIDIA 的显卡。\n安装所需的软件包 首先添加如下安装源：\nsudo zypper ar -r http://download.opensuse.org/repositories/X11:/Bumblebee/openSUSE_Leap_42.1/X11:Bumblebee.repo\n刷新后，安装如下软件包：\nbumblebee nvidia-bumblebee bbswitch bbswitch-kmp-default\n如果你是64位系统，还请安装\nnvidia-bumblebee-32bit","tags":["opensuse","nvidia","linux"],"title":"BUMBLEBEE禁用NVIDIA显卡"},{"categories":["Container"],"contents":"现在 Docker 可所谓是最火的容器技术了，至于什么是 Docker，请到其官方网站或者维基百科查看。\n这里想通过一个示例来看看怎么通过 Dockerfile 来构建一个 Docker 镜像。\n构建MariaDB容器镜像 Docker 提供了两种方法来生产应用镜像:\n通过启动一个基础容器（比如基于某种 Linux 发行版的镜像的容器），然后在容器里执行各种命令来安装相应的软件包，进行配置后，再通过 docker commit 命令把已经更新的容器生产相应的镜像。 通过编写一个 Dockerfile ，然后使用 docker build 命令来构建相应的镜像。 相比第一种方式，通过 Dockerfile 的方式，可以更好的维护镜像，将镜像的 Dockerfile 提交到版本库管理。还可以在 Docker Hub 里创建镜像的自动构建。\n下面让我们通过如何编写 Dockerfile 来构建一个 mariadb 的镜像：\n首先创建一个目录，如 docker-mariadb ，然后编写一个名为 Dockerfile 的文件，内容如下：\nFROM opensuse:13.2 MAINTAINER Mengz You \u0026lt;you.mengz@yahoo.com\u0026gt; ENV MARIADB_MAJOR 10.0 ENV MARIADB_VERSION 10.0.17 ENV MYSQL_ROOT_PASSWORD mysecretpassword ENV MYSQL_DATADIR /var/lib/mysql RUN zypper ar -f -r http://download.opensuse.org/repositories/server:/database/openSUSE_13.2/server:database.repo \\ \u0026amp;\u0026amp; zypper -n --gpg-auto-import-keys ref RUN zypper -n in --no-recommends mariadb-$MARIADB_VERSION net-tools \\ \u0026amp;\u0026amp; zypper clean --all RUN mkdir -p /var/lib/mysql \\ \u0026amp;\u0026amp; mkdir -p /var/log/mysql \\ \u0026amp;\u0026amp; chown mysql:mysql /var/log/mysql VOLUME /var/lib/mysql COPY docker-entrypoint.sh / ENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;] EXPOSE 3306 CMD [\u0026#34;mysqld_safe\u0026#34;] 让我们来看看上面的 Dockerfile 里用到的指令：\nFROM ： 指明用来构建该镜像的基础镜像，该示例中我们使用 Docker Hub 官方的 opensuse 镜像，格式为 image[:tag] ； MAINTAINER ： 指明该 Dockerfile 的维护者； ENV ： 设置环境变量，后面的命令可以使用这些变量。也可以在启动容器时改变其值； RUN ： 执行命令，比如设置软件源，安装相应的软件包，在上面的例子中，将会在构建镜像的时候安装 Mariadb 的包，然后配置一些使用的目录； VOLUME ： 设置目录卷，使得在运行容器的时候可以通过 -v 参数来指定挂载和主机同步的目录； COPY ： 将构建目录下的脚本放到镜像里，然后可以在容器中执行。上例中，docker-entrypoint.sh 将在启动容器时作为容器入口点执行； ENTRYPOINT ： 配置容器启动时执行的命令，每个 Dockerfile 中之应该只有一个 ENTRYPOINT ； EXPOSE ： 配置容器暴露的服务端口号，在启动容器时，如果使用 -P 参数，Docker 主机将会自动分配一个随机端口转发到制定的端口，也可以额使用 -p 制定主机的具体端口； CMD ： 也是可以制定容器启动时执行的命令，其和 ENTRYPOINT 的区别是，其可以作为 ENTRYPOINT 的参数，如上例中启动容器时，其实执行的是 /docker-entrypint.sh mysqld_safe。还有一个区别就是 ENTRYPOINT 制定的命令不能被启动容器时的命令覆盖，而 CMD 指定的可以被覆盖。 以上是本示例中用的指令，其实 Dockerfile 还有其他指令，请参考 Dockerfile 的文档。\n由于本例中我们使用到了一个脚本来作为容器的入口命令，因此还需要在当前目录下写该脚本文件 docker-entrypoint.sh ：\n#!/bin/bash set -e if [ ! -d \u0026#34;$MYSQL_DATADIR/mysql\u0026#34; ]; then echo \u0026#39;Running mysql_install_db ...\u0026#39; mysql_install_db --datadir=\u0026#34;$MYSQL_DATADIR\u0026#34; echo \u0026#39;Finished mysql_install_db\u0026#39; tempSqlFile=\u0026#39;/tmp/mysql-first-time.sql\u0026#39; cat \u0026gt; \u0026#34;$tempSqlFile\u0026#34; \u0026lt;\u0026lt;-EOSQL DELETE FROM mysql.user ; CREATE USER \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;${MYSQL_ROOT_PASSWORD}\u0026#39; ; GRANT ALL ON *.* TO \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; WITH GRANT OPTION ; DROP DATABASE IF EXISTS test ; EOSQL if [ \u0026#34;$MYSQL_DATABASE\u0026#34; ]; then echo \u0026#34;CREATE DATABASE IF NOT EXISTS \\`$MYSQL_DATABASE\\` ;\u0026#34; \u0026gt;\u0026gt; \u0026#34;$tempSqlFile\u0026#34; fi if [ \u0026#34;$MYSQL_USER\u0026#34; -a \u0026#34;$MYSQL_PASSWORD\u0026#34; ]; then echo \u0026#34;CREATE USER \u0026#39;$MYSQL_USER\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;$MYSQL_PASSWORD\u0026#39; ;\u0026#34; \u0026gt;\u0026gt; \u0026#34;$tempSqlFile\u0026#34; if [ \u0026#34;$MYSQL_DATABASE\u0026#34; ]; then echo \u0026#34;GRANT ALL ON \\`$MYSQL_DATABASE\\`.* TO \u0026#39;$MYSQL_USER\u0026#39;@\u0026#39;%\u0026#39; ;\u0026#34; \u0026gt;\u0026gt; \u0026#34;$tempSqlFile\u0026#34; fi fi echo \u0026#39;FLUSH PRIVILEGES ;\u0026#39; \u0026gt;\u0026gt; \u0026#34;$tempSqlFile\u0026#34; set -- \u0026#34;$@\u0026#34; --init-file=\u0026#34;$tempSqlFile\u0026#34; fi chown -R mysql:mysql \u0026#34;$MYSQL_DATADIR\u0026#34; exec \u0026#34;$@\u0026#34; 该脚本通过相应的环境变量来初始化 mariadb 实例，后面我们会给出容器运行的命令示例。\n最后，我们就可以通过在当前目录下执行 docker build 命令来构建镜像，如：\ndocker build -t docker-mariadb:latest .\n如果本地没有 opensuse 的镜像，首先会下载 opensuse 的镜像，然后会基于 opensuse 的镜像，运行 Dockerfile 里指定的命令，最后会生成 docker-mariadb:latest 镜像，可以通过 docker images 看到。\n运行Mariadb容器 镜像生成后，如何来运行该镜像的容器呢？通过上面的脚本，我们可以看到，我们在启动容器时可以指定相应的环境变量，如：\ndocker run --name mydb -e MYSQL_USER=test -e MYSQL_PASSWORD=testpw -e MYSQL_DATABASE=testdb -p 13306:3306 -d docker-mariadb:latest\n该命令会启动一个该镜像的容器，名为 \u0026lsquo;mydb\u0026rsquo;，容器中的 mariadb 会在启动时创建名为 \u0026rsquo;testdb\u0026rsquo; 的数据库实例，名为 \u0026rsquo;test\u0026rsquo; 的数据库用户，其密码为 \u0026rsquo;testpw\u0026rsquo;。\n这时你就可以通过在主机上运行一下命令来尝试登录数据库了：\nmysql -h127.0.0.1 -P13306 -utest -ptestpw testdb\n是不是感觉通过 Dockerfile 来构建 Docker 镜像是如此的简单。最后你可以将该目录作为一个项目 push 到 Github，然后在到 Docker Hub 去创建一个仓库，然后配置到 Github 代码库，每当你 push 更新 Dockerfile 后，都会自动构建镜像。就像我的镜像库一样，当然在 Docker Hub 里还有来自全世界人们构建的镜像。\n","date":"May 27, 2015","hero":"/zh/posts/container-tech/mariadb-docker/mariadb-docker.png","permalink":"/zh/posts/container-tech/mariadb-docker/","summary":"现在 Docker 可所谓是最火的容器技术了，至于什么是 Docker，请到其官方网站或者维基百科查看。\n这里想通过一个示例来看看怎么通过 Dockerfile 来构建一个 Docker 镜像。\n构建MariaDB容器镜像 Docker 提供了两种方法来生产应用镜像:\n通过启动一个基础容器（比如基于某种 Linux 发行版的镜像的容器），然后在容器里执行各种命令来安装相应的软件包，进行配置后，再通过 docker commit 命令把已经更新的容器生产相应的镜像。 通过编写一个 Dockerfile ，然后使用 docker build 命令来构建相应的镜像。 相比第一种方式，通过 Dockerfile 的方式，可以更好的维护镜像，将镜像的 Dockerfile 提交到版本库管理。还可以在 Docker Hub 里创建镜像的自动构建。\n下面让我们通过如何编写 Dockerfile 来构建一个 mariadb 的镜像：\n首先创建一个目录，如 docker-mariadb ，然后编写一个名为 Dockerfile 的文件，内容如下：\nFROM opensuse:13.2 MAINTAINER Mengz You \u0026lt;you.mengz@yahoo.com\u0026gt; ENV MARIADB_MAJOR 10.0 ENV MARIADB_VERSION 10.0.17 ENV MYSQL_ROOT_PASSWORD mysecretpassword ENV MYSQL_DATADIR /var/lib/mysql RUN zypper ar -f -r http://download.opensuse.org/repositories/server:/database/openSUSE_13.2/server:database.repo \\ \u0026amp;\u0026amp; zypper -n --gpg-auto-import-keys ref RUN zypper -n in --no-recommends mariadb-$MARIADB_VERSION net-tools \\ \u0026amp;\u0026amp; zypper clean --all RUN mkdir -p /var/lib/mysql \\ \u0026amp;\u0026amp; mkdir -p /var/log/mysql \\ \u0026amp;\u0026amp; chown mysql:mysql /var/log/mysql VOLUME /var/lib/mysql COPY docker-entrypoint.","tags":["docker","mariadb"],"title":"DOCKER构建MariaDB"},{"categories":["Linux"],"contents":"基于 systemd 的 Linux 系统中提供了一个新的名来 hostnamectl 来管理系统主机名。\n当然除了 hostnamectl 之外，还是可以通过原来的 hostname 命令以及修改 /etc/HOSTNAME 来修改主机名。不过使用 hostnamectl 更方便。\nhostnamectl的语法 hostnamectl [OPTIONS...] {COMMAND}\n有如下 options :\n\u0026ndash;static,\u0026ndash;transient,\u0026ndash;pretty 如果用于 status 命令，static 显示当前的静态主机名; transient 显示临时的主机名，一般用于网络临时设置; pretty 显示良好阅读主机名，如\u0026quot;Sam\u0026rsquo;s Computer\u0026quot;。 H, \u0026ndash;host=user@hostname 用来操作远程主机。 命令：\nstatus ： 显示当前系统主机名和相关信息，可以使用 \u0026ndash;static, \u0026ndash;transient, \u0026ndash;pretty 仅显示指定内容。 set-hostname [NAME] ： 设置系统主机名，默认改变 pretty，static，及 transient 。 指定相应选项只改变相应主机名。 set-icon-name [NAME] ： 设置系统 Icon 名，用于一些图形应用来可视化主机。Icon 名需要符合 Icon 名规范。 set-chassis [TYPE] ： 设置 chassis 类型，用于一些图形应用来可视化主机或者改变用户界面。当前设置以下类型：\u0026ldquo;desktop\u0026rdquo;，\u0026ldquo;laptop\u0026rdquo;，\u0026ldquo;server\u0026rdquo;，\u0026ldquo;tablet\u0026rdquo;，\u0026ldquo;handset\u0026rdquo;，还有 \u0026ldquo;vm\u0026rdquo; 和 \u0026ldquo;container\u0026rdquo;。 示例 查看当前主机名及相关信息： $sudo hostnamectl status\nStatic hostname: mengz-dev.me\nIcon name: computer-laptop\nChassis: laptop\nMachine ID: 74fc18841ff54a7f8e09f6b58d4bad53\nBoot ID: ee3d24a9c1db41cf8f2696aeea513009\nOperating System: openSUSE 13.2 (Harlequin) (x86_64)\nCPE OS Name: cpe:/o:opensuse:opensuse:13.2\nKernel: Linux 3.16.7-21-desktop\nArchitecture: x86-64\n设置/改变静态主机名： $sudo hostnamectl set-hostname example-host.com\n总之，相比使用 hostname 和修改配置文件的方式，使用 hostnamectl 更方便。\n","date":"May 17, 2015","hero":"/zh/posts/linux/systemd-hostnamectl/systemd-hostname.jpg","permalink":"/zh/posts/linux/systemd-hostnamectl/","summary":"基于 systemd 的 Linux 系统中提供了一个新的名来 hostnamectl 来管理系统主机名。\n当然除了 hostnamectl 之外，还是可以通过原来的 hostname 命令以及修改 /etc/HOSTNAME 来修改主机名。不过使用 hostnamectl 更方便。\nhostnamectl的语法 hostnamectl [OPTIONS...] {COMMAND}\n有如下 options :\n\u0026ndash;static,\u0026ndash;transient,\u0026ndash;pretty 如果用于 status 命令，static 显示当前的静态主机名; transient 显示临时的主机名，一般用于网络临时设置; pretty 显示良好阅读主机名，如\u0026quot;Sam\u0026rsquo;s Computer\u0026quot;。 H, \u0026ndash;host=user@hostname 用来操作远程主机。 命令：\nstatus ： 显示当前系统主机名和相关信息，可以使用 \u0026ndash;static, \u0026ndash;transient, \u0026ndash;pretty 仅显示指定内容。 set-hostname [NAME] ： 设置系统主机名，默认改变 pretty，static，及 transient 。 指定相应选项只改变相应主机名。 set-icon-name [NAME] ： 设置系统 Icon 名，用于一些图形应用来可视化主机。Icon 名需要符合 Icon 名规范。 set-chassis [TYPE] ： 设置 chassis 类型，用于一些图形应用来可视化主机或者改变用户界面。当前设置以下类型：\u0026ldquo;desktop\u0026rdquo;，\u0026ldquo;laptop\u0026rdquo;，\u0026ldquo;server\u0026rdquo;，\u0026ldquo;tablet\u0026rdquo;，\u0026ldquo;handset\u0026rdquo;，还有 \u0026ldquo;vm\u0026rdquo; 和 \u0026ldquo;container\u0026rdquo;。 示例 查看当前主机名及相关信息： $sudo hostnamectl status","tags":["systemd","hostname","opensuse"],"title":"HOSTNAMECTL管理主机名"},{"categories":["Linux"],"contents":"自己在使用 opensuse，自己非常喜欢 opensuse 的包管理命令行工具 zypper。这里做一个笔记，也希望能看到这个篇文章的朋友能够快速地掌握 zypper 的用法。\nCentOS 和 Redhat 使用的是 yum 作为命令行的软件包管理工具。\nDebian 和 Ubuntu 使用的是 apt-get。\n在 Debian/Ubuntu 上还有另一个软件包管理工具 - aptitude 。\n同样，在 SUSE/opensuse Linux 上，zypper 就是其命令行的软件包管理工具。\n从高层次的角度，你可以使用 zypper 命令管理两种不同的东西：\n管理软件包： 使用 zypper 来安装，删除，更新以及查询本地的或者远端媒体上的软件包。 管理软件源： 也可以使用 zypper 管理软件源信息，你可以在命令行添加，删除，打开或者关闭某个软件源。它还可以设置在安装过程中软件源的优先级。 I. 使用 zypper 管理软件包 1. 安装软件包 使用如下语法安装一个软件包：\nzypper install \u0026lt;package name\u0026gt;\n如，执行一下命令来安装 火狐浏览器 和它的依赖：\n# zypper install MozillaFirefox Loading repository data... Reading installed packages... Resolving package dependencies... The following NEW packages are going to be installed: MozillaFirefox MozillaFirefox-branding-SLED The following packages are not supported by their vendor: MozillaFirefox MozillaFirefox-branding-SLED 2 new packages to install. Overall download size: 964.0 KiB. After the operation, additional 3.4 MiB will be used. Continue? [y/n/?] (y): y Retrieving package MozillaFirefox-3.6.16-0.2.1.x86_64 (1/2), 949.0 KiB (3.4 MiB unpacked) Retrieving: MozillaFirefox-3.6.16-0.2.1.x86_64.rpm [done] Installing: MozillaFirefox-3.6.16-0.2.1 [done] Retrieving package MozillaFirefox-branding-SLED-3.5-1.1.5.x86_64 (2/2), 15.0 KiB (34.0 KiB unpacked) Retrieving: MozillaFirefox-branding-SLED-3.5-1.1.5.x86_64.rpm [done] Installing: MozillaFirefox-branding-SLED-3.5-1.1.5 [done] 2. 安装源代码包 使用 source-install 选项来从软件源安装源代码包，如下：\n# zypper source-install apache2-mod_nss Reading installed packages... Loading repository data... Resolving package dependencies... Retrieving package mozilla-nss-devel-3.12.8-1.2.1.x86_64 (2/3), 473.0 KiB (2.6 MiB unpacked) Retrieving: mozilla-nss-devel-3.12.8-1.2.1.x86_64.rpm [done] Installing: mozilla-nss-devel-3.12.8-1.2.1 [done] Retrieving: apache2-mod_nss-1.0.8-17.5.src.rpm [done] 3. 更新软件包 一旦安装了一个软件包，当有可用的新版本时，你可以使用更新命令来升级相应的软件包。\n下面的示例仅仅更新火狐浏览器软件包：\nzypper update MozillaFirefox\n而下面的示例将升级安装在系统上的所有可更新的软件包到其最新版本：\nzypper update\n你也可以用下面的命令来查看所有的可用更新：\nzypper list-updates\n4. 系统发行版的升级 要在系统上执行完整的发行版升级，像下面一样使用 dup 选项：\n# zypper dup Warning: You are about to do a distribution upgrade with all enabled repositories. Make sure these repositories are compatible before you continue. See \u0026#39;man zypper\u0026#39; for more information about this command. Loading repository data... Reading installed packages... Computing distribution upgrade... The following NEW packages are going to be installed: drbd-xen libsoftokn3-32bit mozilla-nspr-32bit mozilla-nss-32bit mozilla-nss-certs-32bit suseRegister xen yast2-registration yast2-registration-branding-SLE The following packages are going to be upgraded: libfreebl3-32bit libnsssharedhelper0 libnsssharedhelper0-32bit The following packages are going to change vendor: libfreebl3-32bit SUSE LINUX Products GmbH, Nuernberg, Germany -\u0026gt; openSUSE Build Service libnsssharedhelper0 SUSE LINUX Products GmbH, Nuernberg, Germany -\u0026gt; openSUSE Build Service libnsssharedhelper0-32bit SUSE LINUX Products GmbH, Nuernberg, Germany -\u0026gt; openSUSE Build Service 5. 删除软件包 要删除软件包，使用 remove 命令选项，这也会删除掉所有的依赖。\n# zypper remove MozillaFirefox Loading repository data... Reading installed packages... Resolving package dependencies... The following packages are going to be REMOVED: MozillaFirefox MozillaFirefox-branding-SLED 2 packages to remove. After the operation, 3.4 MiB will be freed. Continue? [y/n/?] (y): y Removing MozillaFirefox-branding-SLED-3.5-1.1.5 [done] Removing MozillaFirefox-3.6.16-0.2.1 [done] 6. 搜索一个指定的软件包 要从软件源搜索一个软件包，使用下面的命令。你也可以在关键字中使用通配符。\n下面的示例将搜索所有以“usb”开头的软件包：\n# zypper search usb* Loading repository data... Reading installed packages... S | Name | Summary | Type --+----------------+----------------------------------------------------------------+-------- | usb_modeswitch | A mode switching tool for controlling multiple-device USB gear | package i | usbutils | Tools and libraries for USB devices | package 7. 查看软件包详细信息 使用 info 命令选项来显示一个软件包的详细信息，如：\n# zypper info usbutils Loading repository data... Reading installed packages... Information for package usbutils: Repository: @System Name: usbutils Version: 0.73-38.19 Arch: x86_64 Vendor: SUSE LINUX Products GmbH, Nuernberg, Germany Support Level: unknown Installed: Yes Status: up-to-date Installed Size: 461.0 KiB Summary: Tools and libraries for USB devices Description: This package contains a utility for inspecting devices connected to USB ports. It requires kernel version 2.3.99-pre7 or newer, or the USB backport which was introduced in 2.2.18 (supporting the /proc/bus/usb interface). 8. 安装补丁 你可以使用 zypper 来安装补丁到你系统。\n首先，使用 patches 命令选项来查看所有可用的补丁：\nzypper pathces\n然后，使用 pathc 命令选项来安装指定的补丁：\nzypper patch \u0026lt;patch name\u0026gt;\n9. 锁定指定软件包 包锁定阻止系统上的包被更改。一旦设置了锁，你不能删除，升级锁定的软件包。\n下面的示例展示了如何设置包锁定和在需要的时候移除锁。\n使用 al 命令选项来为一个软件包加锁，al 表示 “Add Lock”\n# zypper al ypbind Specified lock has been successfully added. 要显示所有被锁定的软件包，使用 ll 命令选项，代表 “List Locks”。下面的命令输出说明 “ypbind” 软件包是锁定状态，你不能删除和升级该包。\n# zypper al ypbind Specified lock has been successfully added. # zypper ll # | Name | Type | Repository --+--------+---------+----------- 1 | ypbind | package | (any) 10. 移除包锁定 使用 rl 命令选项来移除对一个包的锁定，rl 代表 “Remove Lock”\n# zypper rl ypbind Loading repository data... Reading installed packages... 1 lock has been successfully removed. 然后再次查看锁定的包时，已经没有锁定的包了\n# zypper ll There are no package locks defined. II. 管理软件源 11. 添加软件源 使用 RUI 来添加软件源的通用格式如下：\nzypper addrepo \u0026lt;options\u0026gt; \u0026lt;URI\u0026gt; \u0026lt;alias\u0026gt;\n例如：\n# zypper addrepo --check --refresh --name \u0026#34;Mozilla-repo\u0026#34; http://download.opensuse.org/repositories/mozilla/SLE_11/ \u0026#34;Mozillarepo\u0026#34; Adding repository \u0026#39;Mozilla-repo\u0026#39; [done] Repository \u0026#39;Mozilla-repo\u0026#39; successfully added Enabled: Yes Autorefresh: Yes URI: http://download.opensuse.org/repositories/mozilla/SLE_11/ 12. 创建一个本地源 你也可以在你的机器上从一个本地的目录来创建一个本地源，如下的命令创建了一个叫 myrepo 的本地源：\n# zypper addrepo /var/stormgt/dsminst mylocalrepo Adding repository \u0026#39;mylocalrepo\u0026#39; [done] Repository \u0026#39;mylocalrepo\u0026#39; successfully added Enabled: Yes Autorefresh: No URI: dir:///var/stormgt/dsminst # zypper search --repo mylocalrepo Loading repository data... Reading installed packages... S | Name | Summary | Type --+-------------+-----------------------------------------+-------- i | TIVsm-API | the API | package i | TIVsm-API64 | the API | package i | TIVsm-BA | the Backup Archive Client | package i | gskcrypt32 | IBM GSKit Cryptography Runtime | package i | gskcrypt64 | IBM GSKit Cryptography Runtime | package i | gskssl32 | IBM GSKit SSL Runtime With Acme Toolkit | package i | gskssl64 | IBM GSKit SSL Runtime With Acme Toolkit | package 你也可以使用上面的命令添加 NFS 或者 FTP 目录到源。\n13. 查看软件源 可以使用如下命令查看那所有添加的软件源：\n# zypper lr # | Alias | Name | Enabled | Refresh --+--------------------------------------------------+--------------------------------------------------+---------+-------- 1 | Mozillarepo | Mozilla-repo | Yes | Yes 2 | SUSE-Linux-Enterprise-Server-11-SP1 11.1.1-1.152 | SUSE-Linux-Enterprise-Server-11-SP1 11.1.1-1.152 | Yes | Yes 要查看完整的 RUI ，使用：\n# zypper lr --uri # | Alias | Name | Enabled | Refresh | URI --+--------------------------------------------------+--------------------------------------------------+---------+---------+---------------------------------------------------------- 1 | Mozillarepo | Mozilla-repo | Yes | Yes | http://download.opensuse.org/repositories/mozilla/SLE_11/ 2 | SUSE-Linux-Enterprise-Server-11-SP1 11.1.1-1.152 | SUSE-Linux-Enterprise-Server-11-SP1 11.1.1-1.152 | Yes | Yes | http://19.106.65.64/FUSELinux/600RC0 14. 在指定的源里搜索软件包 像列出一个软件源里的所有软件包，使用如下命令：\n# zypper search --repo Mozillarepo Loading repository data... Reading installed packages... S | Name | Summary | Type --+--------------------------------------+-------------------------------------------------------------------------+----------- i | MozillaFirefox | Mozilla Firefox Web Browser | package i | MozillaFirefox-branding-SLED | SLED branding of MozillaFirefox | package | MozillaFirefox-branding-openSUSE | openSUSE branding of MozillaFirefox | package | MozillaFirefox-branding-openSUSE | openSUSE branding of MozillaFirefox 15. 重命名一个软件源 使用 renamerepo 命令选项来重命名一个软件源。\n下面的示例中，名为 “mylocalrepo” 的软件源被重命名为 “LOCALRPM-Repo”\n# zypper renamerepo mylocalrepo LOCALRPM-Repo Repository \u0026#39;mylocalrepo\u0026#39; renamed to \u0026#39;LOCALRPM-Repo\u0026#39;. 16. 删除软件源 使用 removerepo 命令选项来删除软件源：\n# zypper removerepo LOCALRPM-Repo Removing repository \u0026#39;mylocalrepo\u0026#39; [done] Repository \u0026#39;mylocalrepo\u0026#39; has been removed. 17. 备份软件源 可以使用 lr 命令的 export 选项来将所有添加的软件源备份到一个文件：\n# zypper lr --export /var/tmp/backup.repo Repositories have been successfully exported to /var/tmp/backup.repo. 18. 从备份添加软件源 如果你有之前导出的软件源备份文件，那可以使用 addrepo 命令来添加备份的软件源：\nzypper addrepo /var/tmp/backup.repo\n19. 打开或关闭软件源 可以使用 modifyrepo 命令的 -d 选项来关闭一个软件源：\n# zypper modifyrepo -d Mozillarepo Repository \u0026#39;Mozillarepo\u0026#39; has been successfully disabled. 使用 -e 选项来打开一个关闭的软件源：\n# zypper modifyrepo -e Mozillarepo Repository \u0026#39;Mozillarepo\u0026#39; has been successfully enabled. 20. 刷新软件源 当软件源过期时，刷新软件源是非常重要的。你可以像下面一样执行一个手动的刷新，也可以设置当需要时自动刷新：\n# zypper refresh Mozillarepo Repository \u0026#39;Mozilla-repo\u0026#39; is up to date. Specified repositories have been refreshed. 使用如下命令来设置自动刷新：\n# zypper modifyrepo --refresh mylocalrepo Autorefresh has been enabled for repository \u0026#39;mylocalrepo\u0026#39;. 【译者注】 其实这里只是示例了 zypper 的一些基本用法，zypper 是一个很强大的软件包管理工具，请查看 Portal:Zypper 获取更多详细信息。\n","date":"April 27, 2015","hero":"/zh/posts/linux/opensuse-zypper/opensuse-zypper.png","permalink":"/zh/posts/linux/opensuse-zypper/","summary":"自己在使用 opensuse，自己非常喜欢 opensuse 的包管理命令行工具 zypper。这里做一个笔记，也希望能看到这个篇文章的朋友能够快速地掌握 zypper 的用法。\nCentOS 和 Redhat 使用的是 yum 作为命令行的软件包管理工具。\nDebian 和 Ubuntu 使用的是 apt-get。\n在 Debian/Ubuntu 上还有另一个软件包管理工具 - aptitude 。\n同样，在 SUSE/opensuse Linux 上，zypper 就是其命令行的软件包管理工具。\n从高层次的角度，你可以使用 zypper 命令管理两种不同的东西：\n管理软件包： 使用 zypper 来安装，删除，更新以及查询本地的或者远端媒体上的软件包。 管理软件源： 也可以使用 zypper 管理软件源信息，你可以在命令行添加，删除，打开或者关闭某个软件源。它还可以设置在安装过程中软件源的优先级。 I. 使用 zypper 管理软件包 1. 安装软件包 使用如下语法安装一个软件包：\nzypper install \u0026lt;package name\u0026gt;\n如，执行一下命令来安装 火狐浏览器 和它的依赖：\n# zypper install MozillaFirefox Loading repository data... Reading installed packages... Resolving package dependencies... The following NEW packages are going to be installed: MozillaFirefox MozillaFirefox-branding-SLED The following packages are not supported by their vendor: MozillaFirefox MozillaFirefox-branding-SLED 2 new packages to install.","tags":["opensuse","zypper"],"title":"OPENSUSE上的ZYPPER包管理器"},{"categories":["Linux"],"contents":"这是我在使用openSuSE过程中学习和使用systemd来管理系统的一些笔记。 首先那就让我们来先看看什么是systemd：\nSystemd Systemd是Linux下的一个程序，用来初始化系统。像SysV一样，其将会被Linux内核启动。 在opneSuSE上，systemd将会是系统进程号为1的进程，其负责初始化系统和启动系统服务。\nopenSuSE从12.3版本开始，用systemd作为默认的系统初始化程序代替了SysV。 想了解systemd和SysV的对于，可以参看这里。\n用sytemctl进行系统管理 systemd提供了systemctl命令来进行系统服务管理，其调用格式如下：\nsystemctl [通用选项] 子命令 [子命令选项]\n在系统上管理服务 像SysV一样，通过子命令start|stop|restart等来管理服务：\nsystemctl start|stop|status|restart|reload|\u0026hellip; \u0026lt;服务名\u0026gt;.service\n如查看当前cron服务的状态：\n# systemctl status cron.service cron.service - Command Scheduler Loaded: loaded (/usr/lib/systemd/system/cron.service; enabled) Active: active (running) since Mon 2015-01-26 15:50:21 CST; 3 days ago Main PID: 1247 (cron) CGroup: /system.slice/cron.service └─1247 /usr/sbin/cron -n\nsystemctl支持一次操作多个服务，只要在子命令后添加多个服务名即可。\n使用enable|disable来设置开启自动启动或者不启动一个服务：\nsystemctl enable|disable \u0026lt;服务名\u0026gt;.service\nSystemd的启动目标 在SysV的启动系统上，用启动级别（runlevel）来表示系统的启动状态，已经哪些服务伴随这级别一起启动。 如 0 （关闭系统），3 （多用户带网络），5 （多用户带网络，显示图形用户界面）。\n而在Systemd上，用目标（target）表示这个概念，如 graphical.target 提供了多用户带网，显示图像用户界面的启动目标，就相当与level 5。 systemd提供了很多内置的目标单元，可以用下面的命令查看：\n# systemctl list-unit-files \u0026ndash;type=target\n为了和SysV兼容，system提供了如下的启动目标：\nSysV的运行级别 Systemd的启动目标 描述 0 runlevel0.target, halt.target, poweroff.target 关闭系统 1,S runlevel1.target, rescue.target 单用户模式 2 runlevel2.target, multi-user.target 多用户无网络模式 3 runlevel3.target 多用户带网络模式 4 runlevel4.target 未定义 5 runlevel5.target, graphical.target 图形模式 6 runlevel6.target, reboot.target 重启 用systemctl来操作目标 同样可以使用systemctl来操作启动目标，如更改当前目标：\nsystemctl isolate \u0026lt;目标名\u0026gt;.target\n这类似在SysV上使用telinit X到某个运行级别。 登录到默认目标：\nsystemctl default\n获取当前激活的目标：\nsystemctl list-units \u0026ndash;type=target\n显示某个目标的依赖：\nsystemctl show -p \u0026ldquo;Requires\u0026rdquo; \u0026lt;目标名\u0026gt;.target systemctl show -p \u0026ldquo;Wants\u0026rdquo; \u0026lt;目标名\u0026gt;.target\n可以使用下面的方式来修改默认的启动目标：\nln -sf /lib/systemd/system/\u0026lt;目标名\u0026gt;.target /etc/systemd/system/default.target\n这里只是简单地介绍了下systemd在系统管理的一些基本用法，systemd还有很多的高级应用，比如自定目标等等。 想了解更多关于systemd，请参考如下链接：\nhttp://www.freedesktop.org/wiki/Software/systemd http://0pointer.de/blog/projects\n最后在给一个命令systemd-analyze plot，其可以生成系统服务启动时间的一个SVG图，如：\nsystemd-analyze plot \u0026gt; startup-time.svg\n","date":"January 29, 2015","hero":"/zh/posts/linux/systemd-systemctl/systemd.webp","permalink":"/zh/posts/linux/systemd-systemctl/","summary":"这是我在使用openSuSE过程中学习和使用systemd来管理系统的一些笔记。 首先那就让我们来先看看什么是systemd：\nSystemd Systemd是Linux下的一个程序，用来初始化系统。像SysV一样，其将会被Linux内核启动。 在opneSuSE上，systemd将会是系统进程号为1的进程，其负责初始化系统和启动系统服务。\nopenSuSE从12.3版本开始，用systemd作为默认的系统初始化程序代替了SysV。 想了解systemd和SysV的对于，可以参看这里。\n用sytemctl进行系统管理 systemd提供了systemctl命令来进行系统服务管理，其调用格式如下：\nsystemctl [通用选项] 子命令 [子命令选项]\n在系统上管理服务 像SysV一样，通过子命令start|stop|restart等来管理服务：\nsystemctl start|stop|status|restart|reload|\u0026hellip; \u0026lt;服务名\u0026gt;.service\n如查看当前cron服务的状态：\n# systemctl status cron.service cron.service - Command Scheduler Loaded: loaded (/usr/lib/systemd/system/cron.service; enabled) Active: active (running) since Mon 2015-01-26 15:50:21 CST; 3 days ago Main PID: 1247 (cron) CGroup: /system.slice/cron.service └─1247 /usr/sbin/cron -n\nsystemctl支持一次操作多个服务，只要在子命令后添加多个服务名即可。\n使用enable|disable来设置开启自动启动或者不启动一个服务：\nsystemctl enable|disable \u0026lt;服务名\u0026gt;.service\nSystemd的启动目标 在SysV的启动系统上，用启动级别（runlevel）来表示系统的启动状态，已经哪些服务伴随这级别一起启动。 如 0 （关闭系统），3 （多用户带网络），5 （多用户带网络，显示图形用户界面）。\n而在Systemd上，用目标（target）表示这个概念，如 graphical.target 提供了多用户带网，显示图像用户界面的启动目标，就相当与level 5。 systemd提供了很多内置的目标单元，可以用下面的命令查看：\n# systemctl list-unit-files \u0026ndash;type=target","tags":["systemd","linux"],"title":"SYSTEMCTL管理系统服务"},{"categories":["Programming"],"contents":"今天在写一个 shell 脚本的时候遇到了如下的错误：\nline 225: unexpected EOF while looking for matching `”‘\nline 233: syntax error: unexpected end of file\n可是认真查看它提示出错的行时，却发现(\u0026quot;)号是配对的。\n225行是如下的代码:\nif [ ${options} = \u0026#34;ALL\u0026#34; ]; then 很明显报错的行数不对，由于前面的代码比较多，一行行看也不容易看错错误。 既然说文件结尾有问题，于是我在文件尾加上一个(\u0026quot;)符号，再运行。现在提示给出了正确的错误行号。\n原来是前面有这样的代码:\necho \u0026#34;${PACKAGENAME\u0026#34; \u0026gt;\u0026gt; ${file} 是变量的引用时少了相应的 (}) 号。\n所以，如果以后遇到类似的问题，可以尝试加一些符号来调试，也许会有帮助。\n","date":"February 13, 2011","hero":"/images/default-hero.jpg","permalink":"/zh/posts/programming/shell-debug/","summary":"今天在写一个 shell 脚本的时候遇到了如下的错误：\nline 225: unexpected EOF while looking for matching `”‘\nline 233: syntax error: unexpected end of file\n可是认真查看它提示出错的行时，却发现(\u0026quot;)号是配对的。\n225行是如下的代码:\nif [ ${options} = \u0026#34;ALL\u0026#34; ]; then 很明显报错的行数不对，由于前面的代码比较多，一行行看也不容易看错错误。 既然说文件结尾有问题，于是我在文件尾加上一个(\u0026quot;)符号，再运行。现在提示给出了正确的错误行号。\n原来是前面有这样的代码:\necho \u0026#34;${PACKAGENAME\u0026#34; \u0026gt;\u0026gt; ${file} 是变量的引用时少了相应的 (}) 号。\n所以，如果以后遇到类似的问题，可以尝试加一些符号来调试，也许会有帮助。","tags":["shell","debug"],"title":"调试Shell脚本错误"},{"categories":["Linux"],"contents":"前段时间，发现每次开机后一段时间机器就会很慢，似乎在跑些什么任务，于是查看系统任务，发现有updatadb（为locate构建数据索引）在运行。这些任务是由 cron 触发的。\n于是我用crontab命令来查看当前的cron任务列表，可是得到如下返回：\n$sudo crontab -u root -l\nroot’s password:\nno crontab for root\n可是我发现在*/etc/cron.daily/目录下有一些脚本， 其中就有一个suse-updatedb*。那些进程就是由这个脚本启动的。 接下来，我查看了一下*/etc/crontab*文件:\n-*/15 * * * * root test -x /usr/lib/cron/run-crons \u0026amp;\u0026amp; /usr/lib/cron/run-crons \u0026gt;/dev/null 2\u0026gt;\u0026amp;1\n这说明系统会每15分钟调用一次 /usr/lib/cron/run-crons 脚本，接着查看了一下那个脚本，其中发现了一行注释：\n# if DAILY_TIME set, run only at a fixed time of day\n而DAILY_TIME这个变量应该在*/etc/sysconfig/cron*配置文件里指定，在文件中有这么一段：\n## Type: string\n## Default: “”\n#\n# At which time cron.daily should start. Default is 15 minutes after booting\n# the system. Example setting would be “14:00″.\n# Due to the fact that cron script runs only every 15 minutes,\n# it will only run on xx:00, xx:15, xx:30, xx:45, not at the accurate time\n# you set.\nDAILY_TIME=\u0026quot;\u0026quot;\n也就是说如果没有指定DAILY_TIME，则cron.daily将在系统启动后的15分钟触发，并且如果要指定特定时间，也只能指定在每小时的00/15/30/45分钟。\n于是我把这个时间设定为01:45，这样就在每天的凌晨1点45分的时候才会触发那些cron任务。\n不过我看了下*/usr/lib/cron/run-crons*，还是没有完全弄明白这个cron框架是如何去按照指定时间触发的。 好像openSUSE 11.4将使用新的cronie 1.4.4替代现在的vixie-cron 4.1，看看这里SDB:Cron。\n","date":"February 2, 2011","hero":"/zh/posts/linux/opensuse-cronjob/cron-job.webp","permalink":"/zh/posts/linux/opensuse-cronjob/","summary":"前段时间，发现每次开机后一段时间机器就会很慢，似乎在跑些什么任务，于是查看系统任务，发现有updatadb（为locate构建数据索引）在运行。这些任务是由 cron 触发的。\n于是我用crontab命令来查看当前的cron任务列表，可是得到如下返回：\n$sudo crontab -u root -l\nroot’s password:\nno crontab for root\n可是我发现在*/etc/cron.daily/目录下有一些脚本， 其中就有一个suse-updatedb*。那些进程就是由这个脚本启动的。 接下来，我查看了一下*/etc/crontab*文件:\n-*/15 * * * * root test -x /usr/lib/cron/run-crons \u0026amp;\u0026amp; /usr/lib/cron/run-crons \u0026gt;/dev/null 2\u0026gt;\u0026amp;1\n这说明系统会每15分钟调用一次 /usr/lib/cron/run-crons 脚本，接着查看了一下那个脚本，其中发现了一行注释：\n# if DAILY_TIME set, run only at a fixed time of day\n而DAILY_TIME这个变量应该在*/etc/sysconfig/cron*配置文件里指定，在文件中有这么一段：\n## Type: string\n## Default: “”\n#\n# At which time cron.daily should start. Default is 15 minutes after booting\n# the system. Example setting would be “14:00″.","tags":["opensuse"],"title":"OPENSUSE上的定时任务"},{"categories":["Linux"],"contents":"通常，你也许不需要知道你使用了什么样的硬件 — 你也许拥有的是一台来自比较小一点公司的组装机或者一台二手机。本月，我将介绍你可以用来查看你安装的硬件的工具。 第一步，使用lshw — 列举硬件工具。如果你使用普通用户执行，它会警告你需要使用root执行。因此，以sudo lshw执行。你将可以看到屏幕上显示你系统的信息。第一段将是常规信息，看起来就像下面这样：\njbernard-eeepc\ndescription: Notebook\nproduct: 700\nvendor: ASUSTeK Computer INC.\nversion: 0129\nserial: EeePC-1234567890\nwidth: 32 bits\ncapabilities: smbios-2.5 dmi-2.5 smp-1.4 smp\nconfiguration: boot=normal chassis=notebook\ncpus=1 uuid=XXXXXX-XXXXX-XXXXX-XXXXX\n这是我在我的ASUS EeePC执行的结果。你可以看到生产商是ASUSTeK, BIOS的版本是0129， 以及这是一台32位的单一CPU机器。更多的信息以下面的分类来说明：\ncore\nfirmware - motherboard and BIOS information\ncpu - CPU information\ncache - cache information\nmemory - memory information\nbank - specific bank memory information\npci - PCI bus information\ndisplay - PCI display adapter\nmultimedia - PCI audio adapter\npci - other PCI devices\nnetwork - PCI network adapter\nusb - USB devices\nide - IDE information\ndisk - individual disks\nvolume - volumes on this disk\n对于多少信息可用的想法，以下内存段显示了我的EeePC的内存信息：\n*-memory\ndescription: System Memory\nphysical id: 1f slot: System board or motherboard\nsize: 512MiB\n*-bank\ndescription: DIMM DDR2 Synchronous 400 MHz (2.5 ns)\nproduct: PartNum0\nvendor: Manufacturer0\nphysical id: 0\nserial: SerNum0\nslot: DIMM0\nsize: 512MiB\nwidth: 64 bits\nclock: 400MHz (2.5ns)\n这是一个基本的集中成一个命令的工具，执行一次，它可以得到系统的所有信息。但是，如果你只想知道指定子系统的信息，怎么办呢？其实是有一整套的工具的，当你需要一些指定的信息或者想在一个脚本做一些系统查询时，这些工具将更有用。 你也许只想看看CPU，lscpu工具提供了类似如下的输出：\nArchitecture: i686\nCPU op-mode(s): 32-bit\nCPU(s): 1\nThread(s) per core: 1\nCore(s) per socket: 1\nCPU socket(s): 1\nVendor ID: GenuineIntel\nCPU family: 6\nModel: 13\nStepping: 8\nCPU MHz: 571.427\n你可以从中看出生产厂家，是否是32位还是64位的，准确的版本和型号，以及当前CPU频率。 如果你想知道你的显卡是否被X11支持，或者你是否需要第三方驱动，你可以使用lspci。该工具给出了所有插在你的PCI总线的设备的信息。输出类似下面：\n00:02.0 VGA compatible controller: Intel Corporation\nMobile 915GM/GMS/910GML Express Graphics Controller (rev 04)\n00:02.1 Display controller: Intel Corporation\nMobile 915GM/GMS/910GML Express Graphics Controller (rev 04)\n这个信息显示了在我的EeePC上的显卡控制器是一个Intel控制器。因此，如果你想，你现在就可以到Google搜索关于你的显卡的信息以及怎样最大限度的配置它。如果你想看看你系统上的USB设备，那么可以使用lsusb。在我的EeePC上，拥有一个SD卡，显示如下：\nBus 001 Device 002: ID 0951:1606 Kingston Technology\n如果你对硬盘子系统感兴趣，你可以使用blkid工具来查看。该工具打印出所有可用的文件系统，类似如下的输出：\n/dev/sda1: UUID=\u0026ldquo;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026rdquo; TYPE=\u0026ldquo;ext2\u0026rdquo;\n/dev/sda2: UUID=\u0026ldquo;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026rdquo; TYPE=\u0026ldquo;swap\u0026rdquo;\n/dev/sda3: UUID=\u0026ldquo;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026rdquo; TYPE=\u0026ldquo;ext2\u0026rdquo;\n/dev/sdb1: UUID=\u0026ldquo;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026rdquo; TYPE=\u0026ldquo;ext2\u0026rdquo;\n用这个工具，你可以了解到什么设备可用以及什么文件系统正用在它们上面。如果你想在*/etc/fstab*里的条目使用UUID，那相关联的UUID是可用的。\n现在你知道你的系统使用了什么样的硬件，最后需要检查的是看看是否你的内核实际上正在使用可用硬件。在大部分现在的发行版本中，内核是被编译来使用模块的。你可以使用lsmod命令来查看哪个模块被加载了。你将得到像下面的一个别表：\nagpgart 31788 2 drm,intel_agp\nlp 7028 0\nvideo 17375 1 i915\noutput 1871 1 video\n你可以看到agpgart模块拥有31788字节的大小，以及被drm和intel_agp模块使用。\n现在，希望你可以配置和优化你的硬件，使你最大的使用它们。如果你发现有其他的工具这里没有提到，我很愿意得知它们。\n","date":"November 19, 2010","hero":"/zh/posts/linux/what-hardware/linux-hardware.jpg","permalink":"/zh/posts/linux/what-hardware/","summary":"通常，你也许不需要知道你使用了什么样的硬件 — 你也许拥有的是一台来自比较小一点公司的组装机或者一台二手机。本月，我将介绍你可以用来查看你安装的硬件的工具。 第一步，使用lshw — 列举硬件工具。如果你使用普通用户执行，它会警告你需要使用root执行。因此，以sudo lshw执行。你将可以看到屏幕上显示你系统的信息。第一段将是常规信息，看起来就像下面这样：\njbernard-eeepc\ndescription: Notebook\nproduct: 700\nvendor: ASUSTeK Computer INC.\nversion: 0129\nserial: EeePC-1234567890\nwidth: 32 bits\ncapabilities: smbios-2.5 dmi-2.5 smp-1.4 smp\nconfiguration: boot=normal chassis=notebook\ncpus=1 uuid=XXXXXX-XXXXX-XXXXX-XXXXX\n这是我在我的ASUS EeePC执行的结果。你可以看到生产商是ASUSTeK, BIOS的版本是0129， 以及这是一台32位的单一CPU机器。更多的信息以下面的分类来说明：\ncore\nfirmware - motherboard and BIOS information\ncpu - CPU information\ncache - cache information\nmemory - memory information\nbank - specific bank memory information\npci - PCI bus information\ndisplay - PCI display adapter","tags":["linux","hardware"],"title":"查看硬件信息"},{"categories":["Programming"],"contents":"记得以前学习数据结构和算法时就了解了冒泡排序算法， 前几天的一个面试也被问到了这个问题。于是回来后就又温习了一遍，还了解到了一种冒泡的改良算法，叫做鸡尾酒(cocktail)排序算法，其实现是通过两个循环分别从两端进行冒泡。\n通常实现 template\u0026lt; typename T \u0026gt; void cocktail_sort(T array[], int n) { int bottom = 0; int top = n – 1; bool swapped = true; while (swapped) { swapped = false; for ( int i = bottom; i \u0026lt; top; i++ ) { if ( array[i] \u0026gt; array[i+1] ) { swap(array[i], array[i+1]); swapped = true; } } top–; // top is a larger one for ( int i = top; i \u0026gt; bottom; i– ) { if ( array[i] \u0026lt; array[i-1] ) { swap(array[i], array[i-1]); swapped = true; } } bottom++; // bottom is a smaller one } } // end, cocktail_sort 改良实现 我在网上搜索的实现基本上都是上面的实现方法。 我就想为什么不在一个循环中两端一起进行冒泡呢？于是我实现了下面这样的改良的cocktail算法实现，暂且取名为bi_bubble_sort，减少了循环比较的次数：\ntemplate\u0026lt; typename T \u0026gt; void bi_bubble_sort(T *array, int n) { int top = 0; int bottom = n – 1; bool swapped = true; while (swapped) { swapped = false; int i = top; int j = bottom; for (i = top, j = bottom; i \u0026lt; bottom || j \u0026gt; top; i++, j–) { if (array[i] \u0026gt; array[i+1]) { swap(array[i], array[i+1]); swapped |= true; } if (array[j] \u0026lt; array[j-1]) { swap(array[j], array[j-1]); swapped |= true; } } top++; bottom–; } } // end, bi_bubble_sort 不过冒泡排序在排序算法中是效率比较差的算法了，实际中应该很少应用，呵呵！\n","date":"October 27, 2010","hero":"/images/default-hero.jpg","permalink":"/zh/posts/programming/cocktail-improve/","summary":"记得以前学习数据结构和算法时就了解了冒泡排序算法， 前几天的一个面试也被问到了这个问题。于是回来后就又温习了一遍，还了解到了一种冒泡的改良算法，叫做鸡尾酒(cocktail)排序算法，其实现是通过两个循环分别从两端进行冒泡。\n通常实现 template\u0026lt; typename T \u0026gt; void cocktail_sort(T array[], int n) { int bottom = 0; int top = n – 1; bool swapped = true; while (swapped) { swapped = false; for ( int i = bottom; i \u0026lt; top; i++ ) { if ( array[i] \u0026gt; array[i+1] ) { swap(array[i], array[i+1]); swapped = true; } } top–; // top is a larger one for ( int i = top; i \u0026gt; bottom; i– ) { if ( array[i] \u0026lt; array[i-1] ) { swap(array[i], array[i-1]); swapped = true; } } bottom++; // bottom is a smaller one } } // end, cocktail_sort 改良实现 我在网上搜索的实现基本上都是上面的实现方法。 我就想为什么不在一个循环中两端一起进行冒泡呢？于是我实现了下面这样的改良的cocktail算法实现，暂且取名为bi_bubble_sort，减少了循环比较的次数：","tags":["algorithm","cpp"],"title":"改良鸡尾酒排序算法"},{"categories":["DEVOPS"],"contents":"容器 Docker 越来越受开发者和运维人员的喜爱，更是作为实践 DevOps 的一个中要工具。同时 Gitlab 提供了免费的代码管理服务，其 gitlab-ci 更是提供了强大的自动化 CI/CD 流程功能。\n本文以一个静态站点的示例来说明如何使用 gitlab-ci 和 docker 进行容器镜像的构建，以及如何将镜像自动化部署到目标服务器上。\n编写Dockerfile 首先在代码库中增加 Dockerfile ，用于描述如何构建应用的容器镜像。以下是一个基于 Hugo 的静态站点应用的示例：\nFROM mengzyou/hugo:latest as builder COPY . /app/ RUN hugo FROM nginx:1.16-alpine RUN set -x \\ \u0026amp;\u0026amp; rm -f /etc/nginx/conf.d/default.conf \\ \u0026amp;\u0026amp; mkdir -p /usr/share/nginx/html COPY --from=builder /app/nginx-default.conf /etc/nginx/conf.d/default.conf COPY --from=builder /app/public/ /usr/share/nginx/html 其实非常简单，使用了多阶段构建，以 mengzyou/hugo 作为构建镜像，然后将生成的静态文件拷贝到 nginx 镜像中，最终生成静态站点的镜像。\n配置Gitlab-ci构建容器镜像 该阶段，在项目根目录添加 .gitlab-ci.yml 文件，示例内容如下：\nvariables: DOCKER_DRIVER: overlay2 CI_REGISTRY_IMAGE: ${CI_REGISTRY}/mengzyou/app before_script: - echo $CI_JOB_NAME - echo $CI_PROJECT_DIR stages: - build build:docker: stage: build variables: DOCKER_HOST: tcp://docker:2375 image: docker:stable services: - docker:dind script: - echo \u0026#34;Building image - $CI_REGISTRY_IMAGE:latest\u0026#34; - echo \u0026#34;$CI_REGISTRY_PASSWORD\u0026#34; | docker login -u \u0026#34;$CI_REGISTRY_USER\u0026#34; --password-stdin $CI_REGISTRY - docker image build --force-rm --no-cache -t $CI_REGISTRY_IMAGE:latest . - docker image push $CI_REGISTRY_IMAGE:latest only: - master 其中有几个变量是需要在代码库的 Gitlab 上进行配置，如下图所示：\n因为直接是用了 Gitlab 提供的容器镜像服务，构建完成的镜像需要推送到该镜像仓库服务，所以需要配置相应的仓库地址，用户名和登录密码：\nCI_REGISTRY : 仓库地址，如 registry.gitlab.com CI_REGISTRY_USER : Gitlab 的访问用户名 CI_REGISTRY_PASSWORD : Gitlab 的访问密码 这样在推送镜像之前，需要进行一次登录操作。更多关于如何是用 Gitlab 容器镜像仓库可参考 https://docs.gitlab.com/ee/user/project/container_registry.html 。\n该 pipeline 工作使用了 Gitlab Runner 的 docker 执行器，如果是自己安装和注册 gitlab-runner ，请参考其文档。这里直接是用了 Gitlab.com 分享的 Runner 。\n安装用于部署的gitlab-runner 为了在目标服务器上使用 gitlab-ci 进行自动部署（运行容器），需要在目标服务器上安装和注册 gitlab-runner 。由于目标服务器上运行容器，因此应该首先安装好 Docker 环境，同时 gitlab-runner 也直接是用容器运行（当然也可以直接本地运行，具体可以查看 gitlab-runner 的文档）。\n安装注册Runner 可以使用 docker-compose 来部署 gitlab-runner 容器，在目标服务器上的 ~/gitalb/ 目录下编写如下 docker-compose.yml 文件：\nversion: \u0026#39;2.4\u0026#39; services: runner: image: gitlab/gitlab-runner:alpine-v11.11.0 container_name: gitlab_runner restart: always network_mode: bridge volumes: - ./config/:/etc/gitlab-runner/ - /var/run/docker.sock:/var/run/docker.sock 创建目录 config 用于保存 runner 的配置:\nmkdir config\n然后启动 runner 容器：\ndocker-compose up -d runner\n注册 runner 之前需要在 Gitlab 项目的 CI/CD 配置中（设置 \u0026gt; CI/CD \u0026gt; Runner）查看注册 URL 和令牌，如下图所示：\n接下来，运行一下命令进行 runner 的注册:\ndocker-compose exec runner gitlab-runner register \\ --url https://gitlab.com/ \\ --registration-token $REGISTRATION_TOKEN \\ --executor docker \\ --description \u0026#34;app-deployment-runner\u0026#34; \\ --tag-list \u0026#34;deploy,docker\u0026#34; \\ --docker-image \u0026#34;mengzyou/docker:19.03\u0026#34; \\ --docker-volumes /var/run/docker.sock:/var/run/docker.sock 注意 : 上面的 $REGISTRATION_TOKEN 需要替换成真实的令牌值。\n上面的命令注册了一个默认使用 mengzyou/docker 镜像运行容器的执行器，同时挂载了 /var/run/docker.sock，因为默认执行容器里的 docker 将房屋主机上的 docker 服务。\n配置Gitlab-ci进行自动部署 在 .gitlab-ci.yml 中增加部署的工作：\nstages: - build - deploy deoploy:docker: stage: deploy script: - echo \u0026#34;Deploy try_app - $CI_REGISTRY_IMAGE:latest\u0026#34; - echo \u0026#34;$CI_REGISTRY_PASSWORD\u0026#34; | docker login -u \u0026#34;$CI_REGISTRY_USER\u0026#34; --password-stdin $CI_REGISTRY - docker rm -f try_app || true - docker image pull $CI_REGISTRY_IMAGE:latest - docker container run --name try_app -p 80:80 -d $CI_REGISTRY_IMAGE:latest only: - master tags: - deploy - docker 当然，也可以写部署脚本来执行部署的步骤，也可以使用 docker-compose 通过 docker-compose.yml 文件来执行，例如：\ndeoploy:docker: stage: deploy script: - echo \u0026#34;Deploy ${CD_COMPOSE_PROJECT}_app - $CI_REGISTRY_IMAGE:latest\u0026#34; - echo \u0026#34;$CI_REGISTRY_PASSWORD\u0026#34; | docker login -u \u0026#34;$CI_REGISTRY_USER\u0026#34; --password-stdin $CI_REGISTRY - docker image pull $CI_REGISTRY_IMAGE:latest - mkdir -p /tmp/${CD_COMPOSE_PROJECT} \u0026amp;\u0026amp; cp ${CI_PROJECT_DIR}/docker-compose.prod.yml /tmp/${CD_COMPOSE_PROJECT}/docker-compose.yml - cd /tmp/${CD_COMPOSE_PROJECT} \u0026amp;\u0026amp; docker-compose up -d myblog only: - master tags: - deploy - docker 其中可以定义 CD_COMPOSE_PROJECT 变量来指定 docker-compose 的项目名，相应的在代码根目录增加 docker-compose.prod.yml 文件：\nversion: \u0026#39;2.4\u0026#39; networks: web: external: true services: app: image: \u0026#34;registry.gitlab.com/mengzyou/app:latest\u0026#34; networks: - web restart: unless-stopped mem_limit: 256M labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=web\u0026#34; - \u0026#34;traefik.backend=app\u0026#34; - \u0026#34;traefik.frontend.rule=Host:app.mengz.me;PathPrefix:/\u0026#34; - \u0026#34;traefik.frontend.redirect.entryPoint=https\u0026#34; - \u0026#34;traefik.frontend.redirect.permanent=true\u0026#34; - \u0026#34;traefik.port=80\u0026#34; 上面的 docker-composle 文件还定义了 traefik 相关的参数，这要求在目标服务器上已经运行了相应的 traefik 服务作为 Web 代理。\n完成以上步骤之后，每次推送 master 分支，都会触发相应的 gitlab CI/CD pipeline 来执行每个阶段的 CI/CD 工作，如下图：\n总结 以上仅仅进行了一个简单应用的 CI/CD 示例，主要演示了如何使用 gitlab-ci 和 docker 来进行 CI/CD 流程。\n对于不同的项目，如何构建？如何部署，情况都可能不一样，需要根据不同的场景来编写 .gitlab-ci.yml 文件，需要进一步阅读相应的文档 。\n","date":"January 1, 0001","hero":"/zh/posts/devops/gitlab-ci-docker/gitlab-ci-docker.png","permalink":"/zh/posts/devops/gitlab-ci-docker/","summary":"容器 Docker 越来越受开发者和运维人员的喜爱，更是作为实践 DevOps 的一个中要工具。同时 Gitlab 提供了免费的代码管理服务，其 gitlab-ci 更是提供了强大的自动化 CI/CD 流程功能。\n本文以一个静态站点的示例来说明如何使用 gitlab-ci 和 docker 进行容器镜像的构建，以及如何将镜像自动化部署到目标服务器上。\n编写Dockerfile 首先在代码库中增加 Dockerfile ，用于描述如何构建应用的容器镜像。以下是一个基于 Hugo 的静态站点应用的示例：\nFROM mengzyou/hugo:latest as builder COPY . /app/ RUN hugo FROM nginx:1.16-alpine RUN set -x \\ \u0026amp;\u0026amp; rm -f /etc/nginx/conf.d/default.conf \\ \u0026amp;\u0026amp; mkdir -p /usr/share/nginx/html COPY --from=builder /app/nginx-default.conf /etc/nginx/conf.d/default.conf COPY --from=builder /app/public/ /usr/share/nginx/html 其实非常简单，使用了多阶段构建，以 mengzyou/hugo 作为构建镜像，然后将生成的静态文件拷贝到 nginx 镜像中，最终生成静态站点的镜像。\n配置Gitlab-ci构建容器镜像 该阶段，在项目根目录添加 .gitlab-ci.yml 文件，示例内容如下：\nvariables: DOCKER_DRIVER: overlay2 CI_REGISTRY_IMAGE: ${CI_REGISTRY}/mengzyou/app before_script: - echo $CI_JOB_NAME - echo $CI_PROJECT_DIR stages: - build build:docker: stage: build variables: DOCKER_HOST: tcp://docker:2375 image: docker:stable services: - docker:dind script: - echo \u0026#34;Building image - $CI_REGISTRY_IMAGE:latest\u0026#34; - echo \u0026#34;$CI_REGISTRY_PASSWORD\u0026#34; | docker login -u \u0026#34;$CI_REGISTRY_USER\u0026#34; --password-stdin $CI_REGISTRY - docker image build --force-rm --no-cache -t $CI_REGISTRY_IMAGE:latest .","tags":["gitlab","cicd","docker"],"title":"GITLAB CI自动部署容器应用"}]